<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta content="website" property="og:type"/><meta content="app-id=1263413087" name="apple-itunes-app"/><meta content="M-70jYcq5Hj35EY_NQzm9MAPI6pfVrq-hqaiK13ZQeo" name="google-site-verification"/><link href="/manifest.json" rel="manifest"/><link href="/users/sign_up" rel="preconnect"/><link href="https://compliance.datacamp.com" rel="preconnect"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-57x57.png" rel="apple-touch-icon-precomposed" sizes="57x57"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-114x114.png" rel="apple-touch-icon-precomposed" sizes="114x114"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-72x72.png" rel="apple-touch-icon-precomposed" sizes="72x72"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-144x144.png" rel="apple-touch-icon-precomposed" sizes="144x144"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-60x60.png" rel="apple-touch-icon-precomposed" sizes="60x60"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-120x120.png" rel="apple-touch-icon-precomposed" sizes="120x120"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-76x76.png" rel="apple-touch-icon-precomposed" sizes="76x76"/><link href="/marketing-backgrounds/favicons/apple-touch-icon-152x152.png" rel="apple-touch-icon-precomposed" sizes="152x152"/><link href="/marketing-backgrounds/favicons/favicon-196x196.png" rel="icon" sizes="196x196" type="image/png"/><link href="/marketing-backgrounds/favicons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png"/><link href="/marketing-backgrounds/favicons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/><link href="/marketing-backgrounds/favicons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/><link href="/marketing-backgrounds/favicons/favicon-128.png" rel="icon" sizes="128x128" type="image/png"/><link href="https://images.datacamp.com" rel="preconnect"/><meta content=" " name="application-name"/><meta content="#FFFFFF" name="msapplication-TileColor"/><meta content="/marketing-backgrounds/favicons/mstile-144x144.png" name="msapplication-TileImage"/><meta content="/marketing-backgrounds/favicons/mstile-70x70.png" name="msapplication-square70x70logo"/><meta content="mstile-150x150.png" name="/marketing-backgrounds/favicons/msapplication-square150x150logo"/><meta content="mstile-310x150.png" name="/marketing-backgrounds/favicons/msapplication-wide310x150logo"/><meta content="mstile-310x310.png" name="/marketing-backgrounds/favicons/msapplication-square310x310logo"/><title>Apache Spark Tutorial: Machine Learning | DataCamp</title><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","author":[{"@type":"Person","avatarUrl":"https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png","name":"Karlijn Willems","sameAs":["https://www.linkedin.com/in/karlijnwillems/","https://medium.com/@kacawi"],"url":"https://www.datacamp.com/portfolio/karlijn"}],"datePublished":"2017-07-28T00:00:00.000Z","description":"Apache Spark tutorial introduces you to big data processing, analysis and Machine Learning (ML) with PySpark.","headline":"Apache Spark Tutorial: ML with PySpark","mainEntityOfPage":{"@id":"https://www.datacamp.com/tutorial/apache-spark-tutorial-machine-learning?utm_source=google&utm_medium=paid_search&utm_campaignid=19589720824&utm_adgroupid=157156376111&utm_device=m&utm_keyword=&utm_matchtype=&utm_network=g&utm_adpostion=&utm_creative=716160943435&utm_targetid=aud-517318241987%3Adsa-2218886984300&utm_loc_interest_ms=&utm_loc_physical_ms=1009899&utm_content=&utm_campaign=230119_1-sea~dsa~tofu_2-b2c_3-row-p2_4-prc_5-na_6-na_7-le_8-pdsh-go_9-nb-e_10-na_11-na-oct24&gad_source=1&gclid=Cj0KCQjw4Oe4BhCcARIsADQ0cskwCCJkoGWILJvUh5NZgj_VFuSHme5vGerxTGQSBnbR6VxCz0RZ1DwaApxFEALw_wcB","@type":"WebPage"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1603223608/DC_New_mugdv8.png"},"name":"DataCamp"}}</script><link href="https://www.datacamp.com/tutorial/apache-spark-tutorial-machine-learning" rel="canonical"/><meta content="Apache Spark tutorial introduces you to big data processing, analysis and Machine Learning (ML) with PySpark." name="description"/><meta content="Apache Spark tutorial introduces you to big data processing, analysis and Machine Learning (ML) with PySpark." property="og:description"/><meta content="Apache Spark tutorial introduces you to big data processing, analysis and Machine Learning (ML) with PySpark." property="twitter:description"/><meta content="https://www.datacamp.com/tutorial/apache-spark-tutorial-machine-learning?utm_source=google&amp;utm_medium=paid_search&amp;utm_campaignid=19589720824&amp;utm_adgroupid=157156376111&amp;utm_device=m&amp;utm_keyword=&amp;utm_matchtype=&amp;utm_network=g&amp;utm_adpostion=&amp;utm_creative=716160943435&amp;utm_targetid=aud-517318241987%3Adsa-2218886984300&amp;utm_loc_interest_ms=&amp;utm_loc_physical_ms=1009899&amp;utm_content=&amp;utm_campaign=230119_1-sea~dsa~tofu_2-b2c_3-row-p2_4-prc_5-na_6-na_7-le_8-pdsh-go_9-nb-e_10-na_11-na-oct24&amp;gad_source=1&amp;gclid=Cj0KCQjw4Oe4BhCcARIsADQ0cskwCCJkoGWILJvUh5NZgj_VFuSHme5vGerxTGQSBnbR6VxCz0RZ1DwaApxFEALw_wcB" property="og:url"/><meta content="summary_large_image" name="twitter:card"/><meta content="@DataCamp" name="twitter:site"/><meta content="Apache Spark Tutorial: Machine Learning" property="og:title"/><meta content="Apache Spark Tutorial: Machine Learning" property="twitter:title"/><link href="https://www.datacamp.com/tutorial/apache-spark-tutorial-machine-learning" hrefLang="x-default" rel="alternate"/><link href="https://www.datacamp.com/tutorial/apache-spark-tutorial-machine-learning" hrefLang="en" rel="alternate"/><meta name="next-head-count" content="42"/><meta charSet="UTF-8"/><link as="font" crossorigin="anonymous" href="https://dcmfe.datacamp.com/assets/fonts/StudioFeixenSansRegular-english-v2.woff2" rel="preload" type="font/woff2"/><link as="font" crossorigin="anonymous" href="https://dcmfe.datacamp.com/assets/fonts/StudioFeixenSans-Semibold-english.woff2" rel="preload" type="font/woff2"/><meta content="SVheSZoM0DmoV5ac2QhhADLAYXUKObJc20-w0uF3Rfg" name="google-site-verification"/><link rel="preload" href="/media-app/_next/static/css/e6c78026fb4eba48.css" as="style"/><link rel="stylesheet" href="/media-app/_next/static/css/e6c78026fb4eba48.css" data-n-g=""/><link rel="preload" href="/media-app/_next/static/css/9cba2183d137acbe.css" as="style"/><link rel="stylesheet" href="/media-app/_next/static/css/9cba2183d137acbe.css" data-n-p=""/><link rel="preload" href="/media-app/_next/static/css/db757c6e8e0a90b9.css" as="style"/><link rel="stylesheet" href="/media-app/_next/static/css/db757c6e8e0a90b9.css"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/media-app/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script defer="" src="/media-app/_next/static/chunks/56321.501e2a2eaee718fa.js"></script><script defer="" src="/media-app/_next/static/chunks/4981.7c857d0aaf8f4aab.js"></script><script defer="" src="/media-app/_next/static/chunks/91845.69a6d052f8aed2bf.js"></script><script src="/media-app/_next/static/chunks/webpack-aeb1ce13255ab6e9.js" defer=""></script><script src="/media-app/_next/static/chunks/framework-f82d6138897f60a3.js" defer=""></script><script src="/media-app/_next/static/chunks/main-c29a16b0f152816a.js" defer=""></script><script src="/media-app/_next/static/chunks/pages/_app-d20acb986979306c.js" defer=""></script><script src="/media-app/_next/static/chunks/5c0b189e-915768c61c1f7edc.js" defer=""></script><script src="/media-app/_next/static/chunks/80795-3b1bfdf79a119289.js" defer=""></script><script src="/media-app/_next/static/chunks/39512-35dcb51c41a7d959.js" defer=""></script><script src="/media-app/_next/static/chunks/74467-38e97b2830ae4524.js" defer=""></script><script src="/media-app/_next/static/chunks/91453-56666d588a846a58.js" defer=""></script><script src="/media-app/_next/static/chunks/85105-10177f9a1d64018a.js" defer=""></script><script src="/media-app/_next/static/chunks/29238-66534052f4a21bdf.js" defer=""></script><script src="/media-app/_next/static/chunks/89077-0021d366f36be5d7.js" defer=""></script><script src="/media-app/_next/static/chunks/89755-2e5a4c05b3d34c6a.js" defer=""></script><script src="/media-app/_next/static/chunks/84440-f91a3fdb52bcf046.js" defer=""></script><script src="/media-app/_next/static/chunks/5279-c5666005f3696572.js" defer=""></script><script src="/media-app/_next/static/chunks/60545-5b3dd966b32712aa.js" defer=""></script><script src="/media-app/_next/static/chunks/58872-3899b9c8526a7751.js" defer=""></script><script src="/media-app/_next/static/chunks/51432-9f3b0c01e7a827a7.js" defer=""></script><script src="/media-app/_next/static/chunks/72744-8e0cbfdb62eff70e.js" defer=""></script><script src="/media-app/_next/static/chunks/pages/tutorial/%5Bslug%5D-d3eed8f5dd777c1c.js" defer=""></script><script src="/media-app/_next/static/K4oD9ew1IaXuKIxkgk4N0/_buildManifest.js" defer=""></script><script src="/media-app/_next/static/K4oD9ew1IaXuKIxkgk4N0/_ssgManifest.js" defer=""></script><style id="__jsx-4276653274">html,body{scroll-behavior:auto !important}</style></head><body style="display:block"><style data-emotion="css jeybs">.css-jeybs{position:fixed;right:8px;top:8px;z-index:9999;}</style><div id="one_tap_container" class="css-jeybs"></div><div id="__next"><script id="organization_schema" type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"United States","addressLocality":"New York","postalCode":"10118","streetAddress":"350 5th Ave"},"description":"What is DataCamp? Learn the data skills you need online at your own pace—from non-coding essentials to data science and machine learning.","logo":"https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1603223608/DC_New_mugdv8.png","name":"DataCamp","sameAs":["https://twitter.com/DataCamp","https://www.linkedin.com/school/datacampinc/mycompany/","https://www.facebook.com/datacampinc/","https://www.youtube.com/c/Datacamp"],"url":"https://www.datacamp.com/"}</script><style data-emotion="css-global 4li27a">html{color:#05192D;background-color:#FFFBF3;}</style><style data-emotion="css-global 1dnoi28">*{box-sizing:border-box;}html{font-family:Studio-Feixen-Sans,Arial;margin:0;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;}body{margin:0;}</style><style data-emotion="css 1whr9bx">.css-1whr9bx{height:1px;left:-999px;overflow:hidden;position:absolute;top:auto;width:1px;}.css-1whr9bx:focus{background-color:#ffffff;border-color:#fcce0d;border-style:solid;border-width:4px;color:#05192D;height:auto;left:20px;outline:none;overflow:auto;padding-left:15px;padding-right:15px;padding-top:10px;padding-bottom:10px;-webkit-text-decoration:none;text-decoration:none;top:20px;width:auto;z-index:9999;}</style><style data-emotion="css 1gyayi8">.css-1gyayi8{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;height:1px;left:-999px;overflow:hidden;position:absolute;top:auto;width:1px;}.css-1gyayi8:active,.css-1gyayi8:focus,.css-1gyayi8:hover{-webkit-text-decoration:underline;text-decoration:underline;}.css-1gyayi8:focus{background-color:#ffffff;border-color:#fcce0d;border-style:solid;border-width:4px;color:#05192D;height:auto;left:20px;outline:none;overflow:auto;padding-left:15px;padding-right:15px;padding-top:10px;padding-bottom:10px;-webkit-text-decoration:none;text-decoration:none;top:20px;width:auto;z-index:9999;}</style><a class="css-1gyayi8" href="#main">Skip to main content</a><style data-emotion="css 1pq2yf0">.css-1pq2yf0{background-color:#05192d;position:-webkit-sticky;position:sticky;top:0;z-index:800;}</style><style data-emotion="css 1cddlv9">.css-1cddlv9{box-sizing:border-box;margin:0;min-width:0;background-color:#05192d;position:-webkit-sticky;position:sticky;top:0;z-index:800;}</style><nav aria-label="Sitewide Navigation" class="css-1cddlv9"><style data-emotion="css 1mankrf">.css-1mankrf{padding-bottom:12px;position:relative;padding-top:14px;}</style><style data-emotion="css lgmwj8">.css-lgmwj8{box-sizing:border-box;margin:0;min-width:0;width:100%;max-width:container;margin-left:auto;margin-right:auto;max-width:1172px;padding-left:16px;padding-right:16px;padding-bottom:12px;position:relative;padding-top:14px;}</style><div class="css-lgmwj8"><style data-emotion="css 1gsf33e">.css-1gsf33e{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><style data-emotion="css 13ee2hq">.css-13ee2hq{box-sizing:border-box;margin:0;min-width:0;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><div class="css-13ee2hq"><style data-emotion="css 13xhtad">.css-13xhtad{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css 13b9p20">.css-13b9p20{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css dou6ry">.css-dou6ry{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><div class="css-dou6ry"><style data-emotion="css ynaxcp">.css-ynaxcp{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}.css-ynaxcp:active,.css-ynaxcp:focus,.css-ynaxcp:hover{border-bottom:0;}</style><style data-emotion="css 1rhw2qy">.css-1rhw2qy{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}.css-1rhw2qy:active,.css-1rhw2qy:focus,.css-1rhw2qy:hover{-webkit-text-decoration:underline;text-decoration:underline;}.css-1rhw2qy:active,.css-1rhw2qy:focus,.css-1rhw2qy:hover{border-bottom:0;}</style><a aria-label="Navigate to Home Page" data-trackid="navbar-datacamp-logo" class="css-1rhw2qy" href="/"><svg width="140" height="30" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M34.352 22.195c-2.538 0-4.596-2.02-4.596-4.514 0-2.493 2.058-4.515 4.596-4.515 2.539 0 4.597 2.022 4.597 4.515a4.474 4.474 0 01-1.346 3.192 4.639 4.639 0 01-3.25 1.322zm4.605-16.349v7.019c-2.669-2.462-6.851-2.377-9.413.192a6.524 6.524 0 000 9.247c2.562 2.568 6.744 2.654 9.413.192v1.455h2.124V5.846h-2.124zm59.15 16.35c-2.538 0-4.596-2.022-4.596-4.515s2.058-4.515 4.596-4.515c2.539 0 4.597 2.022 4.597 4.515a4.474 4.474 0 01-1.347 3.192 4.638 4.638 0 01-3.25 1.322zm6.726-4.515v-6.538h-2.123v1.722c-2.668-2.462-6.85-2.377-9.412.192a6.523 6.523 0 000 9.247c2.561 2.568 6.744 2.654 9.412.192v1.455h2.124v-6.27h-.001zm16.941-6.052a1.05 1.05 0 01.895-.493h.409c.868 0 1.572.69 1.572 1.544v11.27h-2.169v-9.548c-.999 1.586-4.579 7.333-5.652 9.057a1.052 1.052 0 01-.894.492h-.443c-.579 0-1.048-.46-1.048-1.029v-8.544c-1.171 1.857-4.768 7.63-5.9 9.477l-.06.096h-2.078V11.14h2.167v.026l-.05 8.837 5.197-8.371c.19-.307.529-.494.895-.494h.412c.866 0 1.57.69 1.572 1.54 0 2.356.011 6.646.011 7.27l5.164-8.319zm-30.609 9.942c-1.73 2.335-4.803 3.295-7.588 2.372-2.785-.924-4.637-3.517-4.572-6.404.065-2.886 2.03-5.397 4.855-6.199 2.824-.802 5.851.29 7.475 2.698l-2.037 1.04c-1.099-1.52-3.057-2.193-4.881-1.676-1.824.517-3.116 2.11-3.22 3.973-.105 1.862 1 3.585 2.756 4.296a4.522 4.522 0 005.039-1.139l2.173 1.04zm41.604.634c-2.43 0-4.439-1.86-4.584-4.242v-.54c.149-2.455 2.271-4.34 4.772-4.241 2.5.098 4.462 2.143 4.411 4.601-.05 2.458-2.093 4.424-4.596 4.424l-.003-.002zm.003-11.119a6.763 6.763 0 00-4.587 1.776v-1.719h-2.125v18.038h2.125v-6.66c2.205 2.02 5.509 2.353 8.085.813 2.576-1.54 3.794-4.575 2.979-7.427-.814-2.852-3.461-4.822-6.477-4.822v.001zm-72.011 2.056v7.879c0 .467.386.846.861.846h2.474v2.084h-2.472c-1.647-.002-2.982-1.313-2.984-2.93l-.008-7.879h-2.29v-2.03h2.29V7.612h2.128v3.51h3.77v2.021h-3.77zm10.297 9.052c-2.539 0-4.596-2.021-4.596-4.514 0-2.494 2.057-4.515 4.596-4.515s4.596 2.021 4.596 4.515a4.474 4.474 0 01-1.346 3.192 4.64 4.64 0 01-3.25 1.322zm4.603-11.052v1.723c-2.668-2.462-6.851-2.377-9.412.192a6.523 6.523 0 000 9.247c2.561 2.568 6.744 2.654 9.412.192v1.455h2.124V11.14l-2.124.002zM48.66 22.195c-2.539 0-4.597-2.02-4.597-4.514 0-2.493 2.058-4.515 4.597-4.515 2.538 0 4.596 2.022 4.596 4.515a4.473 4.473 0 01-1.346 3.192 4.638 4.638 0 01-3.25 1.322zm4.603-11.052v1.722c-2.669-2.462-6.851-2.377-9.413.192a6.523 6.523 0 000 9.247c2.562 2.568 6.744 2.654 9.413.192v1.455h2.123V11.14l-2.123.003z" fill="#fff"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M9.443 7.461v6.606l-7.136 3.998V3.44L9.443 7.46zm2.31 13.88v-5.955l9.557-5.356-2.325-1.308-7.232 4.053V6.8a1.138 1.138 0 00-.58-.984L2.467.915A1.694 1.694 0 00.782.963c-.487.3-.783.825-.782 1.39v16.794a1.632 1.632 0 00.781 1.393c.488.301 1.1.334 1.617.086l.066-.033 6.974-3.91v5.327c.002.407.224.783.583.986l8.95 4.987 2.325-1.309-9.543-5.334z" fill="#03EF62"></path></svg></a><style data-emotion="css 15byibb">.css-15byibb{display:none;margin-left:8px;}@media screen and (min-width: 1200px){.css-15byibb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1oj1utp">.css-1oj1utp{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;display:none;margin-left:8px;}.css-1oj1utp:active,.css-1oj1utp:focus,.css-1oj1utp:hover{-webkit-text-decoration:underline;text-decoration:underline;}@media screen and (min-width: 1200px){.css-1oj1utp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><a class="css-1oj1utp" href="https://datacamp-1.gitbook.io/community-submissions"><style data-emotion="css 16ja2ua">.css-16ja2ua{background-color:#213147;border-radius:4px;color:#03ef62;display:inline-block;font-size:14px;padding-left:4px;padding-right:4px;text-transform:uppercase;text-wrap:nowrap;-webkit-transition:color 0.3s cubic-bezier(0.85, 0, 0.15, 1);transition:color 0.3s cubic-bezier(0.85, 0, 0.15, 1);}.css-16ja2ua:hover{color:#65ff8f;}</style><strong class="css-16ja2ua">Write for us</strong></a></div><style data-emotion="css vsduwy">.css-vsduwy{-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><style data-emotion="css 1s7doqw">.css-1s7doqw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><style data-emotion="css 1pu4ebs">.css-1pu4ebs{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><div class="css-1pu4ebs"><style data-emotion="css swvild">.css-swvild{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}@media screen and (min-width: 992px){.css-swvild{display:none;}}</style><style data-emotion="css 151tu4x">.css-151tu4x{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}@media screen and (min-width: 992px){.css-151tu4x{display:none;}}</style><style data-emotion="css 11mv1vq">.css-11mv1vq{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}@media screen and (min-width: 992px){.css-11mv1vq{display:none;}}</style><div data-testid="mega-menu-open-mobile-menu" class="css-11mv1vq"><div style="position:relative"></div><style data-emotion="css 1uug8y5">.css-1uug8y5{max-height:unset;}</style><style data-emotion="css o4ll7d">.css-o4ll7d{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#ffffff;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:32px;margin-left:8px;padding:0 8px;}@media screen and (min-width: 768px){.css-o4ll7d{height:36px;}}</style><style data-emotion="css 1gobstp">.css-1gobstp{box-sizing:border-box;margin:0;min-width:0;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;display:inline-block;text-align:center;line-height:inherit;-webkit-text-decoration:none;text-decoration:none;font-size:inherit;padding-left:3px;padding-right:3px;padding-top:2px;padding-bottom:2px;color:#ffffff;background-color:#33e;border:0;border-radius:4px;background-color:transparent;cursor:pointer;height:36px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#ffffff;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:32px;margin-left:8px;padding:0 8px;}.css-1gobstp:active,.css-1gobstp:focus,.css-1gobstp:hover{background-color:#213147;border-radius:4px;}.css-1gobstp:focus{outline:0;}@media screen and (min-width: 768px){.css-1gobstp{height:36px;}}</style><button aria-label="Select language" data-trackid="navbar-language-selector" aria-haspopup="menu" id="dropdown-trigger-:R59l6lqkddqkm:" aria-expanded="false" class="css-1gobstp"><style data-emotion="css 1lsep5x">.css-1lsep5x{color:#ffffff;margin-right:6px;}</style><style data-emotion="css 1oabu11">.css-1oabu11{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:#ffffff;margin-right:6px;}</style><svg aria-hidden="true" height="16" width="16" viewBox="0 0 18 18" class="css-1oabu11"><path fill-rule="evenodd" clip-rule="evenodd" d="M9 0a9.001 9.001 0 0 0 0 18A9.001 9.001 0 0 0 9 0Zm5.98 5.222h-2.01a14.073 14.073 0 0 0-.91-2.121c-.12-.222-.242-.434-.353-.636a7.156 7.156 0 0 1 3.273 2.757ZM7.859 3.616C8.455 2.566 9 1.93 9 1.93s.545.637 1.141 1.687c.263.465.536 1 .778 1.606H7.081a13.28 13.28 0 0 1 .778-1.606ZM6.293 2.465c-.111.202-.232.414-.354.636a14.073 14.073 0 0 0-.909 2.121H3.02a7.156 7.156 0 0 1 3.273-2.757Zm-4.212 7.98A6.804 6.804 0 0 1 1.929 9c0-.636.081-1.263.253-1.848h2.343A10.944 10.944 0 0 0 4.364 9c0 .495.04.98.11 1.444H2.082Zm.707 1.929h2.1c.294.97.678 1.828 1.051 2.525.122.222.243.434.354.636a7.044 7.044 0 0 1-3.505-3.161Zm7.353 2.01C9.545 15.434 9 16.07 9 16.07s-.545-.637-1.141-1.687a12.498 12.498 0 0 1-.93-2.01h4.142a12.5 12.5 0 0 1-.93 2.01Zm-3.727-3.94A8.445 8.445 0 0 1 6.293 9c0-.636.07-1.263.202-1.848h5.01c.131.585.202 1.212.202 1.848 0 .495-.04.98-.121 1.444H6.414Zm5.293 5.091c.111-.202.232-.414.354-.636.373-.697.757-1.556 1.05-2.525h2.101a7.045 7.045 0 0 1-3.505 3.161Zm1.818-5.09c.071-.465.111-.95.111-1.445 0-.636-.06-1.263-.161-1.848h2.343c.172.585.253 1.212.253 1.848 0 .495-.05.98-.152 1.444h-2.394Z" fill="currentColor"></path></svg>EN</button><style data-emotion="css ojkr3u">.css-ojkr3u{margin-left:8px;}</style><style data-emotion="css dr8vgr">.css-dr8vgr{box-sizing:border-box;margin:0;min-width:0;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;display:inline-block;text-align:center;line-height:inherit;-webkit-text-decoration:none;text-decoration:none;font-size:inherit;padding-left:3px;padding-right:3px;padding-top:2px;padding-bottom:2px;color:#ffffff;background-color:#33e;border:0;border-radius:4px;background-color:transparent;cursor:pointer;height:36px;margin-left:8px;}.css-dr8vgr:active,.css-dr8vgr:focus,.css-dr8vgr:hover{background-color:#213147;border-radius:4px;}.css-dr8vgr:focus{outline:0;}</style><button aria-label="Open mobile menu" data-trackid="navbar-mobile-menu" class="css-dr8vgr"><svg viewBox="0 0 18 18" aria-hidden="true" height="18" role="img" width="18"><path fill="currentColor" d="M4 6a1 1 0 110-2h10a1 1 0 010 2H4zm0 4a1 1 0 110-2h10a1 1 0 010 2H4zm0 4a1 1 0 010-2h10a1 1 0 010 2H4z" fill-rule="evenodd"></path></svg></button></div><style data-emotion="css uah6cx">.css-uah6cx{display:none;}@media screen and (min-width: 992px){.css-uah6cx{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1h3atgu">.css-1h3atgu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;display:none;}@media screen and (min-width: 992px){.css-1h3atgu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1ek4785">.css-1ek4785{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;display:none;}@media screen and (min-width: 992px){.css-1ek4785{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><div class="css-1ek4785"><div style="position:relative"></div><button aria-label="Select language" data-trackid="navbar-language-selector" aria-haspopup="menu" id="dropdown-trigger-:R5alalqkddqkm:" aria-expanded="false" class="css-1gobstp"><svg aria-hidden="true" height="16" width="16" viewBox="0 0 18 18" class="css-1oabu11"><path fill-rule="evenodd" clip-rule="evenodd" d="M9 0a9.001 9.001 0 0 0 0 18A9.001 9.001 0 0 0 9 0Zm5.98 5.222h-2.01a14.073 14.073 0 0 0-.91-2.121c-.12-.222-.242-.434-.353-.636a7.156 7.156 0 0 1 3.273 2.757ZM7.859 3.616C8.455 2.566 9 1.93 9 1.93s.545.637 1.141 1.687c.263.465.536 1 .778 1.606H7.081a13.28 13.28 0 0 1 .778-1.606ZM6.293 2.465c-.111.202-.232.414-.354.636a14.073 14.073 0 0 0-.909 2.121H3.02a7.156 7.156 0 0 1 3.273-2.757Zm-4.212 7.98A6.804 6.804 0 0 1 1.929 9c0-.636.081-1.263.253-1.848h2.343A10.944 10.944 0 0 0 4.364 9c0 .495.04.98.11 1.444H2.082Zm.707 1.929h2.1c.294.97.678 1.828 1.051 2.525.122.222.243.434.354.636a7.044 7.044 0 0 1-3.505-3.161Zm7.353 2.01C9.545 15.434 9 16.07 9 16.07s-.545-.637-1.141-1.687a12.498 12.498 0 0 1-.93-2.01h4.142a12.5 12.5 0 0 1-.93 2.01Zm-3.727-3.94A8.445 8.445 0 0 1 6.293 9c0-.636.07-1.263.202-1.848h5.01c.131.585.202 1.212.202 1.848 0 .495-.04.98-.121 1.444H6.414Zm5.293 5.091c.111-.202.232-.414.354-.636.373-.697.757-1.556 1.05-2.525h2.101a7.045 7.045 0 0 1-3.505 3.161Zm1.818-5.09c.071-.465.111-.95.111-1.445 0-.636-.06-1.263-.161-1.848h2.343c.172.585.253 1.212.253 1.848 0 .495-.05.98-.152 1.444h-2.394Z" fill="currentColor"></path></svg>EN</button></div></div></div></div></nav><div id="main"><style data-emotion="css 1w9rved">.css-1w9rved{background-color:#ffffff;}</style><style data-emotion="css i36vfu">.css-i36vfu{box-sizing:border-box;margin:0;min-width:0;background-color:#ffffff;}</style><div class="css-i36vfu"><style data-emotion="css 1dj1xmd">.css-1dj1xmd{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#EFEFF5;border-bottom:1px solid rgba(5, 25, 45, .15);height:54px;position:-webkit-sticky;position:sticky;top:0;z-index:700;}</style><style data-emotion="css 4pk2qh">.css-4pk2qh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#EFEFF5;border-bottom:1px solid rgba(5, 25, 45, .15);height:54px;position:-webkit-sticky;position:sticky;top:0;z-index:700;}</style><style data-emotion="css b17bze">.css-b17bze{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#EFEFF5;border-bottom:1px solid rgba(5, 25, 45, .15);height:54px;position:-webkit-sticky;position:sticky;top:0;z-index:700;}</style><nav aria-label="Subnavigation" class="css-b17bze"><style data-emotion="css 1bwgi3a">.css-1bwgi3a{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;padding:16px;position:relative;}</style><style data-emotion="css 146elor">.css-146elor{box-sizing:border-box;margin:0;min-width:0;width:100%;max-width:container;margin-left:auto;margin-right:auto;max-width:1172px;padding-left:16px;padding-right:16px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;padding:16px;position:relative;}</style><div class="css-146elor"><style data-emotion="css me9la6">.css-me9la6{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#05192D;}</style><style data-emotion="css 19d0psd">.css-19d0psd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#05192D;}</style><style data-emotion="css nu4s2c">.css-nu4s2c{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#05192D;}</style><div class="css-nu4s2c"><style data-emotion="css wxlnuv">.css-wxlnuv{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:transparent;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:0;}.css-wxlnuv:active,.css-wxlnuv:focus,.css-wxlnuv:hover{background-color:rgba(5, 25, 45, 0.1);}@media screen and (min-width: 992px){.css-wxlnuv{display:none;}}</style><style data-emotion="css fodiii">.css-fodiii{box-sizing:border-box;margin:0;min-width:0;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;display:inline-block;text-align:center;line-height:inherit;-webkit-text-decoration:none;text-decoration:none;font-size:inherit;padding-left:3px;padding-right:3px;padding-top:2px;padding-bottom:2px;color:#ffffff;background-color:#33e;border:0;border-radius:4px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:transparent;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:0;}.css-fodiii:active,.css-fodiii:focus,.css-fodiii:hover{background-color:rgba(5, 25, 45, 0.1);}@media screen and (min-width: 992px){.css-fodiii{display:none;}}</style><button aria-label="Open subnavigation mobile menu" data-trackid="subnav-mobile" class="css-fodiii"><style data-emotion="css 6uaxj2">.css-6uaxj2{color:#05192D;}</style><style data-emotion="css 12os281">.css-12os281{box-sizing:border-box;margin:0;min-width:0;font-size:1rem;font-weight:700;letter-spacing:2px;line-height:1.2;margin-top:0;text-transform:uppercase;color:#05192D;}</style><span class="css-12os281">tutorials</span><style data-emotion="css 9lclql">.css-9lclql{color:#05192D;margin-left:6px;}</style><svg viewBox="0 0 18 18" aria-hidden="true" class="css-9lclql" height="18" role="img" width="18"><path fill="currentColor" d="M8.244 12.155l-4.95-4.947a1 1 0 111.415-1.415l4.294 4.291 4.293-4.279a.998.998 0 011.413.003c.39.392.388 1.025-.003 1.415l-5.002 4.986a.998.998 0 01-1.46-.054z" fill-rule="evenodd"></path></svg></button><style data-emotion="css 1p6h8s2">.css-1p6h8s2{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;}.css-1p6h8s2>a{-webkit-text-decoration:none;text-decoration:none;}.css-1p6h8s2>a:active,.css-1p6h8s2>a:focus,.css-1p6h8s2>a:hover{-webkit-text-decoration:none;text-decoration:none;background-color:#F7F7FC;border-radius:4px;}</style><div class="css-1p6h8s2"><style data-emotion="css 1sr07cd">.css-1sr07cd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css wst4g3">.css-wst4g3{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-wst4g3:active,.css-wst4g3:focus,.css-wst4g3:hover{-webkit-text-decoration:underline;text-decoration:underline;}</style><a class="css-wst4g3" href="/blog" target="_self" aria-label="Blogs"><style data-emotion="css e4rzz6">.css-e4rzz6{border-radius:4px;color:#05192D;display:none;font-size:16px;font-weight:200;margin-left:2px;padding-left:8px;padding-right:8px;padding-top:4px;padding-bottom:4px;text-transform:capitalize;white-space:nowrap;}@media screen and (min-width: 992px){.css-e4rzz6{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css yrz09x">.css-yrz09x{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;border-radius:4px;color:#05192D;display:none;font-size:16px;font-weight:200;margin-left:2px;padding-left:8px;padding-right:8px;padding-top:4px;padding-bottom:4px;text-transform:capitalize;white-space:nowrap;}@media screen and (min-width: 992px){.css-yrz09x{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><span class="css-yrz09x">Blogs</span></a></div><div class="css-1p6h8s2"><a class="css-wst4g3" href="/tutorial" target="_self" aria-label="Tutorials"><style data-emotion="css ggcbuk">.css-ggcbuk{border-radius:4px;color:#05192D;display:none;font-size:16px;font-weight:800;margin-left:2px;padding-left:8px;padding-right:8px;padding-top:4px;padding-bottom:4px;text-transform:capitalize;white-space:nowrap;}@media screen and (min-width: 992px){.css-ggcbuk{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1ea10ya">.css-1ea10ya{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;border-radius:4px;color:#05192D;display:none;font-size:16px;font-weight:800;margin-left:2px;padding-left:8px;padding-right:8px;padding-top:4px;padding-bottom:4px;text-transform:capitalize;white-space:nowrap;}@media screen and (min-width: 992px){.css-1ea10ya{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><span class="css-1ea10ya">Tutorials</span></a></div><div class="css-1p6h8s2"><a class="css-wst4g3" href="/doc" target="_self" aria-label="docs"><span class="css-yrz09x">docs</span><style data-emotion="css 17rdn1q">.css-17rdn1q{display:none;margin-right:8px;font-weight:800;}@media screen and (min-width: 480px){.css-17rdn1q{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 171hxsb">.css-171hxsb{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;letter-spacing:0.5px;line-height:1;padding:0 4px;text-transform:uppercase;border-radius:4px;background-color:var(--wf-yellow--main, #FCCE0D);color:var(--wf-yellow--main, #FCCE0D);font-size:12px;height:18px;display:none;margin-right:8px;font-weight:800;}.css-171hxsb span{color:var(--wf-yellow--text-on-color, #05192D);}@media screen and (min-width: 480px){.css-171hxsb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1sdmyvn">.css-1sdmyvn{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;letter-spacing:0.5px;line-height:1;padding:0 4px;text-transform:uppercase;border-radius:4px;background-color:var(--wf-yellow--main, #FCCE0D);color:var(--wf-yellow--main, #FCCE0D);font-size:12px;height:18px;display:none;margin-right:8px;font-weight:800;}.css-1sdmyvn span{color:var(--wf-yellow--text-on-color, #05192D);}@media screen and (min-width: 480px){.css-1sdmyvn{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><span class="css-1sdmyvn"><style data-emotion="css 19ist84">.css-19ist84{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:inherit;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;gap:4px;max-width:164px;}</style><span class="css-19ist84"><style data-emotion="css 8uhtka">.css-8uhtka{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}</style><span class="css-8uhtka">new</span></span></span></a></div><div class="css-1p6h8s2"><a class="css-wst4g3" href="/podcast" target="_self" aria-label="Podcasts"><span class="css-yrz09x">Podcasts</span></a></div><div class="css-1p6h8s2"><a class="css-wst4g3" href="/cheat-sheet" target="_self" aria-label="Cheat Sheets"><span class="css-yrz09x">Cheat Sheets</span></a></div><div class="css-1p6h8s2"><a class="css-wst4g3" href="/code-along" target="_self" aria-label="code-alongs"><span class="css-yrz09x">code-alongs</span></a></div></div><style data-emotion="css l7h7cx">.css-l7h7cx{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><style data-emotion="css 1wtf37">.css-1wtf37{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><style data-emotion="css 1yo0l5a">.css-1yo0l5a{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="css-1yo0l5a"><style data-emotion="css uah6cx">.css-uah6cx{display:none;}@media screen and (min-width: 992px){.css-uah6cx{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 7v775s">.css-7v775s{box-sizing:border-box;margin:0;min-width:0;display:none;}@media screen and (min-width: 992px){.css-7v775s{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><div class="css-7v775s"><style data-emotion="css 10dcl03">.css-10dcl03{position:relative;}.css-10dcl03:active >div,.css-10dcl03:focus >div,.css-10dcl03:hover >div{opacity:1;pointer-events:auto;top:28px;visibility:visible;z-index:800;}</style><style data-emotion="css vpie0v">.css-vpie0v{box-sizing:border-box;margin:0;min-width:0;position:relative;}.css-vpie0v:active >div,.css-vpie0v:focus >div,.css-vpie0v:hover >div{opacity:1;pointer-events:auto;top:28px;visibility:visible;z-index:800;}</style><div aria-label="Feature Menu" role="navigation" class="css-vpie0v"><style data-emotion="css 1cdc6ln">.css-1cdc6ln{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin-left:4px;margin-right:4px;}.css-1cdc6ln:active,.css-1cdc6ln:focus,.css-1cdc6ln:hover{background-color:#F7F7FC;}</style><style data-emotion="css 1ij6q7x">.css-1ij6q7x{box-sizing:border-box;margin:0;min-width:0;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;display:inline-block;text-align:center;line-height:inherit;-webkit-text-decoration:none;text-decoration:none;font-size:inherit;padding-left:3px;padding-right:3px;padding-top:2px;padding-bottom:2px;color:#ffffff;background-color:#33e;border:0;border-radius:4px;background-color:transparent;cursor:pointer;height:36px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin-left:4px;margin-right:4px;}.css-1ij6q7x:active,.css-1ij6q7x:focus,.css-1ij6q7x:hover{background-color:#213147;border-radius:4px;}.css-1ij6q7x:focus{outline:0;}.css-1ij6q7x:active,.css-1ij6q7x:focus,.css-1ij6q7x:hover{background-color:#F7F7FC;}</style><button data-trackid="subnav-categories" class="css-1ij6q7x"><style data-emotion="css 1o1q4xw">.css-1o1q4xw{color:#05192D;font-weight:200;}</style><style data-emotion="css fxak73">.css-fxak73{box-sizing:border-box;margin:0;min-width:0;font-size:1rem;line-height:1.5;margin-top:0;color:#05192D;font-weight:200;}</style><span class="css-fxak73">Category</span><style data-emotion="css wcoxuz">.css-wcoxuz{color:#05192D;margin-left:4px;margin-top:1px;}</style><svg viewBox="0 0 18 18" aria-hidden="true" class="css-wcoxuz" height="18" role="img" width="18"><path fill="currentColor" d="M8.244 12.155l-4.95-4.947a1 1 0 111.415-1.415l4.294 4.291 4.293-4.279a.998.998 0 011.413.003c.39.392.388 1.025-.003 1.415l-5.002 4.986a.998.998 0 01-1.46-.054z" fill-rule="evenodd"></path></svg></button><style data-emotion="css 1g2lhf1">.css-1g2lhf1{opacity:0;pointer-events:none;position:absolute;padding-top:18px;right:-18px;top:28px;-webkit-transition:visibility .15s,opacity .3s ease,top .25s;transition:visibility .15s,opacity .3s ease,top .25s;visibility:hidden;}</style><style data-emotion="css 1ad87ja">.css-1ad87ja{box-sizing:border-box;margin:0;min-width:0;opacity:0;pointer-events:none;position:absolute;padding-top:18px;right:-18px;top:28px;-webkit-transition:visibility .15s,opacity .3s ease,top .25s;transition:visibility .15s,opacity .3s ease,top .25s;visibility:hidden;}</style><div class="css-1ad87ja"><style data-emotion="css 1nty8jf">.css-1nty8jf{background-color:#ffffff;border-radius:4px;box-shadow:0px 0px 1px rgba(5, 25, 45, 0.3),0px 8px 12px -4px rgba(5, 25, 45, 0.3);color:#05192D;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;overflow:hidden;}</style><style data-emotion="css 1lkyime">.css-1lkyime{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background-color:#ffffff;border-radius:4px;box-shadow:0px 0px 1px rgba(5, 25, 45, 0.3),0px 8px 12px -4px rgba(5, 25, 45, 0.3);color:#05192D;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;overflow:hidden;}</style><style data-emotion="css lgu5yw">.css-lgu5yw{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background-color:#ffffff;border-radius:4px;box-shadow:0px 0px 1px rgba(5, 25, 45, 0.3),0px 8px 12px -4px rgba(5, 25, 45, 0.3);color:#05192D;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;overflow:hidden;}</style><div class="css-lgu5yw"><style data-emotion="css 2jb5j6">.css-2jb5j6{border-bottom:1px solid;border-color:#e5e1da;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}</style><style data-emotion="css 10r4mp">.css-10r4mp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;border-bottom:1px solid;border-color:#e5e1da;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}</style><style data-emotion="css 1fx7j4f">.css-1fx7j4f{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;border-bottom:1px solid;border-color:#e5e1da;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}</style><div class="css-1fx7j4f"><style data-emotion="css pkhv09">.css-pkhv09{background-color:#efefef;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;padding:24px;padding-right:21px;width:250px;}</style><style data-emotion="css sski9t">.css-sski9t{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background-color:#efefef;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;padding:24px;padding-right:21px;width:250px;}</style><style data-emotion="css 196qkq0">.css-196qkq0{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background-color:#efefef;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;padding:24px;padding-right:21px;width:250px;}</style><div class="css-196qkq0"><style data-emotion="css 1frwd2h">.css-1frwd2h{color:#05192D;cursor:auto;font-size:1.25rem;font-weight:700;line-height:1.2;pointer-events:auto;-webkit-text-decoration:none;text-decoration:none;}.css-1frwd2h:active,.css-1frwd2h:focus,.css-1frwd2h:hover{cursor:auto;-webkit-text-decoration:none;text-decoration:none;}</style><style data-emotion="css ozkl8y">.css-ozkl8y{box-sizing:border-box;margin:0;min-width:0;color:#05192D;cursor:auto;font-size:1.25rem;font-weight:700;line-height:1.2;pointer-events:auto;-webkit-text-decoration:none;text-decoration:none;}.css-ozkl8y:active,.css-ozkl8y:focus,.css-ozkl8y:hover{cursor:auto;-webkit-text-decoration:none;text-decoration:none;}</style><div tabindex="0" class="css-ozkl8y"><style data-emotion="css bgl0b">.css-bgl0b{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}</style><style data-emotion="css ionmi9">.css-ionmi9{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}</style><style data-emotion="css er0ssy">.css-er0ssy{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}</style><div class="css-er0ssy"><style data-emotion="css 17akths">.css-17akths{background-color:transparent;color:#ff6ea9;font-size:0.75rem;letter-spacing:1.5px;text-transform:uppercase;width:-webkit-max-content;width:-moz-max-content;width:max-content;}</style><style data-emotion="css j15h65">.css-j15h65{box-sizing:border-box;margin:0;min-width:0;display:inline-block;vertical-align:baseline;font-size:0;font-weight:700;white-space:nowrap;padding-left:1px;padding-right:1px;border-radius:2px;color:#ffffff;background-color:#33e;background-color:transparent;color:#ff6ea9;font-size:0.75rem;letter-spacing:1.5px;text-transform:uppercase;width:-webkit-max-content;width:-moz-max-content;width:max-content;}</style><div class="css-j15h65">Category</div><style data-emotion="css 13xhtad">.css-13xhtad{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css 13b9p20">.css-13b9p20{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css dou6ry">.css-dou6ry{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><div class="css-dou6ry"> <!-- -->Technologies<!-- --> </div></div><style data-emotion="css vme3eq">.css-vme3eq{color:#65707C;font-weight:400;line-height:22px;margin-top:8px;}</style><style data-emotion="css 5stbd">.css-5stbd{box-sizing:border-box;margin:0;min-width:0;font-size:0.875rem;line-height:1.5;margin-top:0;color:#65707C;font-weight:400;line-height:22px;margin-top:8px;}</style><p class="css-5stbd">Discover content by tools and technology</p></div></div><style data-emotion="css a1vd89">.css-a1vd89{padding:24px;width:100%;}</style><style data-emotion="css 134yk8s">.css-134yk8s{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:24px;width:100%;}</style><style data-emotion="css 1oixegt">.css-1oixegt{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:24px;width:100%;}</style><div class="css-1oixegt"><style data-emotion="css orlf1r">.css-orlf1r{-webkit-column-gap:48px;column-gap:48px;display:grid;grid-template-columns:1fr 1fr;pointer-events:auto;width:455px;}.css-orlf1r>a{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#05192d;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;padding-bottom:6px;padding-top:8px;-webkit-text-decoration:none;text-decoration:none;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;}.css-orlf1r>a:active,.css-orlf1r>a:focus,.css-orlf1r>a:hover{-webkit-text-decoration:none;text-decoration:none;}.css-orlf1r>a:active>span:first-of-type,.css-orlf1r>a:focus>span:first-of-type,.css-orlf1r>a:hover>span:first-of-type{-webkit-text-decoration:underline;text-decoration:underline;}</style><style data-emotion="css xggfa2">.css-xggfa2{box-sizing:border-box;margin:0;min-width:0;-webkit-column-gap:48px;column-gap:48px;display:grid;grid-template-columns:1fr 1fr;pointer-events:auto;width:455px;}.css-xggfa2>a{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#05192d;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;padding-bottom:6px;padding-top:8px;-webkit-text-decoration:none;text-decoration:none;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;}.css-xggfa2>a:active,.css-xggfa2>a:focus,.css-xggfa2>a:hover{-webkit-text-decoration:none;text-decoration:none;}.css-xggfa2>a:active>span:first-of-type,.css-xggfa2>a:focus>span:first-of-type,.css-xggfa2>a:hover>span:first-of-type{-webkit-text-decoration:underline;text-decoration:underline;}</style><div data-test-id="menu-list" class="css-xggfa2"><a tabindex="0" href="/tutorial/category/ai"><style data-emotion="css 1dryz1c">.css-1dryz1c{font-size:0.875rem;font-weight:400;}</style><style data-emotion="css 16ukq0f">.css-16ukq0f{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;font-size:0.875rem;font-weight:400;}</style><span class="css-16ukq0f">Artificial Intelligence (AI)</span></a><a tabindex="0" href="/tutorial/category/aws"><span class="css-16ukq0f">AWS</span></a><a tabindex="0" href="/tutorial/category/learn-business-intelligence"><span class="css-16ukq0f">Business Intelligence</span></a><a tabindex="0" href="/tutorial/category/chatgpt"><span class="css-16ukq0f">ChatGPT</span></a><a tabindex="0" href="/tutorial/category/excel"><span class="css-16ukq0f">Excel</span></a><a tabindex="0" href="/tutorial/category/generative-ai"><span class="css-16ukq0f">Generative AI</span></a><a tabindex="0" href="/tutorial/category/git"><span class="css-16ukq0f">Git</span></a><a tabindex="0" href="/tutorial/category/julia"><span class="css-16ukq0f">Julia</span></a><a tabindex="0" href="/tutorial/category/large-language-models"><span class="css-16ukq0f">Large Language Models</span></a><a tabindex="0" href="/tutorial/category/microsoft-azure"><span class="css-16ukq0f">Microsoft Azure</span></a><a tabindex="0" href="/tutorial/category/OpenAI"><span class="css-16ukq0f">OpenAI</span></a><a tabindex="0" href="/tutorial/category/power-bi"><span class="css-16ukq0f">Power BI</span></a><a tabindex="0" href="/tutorial/category/python"><span class="css-16ukq0f">Python</span></a><a tabindex="0" href="/tutorial/category/r-programming"><span class="css-16ukq0f">R Programming</span></a><a tabindex="0" href="/tutorial/category/scala"><span class="css-16ukq0f">Scala</span></a><a tabindex="0" href="/tutorial/category/snowflake"><span class="css-16ukq0f">Snowflake</span></a><a tabindex="0" href="/tutorial/category/spreadsheets"><span class="css-16ukq0f">Spreadsheets</span></a><a tabindex="0" href="/tutorial/category/sql"><span class="css-16ukq0f">SQL</span></a><a tabindex="0" href="/tutorial/category/tableau"><span class="css-16ukq0f">Tableau</span></a></div></div></div><div class="css-1fx7j4f"><div class="css-196qkq0"><div tabindex="0" class="css-ozkl8y"><div class="css-er0ssy"><div class="css-j15h65">Category</div><div class="css-dou6ry"> <!-- -->Topics<!-- --> </div></div><p class="css-5stbd">Discover content by data science topics</p></div></div><div class="css-1oixegt"><div data-test-id="menu-list" class="css-xggfa2"><a tabindex="0" href="/tutorial/category/big-data"><span class="css-16ukq0f">Big Data</span></a><a tabindex="0" href="/tutorial/category/career-services"><span class="css-16ukq0f">Career Services</span></a><a tabindex="0" href="/tutorial/category/data-analysis"><span class="css-16ukq0f">Data Analysis</span></a><a tabindex="0" href="/tutorial/category/data-engineering"><span class="css-16ukq0f">Data Engineering</span></a><a tabindex="0" href="/tutorial/category/data-literacy"><span class="css-16ukq0f">Data Literacy</span></a><a tabindex="0" href="/tutorial/category/data-science"><span class="css-16ukq0f">Data Science</span></a><a tabindex="0" href="/tutorial/category/data-visualization"><span class="css-16ukq0f">Data Visualization</span></a><a tabindex="0" href="/tutorial/category/datalab"><span class="css-16ukq0f">DataLab</span></a><a tabindex="0" href="/tutorial/category/deep-learning"><span class="css-16ukq0f">Deep Learning</span></a><a tabindex="0" href="/tutorial/category/machine-learning"><span class="css-16ukq0f">Machine Learning</span></a><a tabindex="0" href="/tutorial/category/mlops"><span class="css-16ukq0f">MLOps</span></a></div></div></div></div></div></div><style data-emotion="css hh2wsv">.css-hh2wsv{background:#ffffff;border:2px solid rgba(5, 25, 45, 0.6);border-radius:4px;color:#05192D;font-size:14px;min-width:-webkit-fit-content;min-width:-moz-fit-content;min-width:fit-content;padding:4px 10px;-webkit-text-decoration:none;text-decoration:none;white-space:nowrap;}.css-hh2wsv:active,.css-hh2wsv:focus,.css-hh2wsv:hover{-webkit-text-decoration:none;text-decoration:none;background-color:#F7F7FC;}</style><style data-emotion="css 1ijvzwh">.css-1ijvzwh{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;background:#ffffff;border:2px solid rgba(5, 25, 45, 0.6);border-radius:4px;color:#05192D;font-size:14px;min-width:-webkit-fit-content;min-width:-moz-fit-content;min-width:fit-content;padding:4px 10px;-webkit-text-decoration:none;text-decoration:none;white-space:nowrap;}.css-1ijvzwh:active,.css-1ijvzwh:focus,.css-1ijvzwh:hover{-webkit-text-decoration:underline;text-decoration:underline;}.css-1ijvzwh:active,.css-1ijvzwh:focus,.css-1ijvzwh:hover{-webkit-text-decoration:none;text-decoration:none;background-color:#F7F7FC;}</style><a data-trackid="media-center-demo-request" href="/business" target="_blank" class="css-1ijvzwh">Request a Demo</a></div><style data-emotion="css swvild">.css-swvild{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}@media screen and (min-width: 992px){.css-swvild{display:none;}}</style><style data-emotion="css n78vb0">.css-n78vb0{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}@media screen and (min-width: 992px){.css-n78vb0{display:none;}}</style><div class="css-n78vb0"><style data-emotion="css z9a9z9">.css-z9a9z9{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}.css-z9a9z9:active,.css-z9a9z9:focus,.css-z9a9z9:hover{background-color:rgba(5, 25, 45, 0.1);}</style><style data-emotion="css 11eqg9o">.css-11eqg9o{box-sizing:border-box;margin:0;min-width:0;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;display:inline-block;text-align:center;line-height:inherit;-webkit-text-decoration:none;text-decoration:none;font-size:inherit;padding-left:3px;padding-right:3px;padding-top:2px;padding-bottom:2px;color:#ffffff;background-color:#33e;border:0;border-radius:4px;background-color:transparent;cursor:pointer;height:36px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}.css-11eqg9o:active,.css-11eqg9o:focus,.css-11eqg9o:hover{background-color:#213147;border-radius:4px;}.css-11eqg9o:focus{outline:0;}.css-11eqg9o:active,.css-11eqg9o:focus,.css-11eqg9o:hover{background-color:rgba(5, 25, 45, 0.1);}</style><button data-trackid="subnav-categories" class="css-11eqg9o"><style data-emotion="css 1xf4f53">.css-1xf4f53{color:#05192D;font-weight:400;}</style><style data-emotion="css dso331">.css-dso331{box-sizing:border-box;margin:0;min-width:0;font-size:1rem;line-height:1.5;margin-top:0;color:#05192D;font-weight:400;}</style><span class="css-dso331">Category</span><svg viewBox="0 0 18 18" aria-hidden="true" class="css-9lclql" height="18" role="img" width="18"><path fill="currentColor" d="M8.244 12.155l-4.95-4.947a1 1 0 111.415-1.415l4.294 4.291 4.293-4.279a.998.998 0 011.413.003c.39.392.388 1.025-.003 1.415l-5.002 4.986a.998.998 0 01-1.46-.054z" fill-rule="evenodd"></path></svg></button></div><style data-emotion="css 1hiw4vm">.css-1hiw4vm{padding:8px;margin-left:4px;}.css-1hiw4vm:hover{background-color:#F7F7FC;}.css-1hiw4vm span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css 102d7q">.css-102d7q{background-color:rgba(255, 255, 255, 0.01);border-radius:4px;-webkit-box-decoration-break:clone;box-decoration-break:clone;color:var(--wf-text--link, #0065D1);display:inline;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-size:inherit;font-weight:800;line-height:inherit;outline:0;-webkit-text-decoration:underline;text-decoration:underline;text-decoration-color:transparent;text-decoration-thickness:1.25px;-webkit-transition:box-shadow 125ms ease-out,background-color 125ms ease-out,text-decoration-color 125ms ease-out;transition:box-shadow 125ms ease-out,background-color 125ms ease-out,text-decoration-color 125ms ease-out;font-size:16px;line-height:1.5;padding:8px;margin-left:4px;}.css-102d7q:hover{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-102d7q:hover{box-shadow:0 0 0 2px var(--wf-bg--hover, rgba(48, 57, 105, 0.06));text-decoration-color:var(--wf-text--link, #0065D1);}.css-102d7q:hover{background-color:#F7F7FC;}.css-102d7q span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><a aria-label="Navigate to Search Resources Page" href="/search-resources" class="css-102d7q"><span class="css-0"><style data-emotion="css 6su6fj">.css-6su6fj{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}</style><svg aria-hidden="true" height="18" width="18" viewBox="0 0 18 18" color="#05192D" class="css-6su6fj"><path fill="currentColor" d="m13.26 12.303 4.408 3.953a1 1 0 0 1-1.336 1.488L11.78 13.66a7.5 7.5 0 1 1 1.482-1.358ZM7.5 13a5.5 5.5 0 1 0 0-11 5.5 5.5 0 0 0 0 11Z"></path></svg></span></a></div></div></nav><style data-emotion="css j57ii">.css-j57ii{padding:0 16px;}@media screen and (min-width: 1200px){.css-j57ii{padding:0;}}</style><style data-emotion="css xr19k8">.css-xr19k8{box-sizing:border-box;margin:0;min-width:0;padding:0 16px;}@media screen and (min-width: 1200px){.css-xr19k8{padding:0;}}</style><div id="main-container" class="css-xr19k8"><style data-emotion="css tcaht5">.css-tcaht5{display:none;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:100%;}@media screen and (min-width: 1200px){.css-tcaht5{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css kbcsae">.css-kbcsae{left:0;overflow:hidden;pointer-events:none;position:absolute;right:0;display:none;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:100%;}.css-kbcsae >svg{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}@media screen and (min-width: 1200px){.css-kbcsae{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css rk52od">.css-rk52od{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;overflow:hidden;pointer-events:none;position:absolute;right:0;display:none;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:100%;}.css-rk52od >svg{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}@media screen and (min-width: 1200px){.css-rk52od{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css k13und">.css-k13und{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;left:0;overflow:hidden;pointer-events:none;position:absolute;right:0;display:none;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:100%;}.css-k13und >svg{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}@media screen and (min-width: 1200px){.css-k13und{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><div aria-hidden="true" class="ie-vsWrapper css-k13und"><style data-emotion="css 1p43dcl">.css-1p43dcl{margin-top:48px;}</style><svg xmlns="http://www.w3.org/2000/svg" width="158" height="79" fill="none" class="css-1p43dcl"><clipPath id="media-left_svg__a"><path d="M0 0h158v79H0z"></path></clipPath><g clip-path="url(#media-left_svg__a)"><path fill="#7933ff" d="M-369 0H39v79h-408z"></path><path fill="#974dff" d="M-148 0h306v45h-306z"></path></g></svg><style data-emotion="css 12j9cj">.css-12j9cj{margin-top:155px;}</style><svg xmlns="http://www.w3.org/2000/svg" width="188" height="60" fill="none" class="css-12j9cj"><clipPath id="media-right_svg__a"><path d="M0 60h188V0H0z"></path></clipPath><g clip-path="url(#media-right_svg__a)"><path fill="#974dff" d="M76 0h484v36H76z"></path><path fill="#06bdfc" d="M0 60h384V36H0z"></path></g></svg></div><style data-emotion="css 1if6clc">.css-1if6clc{max-width:696px;margin-bottom:32px;margin-left:auto;margin-right:auto;width:100%;}@media screen and (min-width: 768px){.css-1if6clc{margin-bottom:57px;}}</style><style data-emotion="css dwmki1">.css-dwmki1{box-sizing:border-box;margin:0;min-width:0;max-width:696px;margin-bottom:32px;margin-left:auto;margin-right:auto;width:100%;}@media screen and (min-width: 768px){.css-dwmki1{margin-bottom:57px;}}</style><div class="css-dwmki1"><style data-emotion="css 1kyp1ct">.css-1kyp1ct{background-color:transparent;color:#213147;position:relative;padding-top:16px;width:100%;}.css-1kyp1ct>div{padding-left:0;padding-right:0;}@media screen and (min-width: 1200px){.css-1kyp1ct{padding-top:56px;}}</style><style data-emotion="css qru17p">.css-qru17p{background-color:#05192d;background-color:transparent;color:#213147;position:relative;padding-top:16px;width:100%;}.css-qru17p>div{padding-left:0;padding-right:0;}@media screen and (min-width: 1200px){.css-qru17p{padding-top:56px;}}</style><style data-emotion="css fvnsey">.css-fvnsey{box-sizing:border-box;margin:0;min-width:0;background-color:#05192d;background-color:transparent;color:#213147;position:relative;padding-top:16px;width:100%;}.css-fvnsey>div{padding-left:0;padding-right:0;}@media screen and (min-width: 1200px){.css-fvnsey{padding-top:56px;}}</style><header class="css-fvnsey"><style data-emotion="css upiy67">.css-upiy67{box-sizing:border-box;margin:0;min-width:0;width:100%;max-width:container;margin-left:auto;margin-right:auto;max-width:1172px;padding-left:16px;padding-right:16px;}</style><div class="css-upiy67"><style data-emotion="css 10119ac">.css-10119ac{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin-left:-8px;}</style><style data-emotion="css 12sbgof">.css-12sbgof{box-sizing:border-box;margin:0;min-width:0;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin-left:-8px;}</style><div class="css-12sbgof"><style data-emotion="css 1xe9zab">.css-1xe9zab{color:#626D79;font-size:14px;font-weight:400;padding-left:8px;padding-right:8px;padding-top:10px;padding-bottom:10px;-webkit-text-decoration:none;text-decoration:none;}</style><style data-emotion="css 1llavj6">.css-1llavj6{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;color:#626D79;font-size:14px;font-weight:400;padding-left:8px;padding-right:8px;padding-top:10px;padding-bottom:10px;-webkit-text-decoration:none;text-decoration:none;}.css-1llavj6:active,.css-1llavj6:focus,.css-1llavj6:hover{-webkit-text-decoration:underline;text-decoration:underline;}</style><a data-trackid="breadcrumb-Home" href="https://www.datacamp.com" class="css-1llavj6">Home</a><style data-emotion="css 1f0j76p">.css-1f0j76p{color:#626D79;height:6px;width:4px;}</style><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11 16" class="css-1f0j76p"><path fill="currentColor" d="M10.54 7.18c.46.41.47 1.14 0 1.56L2.37 15.9l-1.37-1.57 7.26-6.36L1 1.56 2.39 0z"></path></svg><a data-trackid="breadcrumb-Tutorials" href="https://www.datacamp.com/tutorial" class="css-1llavj6">Tutorials</a><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11 16" class="css-1f0j76p"><path fill="currentColor" d="M10.54 7.18c.46.41.47 1.14 0 1.56L2.37 15.9l-1.37-1.57 7.26-6.36L1 1.56 2.39 0z"></path></svg><a data-trackid="breadcrumb-Python" href="https://www.datacamp.com/tutorial/category/python" class="css-1llavj6">Python</a></div><style data-emotion="css 13bdew3">.css-13bdew3{-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}@media screen and (min-width: 992px){.css-13bdew3{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}}</style><style data-emotion="css 1hc2jvx">.css-1hc2jvx{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}@media screen and (min-width: 992px){.css-1hc2jvx{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}}</style><style data-emotion="css 1lrls0v">.css-1lrls0v{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}@media screen and (min-width: 992px){.css-1lrls0v{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}}</style><div class="css-1lrls0v"><style data-emotion="css 104872g">.css-104872g{color:#05192d;font-size:2rem;line-height:125%;margin-bottom:16px;margin-top:0;text-wrap:balance;}@media screen and (min-width: 768px){.css-104872g{font-size:2.5rem;}}</style><style data-emotion="css 1rnk3jc">.css-1rnk3jc{box-sizing:border-box;margin:0;min-width:0;font-size:2rem;font-weight:700;letter-spacing:-1px;line-height:1.2;margin:0;color:#05192d;font-size:2rem;line-height:125%;margin-bottom:16px;margin-top:0;text-wrap:balance;}@media screen and (min-width: 992px){.css-1rnk3jc{font-size:2.5rem;}}@media screen and (min-width: 768px){.css-1rnk3jc{font-size:2.5rem;}}</style><h1 data-testid="header-content" class="css-1rnk3jc">Apache Spark Tutorial: ML with PySpark</h1></div><style data-emotion="css tnpv4p">.css-tnpv4p{max-width:696px;margin-bottom:16px;margin-left:auto;margin-right:auto;width:100%;}</style><style data-emotion="css 1coesg8">.css-1coesg8{box-sizing:border-box;margin:0;min-width:0;max-width:696px;margin-bottom:16px;margin-left:auto;margin-right:auto;width:100%;}</style><div class="css-1coesg8"><style data-emotion="css qbm0c9">.css-qbm0c9{font-size:20px;font-weight:normal;line-height:135%;}</style><style data-emotion="css caxq9m">.css-caxq9m{box-sizing:border-box;margin:0;min-width:0;font-size:1.5rem;letter-spacing:-0.5px;line-height:1.2;margin-top:0;font-size:20px;font-weight:normal;line-height:135%;}</style><div class="css-caxq9m">Apache Spark tutorial introduces you to big data processing, analysis and ML with PySpark.</div></div><style data-emotion="css tfz7as">.css-tfz7as{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:start;justify-content:start;}</style><style data-emotion="css rql16b">.css-rql16b{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:start;justify-content:start;}</style><style data-emotion="css 156wl3l">.css-156wl3l{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:start;justify-content:start;}</style><div class="css-156wl3l"><style data-emotion="css g1hqfm">.css-g1hqfm{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:#ffffff;border:2px solid rgba(5, 25, 45, 0.6);border-radius:4px;bottom:8px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-weight:700;left:6px;margin:0;margin-right:16px;position:fixed;padding-left:16px;padding-right:16px;padding-top:6px;padding-bottom:6px;text-align:center;-webkit-text-decoration:none;text-decoration:none;z-index:300;}@media screen and (min-width: 1200px){.css-g1hqfm{display:none;}}</style><button class="css-g1hqfm"><style data-emotion="css vkwxc7">.css-vkwxc7{margin-right:0;width:16px;}</style><svg viewBox="0 0 18 18" aria-hidden="false" class="css-vkwxc7" height="18" role="img" width="18"><title>List</title><path fill="currentColor" d="M6 6a1 1 0 110-2h10a1 1 0 010 2H6zm0 4a1 1 0 110-2h10a1 1 0 010 2H6zm0 4a1 1 0 010-2h10a1 1 0 010 2H6zM1 5a1 1 0 112 0 1 1 0 01-2 0zm0 4a1 1 0 112 0 1 1 0 01-2 0zm0 4a1 1 0 112 0 1 1 0 01-2 0z" fill-rule="evenodd"></path></svg> </button><style data-emotion="css lqbrql">.css-lqbrql{color:#626D79;font-size:0.875rem;font-weight:400;line-height:130%;}</style><style data-emotion="css 2ogyk1">.css-2ogyk1{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;color:#626D79;font-size:0.875rem;font-weight:400;line-height:130%;}</style><style data-emotion="css u5f2co">.css-u5f2co{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;color:#626D79;font-size:0.875rem;font-weight:400;line-height:130%;}</style><div class="css-u5f2co"><span>Jul 28, 2017</span> <span> · <!-- -->34 min<!-- --> <!-- -->read</span></div></div></div></header></div><style data-emotion="css zns64m">.css-zns64m{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;gap:0;margin:auto;max-width:696px;}@media screen and (min-width: 1200px){.css-zns64m{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:38px;max-width:100%;}}@media screen and (min-width: 1366px){.css-zns64m{gap:56px;}}</style><style data-emotion="css 1q43bnb">.css-1q43bnb{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;gap:0;margin:auto;max-width:696px;}@media screen and (min-width: 1200px){.css-1q43bnb{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:38px;max-width:100%;}}@media screen and (min-width: 1366px){.css-1q43bnb{gap:56px;}}</style><div class="css-1q43bnb"><style data-emotion="css 13zssoz">.css-13zssoz{-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;height:100%;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:start;justify-content:start;margin-bottom:0;margin-left:0;margin-right:0;overflow-x:hidden;overflow-y:auto;padding-left:0;position:static;padding-right:0;width:100%;}@media screen and (min-width: 1200px){.css-13zssoz{margin-left:16px;margin-right:0;padding-left:0;position:-webkit-sticky;position:sticky;padding-right:16px;width:180px;}}</style><aside class="css-13zssoz"><style data-emotion="css 1bi8qhr">.css-1bi8qhr{-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:100%;}@media screen and (min-width: 768px){.css-1bi8qhr{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}}@media screen and (min-width: 1200px){.css-1bi8qhr{width:180px;}}</style><style data-emotion="css cwoz0q">.css-cwoz0q{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:100%;}@media screen and (min-width: 768px){.css-cwoz0q{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}}@media screen and (min-width: 1200px){.css-cwoz0q{width:180px;}}</style><style data-emotion="css 1ng2sso">.css-1ng2sso{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:100%;}@media screen and (min-width: 768px){.css-1ng2sso{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}}@media screen and (min-width: 1200px){.css-1ng2sso{width:180px;}}</style><div class="css-1ng2sso"><style data-emotion="css erh6a1">.css-erh6a1{-webkit-flex:1;-ms-flex:1;flex:1;}.css-erh6a1 a:active,.css-erh6a1 a:focus,.css-erh6a1 a:hover{-webkit-text-decoration:none;text-decoration:none;}</style><style data-emotion="css 106n4fp">.css-106n4fp{box-sizing:border-box;margin:0;min-width:0;-webkit-flex:1;-ms-flex:1;flex:1;}.css-106n4fp a:active,.css-106n4fp a:focus,.css-106n4fp a:hover{-webkit-text-decoration:none;text-decoration:none;}</style><div class="css-106n4fp"><style data-emotion="css 8ruc21">.css-8ruc21{-webkit-align-items:start;-webkit-box-align:start;-ms-flex-align:start;align-items:start;display:none;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding-left:8px;width:180px;}@media screen and (min-width: 1200px){.css-8ruc21{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1f9blfu">.css-1f9blfu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:start;-webkit-box-align:start;-ms-flex-align:start;align-items:start;display:none;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding-left:8px;width:180px;}@media screen and (min-width: 1200px){.css-1f9blfu{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1tj3ecy">.css-1tj3ecy{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:start;-webkit-box-align:start;-ms-flex-align:start;align-items:start;display:none;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding-left:8px;width:180px;}@media screen and (min-width: 1200px){.css-1tj3ecy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><div class="css-1tj3ecy"><style data-emotion="css 1f82g4n">.css-1f82g4n{font-size:12px;font-weight:800;letter-spacing:1.5px;line-height:15px;padding-bottom:20px;text-transform:uppercase;}</style><style data-emotion="css 1qsthh7">.css-1qsthh7{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;font-size:12px;font-weight:800;letter-spacing:1.5px;line-height:15px;padding-bottom:20px;text-transform:uppercase;}</style><span class="css-1qsthh7">Share</span><style data-emotion="css 1y7vqrm">.css-1y7vqrm{-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:152px;}</style><style data-emotion="css 16sbn6q">.css-16sbn6q{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:152px;}</style><style data-emotion="css 1a9812i">.css-1a9812i{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;width:152px;}</style><div class="css-1a9812i"><style data-emotion="css xlmvza">.css-xlmvza{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;}.css-xlmvza:active,.css-xlmvza:focus,.css-xlmvza:hover{-webkit-text-decoration:underline;text-decoration:underline;}</style><a data-heap-redact-attributes="href" data-trackid="social-media-linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fapache-spark-tutorial-machine-learning&amp;title=Apache%20Spark%20Tutorial%3A%20ML%20with%20PySpark&amp;summary=Apache%20Spark%20Tutorial%3A%20ML%20with%20PySpark&amp;source=https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fapache-spark-tutorial-machine-learning" rel="noreferrer" target="_blank" class="css-xlmvza"><style data-emotion="css ic8852">.css-ic8852{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:50%;height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:32px;background-color:#EFEFF5;}.css-ic8852:hover{background-color:#E1E1E8;cursor:pointer;}</style><style data-emotion="css 12xgnf1">.css-12xgnf1{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:50%;height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:32px;background-color:#EFEFF5;}.css-12xgnf1:hover{background-color:#E1E1E8;cursor:pointer;}</style><style data-emotion="css 1qbqgqz">.css-1qbqgqz{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:50%;height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:32px;background-color:#EFEFF5;}.css-1qbqgqz:hover{background-color:#E1E1E8;cursor:pointer;}</style><div class="css-1qbqgqz"><style data-emotion="css sen2o3">.css-sen2o3{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;color:#6B6A67;height:14px;width:14px;}</style><svg viewBox="0 0 18 18" aria-hidden="false" class="css-sen2o3" height="18" role="img" width="18"><title>LinkedIn</title><path fill="currentColor" d="M4 2c0 1.1-.7 2-2 2-1.2 0-2-.9-2-1.9C0 1 .8 0 2 0s2 .9 2 2zM0 18h4V5H0v13zM13.6 5.2c-2.1 0-3.3 1.2-3.8 2h-.1l-.2-1.7H5.9c0 1.1.1 2.4.1 3.9V18h4v-7.1c0-.4 0-.7.1-1 .3-.7.8-1.6 1.9-1.6 1.4 0 2 1.2 2 2.8V18h4v-7.4c0-3.7-1.9-5.4-4.4-5.4z" fill-rule="evenodd"></path></svg></div></a><a data-heap-redact-attributes="href" data-trackid="social-media-facebook" href="https://www.facebook.com/dialog/share?app_id=408074879325153&amp;display=page&amp;href=https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fapache-spark-tutorial-machine-learning&amp;redirect_uri=https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fapache-spark-tutorial-machine-learning&amp;quote=Apache%20Spark%20Tutorial%3A%20ML%20with%20PySpark&amp;hashtag=%23Datacamp" rel="noreferrer" target="_blank" class="css-xlmvza"><div class="css-1qbqgqz"><style data-emotion="css 1oqmzj9">.css-1oqmzj9{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;color:#6B6A67;height:16px;width:16px;}</style><svg viewBox="0 0 18 18" aria-hidden="false" class="css-1oqmzj9" height="18" role="img" width="18"><title>Facebook</title><path d="M18 9c0-4.968-4.032-9-9-9S0 4.032 0 9a8.99 8.99 0 007.6 8.889v-6.294H5.302V9H7.6V7.016c0-2.252 1.335-3.503 3.393-3.503.982 0 2.002.185 2.002.185v2.197h-1.13c-1.122 0-1.475.704-1.475 1.409V8.99h2.503l-.399 2.595H10.39v6.294A8.974 8.974 0 0018 9z" fill="currentColor" fill-rule="evenodd"></path></svg></div></a><a data-heap-redact-attributes="href" data-trackid="social-media-twitter" href="https://twitter.com/intent/tweet?text=Apache%20Spark%20Tutorial%3A%20ML%20with%20PySpark&amp;url=https%3A%2F%2Fwww.datacamp.com%2Ftutorial%2Fapache-spark-tutorial-machine-learning&amp;via=dataCamp" rel="noreferrer" target="_blank" class="css-xlmvza"><div class="css-1qbqgqz"><style data-emotion="css 1ej0pg1">.css-1ej0pg1{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;color:#6B6A67;height:18px;width:18px;}</style><svg viewBox="0 0 18 18" aria-hidden="false" class="css-1ej0pg1" height="18" role="img" width="18"><title>Twitter</title><path fill="currentColor" d="M15.864 5.35c.008.146.01.293.01.439 0 4.491-3.417 9.668-9.666 9.668A9.61 9.61 0 011 13.93c.266.031.536.047.81.047a6.819 6.819 0 004.22-1.453 3.4 3.4 0 01-3.174-2.36 3.472 3.472 0 001.534-.058 3.4 3.4 0 01-2.725-3.332V6.73a3.39 3.39 0 001.54.426A3.397 3.397 0 012.152 2.62a9.644 9.644 0 007.003 3.55A3.396 3.396 0 0112.466 2c.977 0 1.86.411 2.48 1.072a6.842 6.842 0 002.157-.825 3.41 3.41 0 01-1.494 1.88 6.796 6.796 0 001.951-.535 6.86 6.86 0 01-1.696 1.758z" fill-rule="evenodd"></path></svg></div></a><div class="css-1qbqgqz"><style data-emotion="css 10n9pgp">.css-10n9pgp{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#05192D;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-weight:400;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="css c8hy1q">.css-c8hy1q{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#05192D;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-weight:400;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><span data-trackid="social-media-copy-to-clipboard" class="css-c8hy1q"><svg viewBox="0 0 18 18" aria-hidden="false" class="css-1oqmzj9" height="18" role="img" width="18"><title>Copy</title><path fill="currentColor" d="M7 4H5a4 4 0 014-4h5a4 4 0 014 4v5a4 4 0 01-4 4v-2a2 2 0 002-2V4a2 2 0 00-2-2H9a2 2 0 00-2 2zM4 7a2 2 0 00-2 2v5a2 2 0 002 2h5a2 2 0 002-2V9a2 2 0 00-2-2H4zm0-2h5a4 4 0 014 4v5a4 4 0 01-4 4H4a4 4 0 01-4-4V9a4 4 0 014-4z" fill-rule="evenodd"></path></svg></span></div></div></div></div></div></aside><style data-emotion="css 19m3yeo">.css-19m3yeo{max-width:696px;width:auto;}.css-19m3yeo div.author-biography:last-of-type{border-bottom:2px solid rgba(5, 25, 45, 0.1);}</style><style data-emotion="css cnyl9n">.css-cnyl9n{box-sizing:border-box;margin:0;min-width:0;max-width:696px;width:auto;}.css-cnyl9n div.author-biography:last-of-type{border-bottom:2px solid rgba(5, 25, 45, 0.1);}</style><div class="css-cnyl9n"><style data-emotion="css ztnvow">.css-ztnvow{margin:0 auto;max-width:757px;width:100%;}.css-ztnvow h2.no-top-margin{margin-top:0;}.css-ztnvow img{margin-top:14px!important;margin-bottom:14px!important;}.css-ztnvow p{margin-bottom:0px!important;}</style><style data-emotion="css iom6ik">.css-iom6ik{box-sizing:border-box;margin:0;min-width:0;margin:0 auto;max-width:757px;width:100%;}.css-iom6ik h2.no-top-margin{margin-top:0;}.css-iom6ik img{margin-top:14px!important;margin-bottom:14px!important;}.css-iom6ik p{margin-bottom:0px!important;}</style><div class="css-iom6ik"><style data-emotion="css 1skm3m9">.css-1skm3m9{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1skm3m9 .overflow-content{overflow-x:auto;}</style><style data-emotion="css lzghvr">.css-lzghvr{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-lzghvr .overflow-content{overflow-x:auto;}</style><style data-emotion="css dy8gfm">.css-dy8gfm{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-dy8gfm .overflow-content{overflow-x:auto;}</style><div class="css-dy8gfm"><style data-emotion="css 18y02vl">.css-18y02vl{margin-bottom:24px;}.css-18y02vl a{color:#0075AD;font-weight:700;-webkit-text-decoration:none;text-decoration:none;}.css-18y02vl a:active,.css-18y02vl a:focus,.css-18y02vl a:hover{-webkit-text-decoration:underline;text-decoration:underline;}.css-18y02vl a u{-webkit-text-decoration:none;text-decoration:none;}.css-18y02vl root{font-family:Studio-Feixen-Sans,Arial;margin:0;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;}.css-18y02vl *{max-width:100%;}.css-18y02vl *>code{background-color:#F7F7FC!important;color:#05192d;}.css-18y02vl .ai-code-explanation{background:#EFEFF5;color:#05192D;display:none;font-family:Studio-Feixen-Sans;font-size:14px;line-height:1.25;overflow-x:hidden;padding:8px 16px;white-space:pre-line;}.css-18y02vl .ai-code-explanation code{font-size:inherit;padding:4px;display:inline;background-color:#F7F7FC!important;}.css-18y02vl .ai-code-explanation li,.css-18y02vl .ai-code-explanation ul,.css-18y02vl .ai-code-explanation ol,.css-18y02vl .ai-code-explanation pre,.css-18y02vl .ai-code-explanation p{font-size:14px;margin:0;}.css-18y02vl .ai-code-explanation ul{padding-left:0;}.css-18y02vl .ai-code-explanation ol{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:0;}.css-18y02vl .ai-code-explanation p{margin-bottom:0!important;font-size:14px;}.css-18y02vl .ai-code-explanation pre{margin:-6px;}.css-18y02vl .ai-explain-code-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:transparent;border:2px solid #05192D;border-radius:4px;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:Studio-Feixen-Sans;font-size:12px;font-weight:800;height:32px;padding:8px;padding-top:0;padding-bottom:0;}.css-18y02vl .ai-explain-code-button::after{content:attr(data-value);margin-left:8px;}.css-18y02vl .ai-explain-code-button:active:not(:disabled),.css-18y02vl .ai-explain-code-button:hover{background-color:rgba(48, 57, 105, 0.06);-webkit-text-decoration:none;text-decoration:none;}.css-18y02vl .ai-explain-code-negative-feedback-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-top:4px;padding-bottom:4px;}.css-18y02vl .ai-explain-code-negative-feedback-button:hover{background:#E1E1E8;}.css-18y02vl .ai-explain-code-positive-feedback-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-top:4px;padding-bottom:4px;}.css-18y02vl .ai-explain-code-positive-feedback-button:hover{background:#E1E1E8;}.css-18y02vl .ai-feedback{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;background:#EFEFF5;border-top:1px solid #E1E1E8;color:#213147;display:none;font-family:Studio-Feixen-Sans;font-size:12px;line-height:1.5;overflow-x:hidden;padding:8px;padding-left:16px;padding-right:16px;padding-top:8px;padding-bottom:8px;width:100%;}.css-18y02vl .datalab-powered-text{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:Studio-Feixen-Sans;font-size:10px;font-weight:600;letter-spacing:1.5px;line-height:1.5;margin-left:auto;overflow-x:hidden;text-transform:uppercase;}.css-18y02vl .datalab-powered-text a{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;color:#5D6A77;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:4px;-webkit-text-decoration:none;text-decoration:none;}.css-18y02vl .datalab-powered-text a:hover{background-color:#E1E1E8;}.css-18y02vl .datalab-powered-text svg{height:17px;width:71px;}.css-18y02vl .ai-was-this-helpful{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:4px;}@media screen and (min-width: 480px){.css-18y02vl .ai-was-this-helpful{gap:8px;}}@media screen and (min-width: 768px){.css-18y02vl .ai-was-this-helpful{gap:16px;}}.css-18y02vl .atrule,.css-18y02vl .attr-value,.css-18y02vl .keyword{color:#008031;}.css-18y02vl .attrName,.css-18y02vl .builtin,.css-18y02vl .char,.css-18y02vl .inserted,.css-18y02vl .selector,.css-18y02vl .string,.css-18y02vl .url{color:#5646A5;}.css-18y02vl .boolean,.css-18y02vl .constant,.css-18y02vl .deleted,.css-18y02vl .number,.css-18y02vl .property,.css-18y02vl .symbol,.css-18y02vl .tag{color:#A85200;}.css-18y02vl .cdata,.css-18y02vl .comment,.css-18y02vl .doctype,.css-18y02vl .prolog{color:#5D6A77;}.css-18y02vl .className,.css-18y02vl .function{color:#0065D1;}.css-18y02vl .code-block-footer{background-color:#EFEFF5;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;min-width:-webkit-fit-content;min-width:-moz-fit-content;min-width:fit-content;padding-left:16px;padding-right:16px;padding-top:8px;padding-bottom:8px;width:100%;}.css-18y02vl .code-copy-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#EFEFF5;border-radius:50%;color:#5D6A77;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:Studio-Feixen-Sans;font-size:12px;font-weight:800;height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1.5;padding:0;position:absolute;right:16px;text-shadow:none;top:16px;width:32px;}.css-18y02vl .code-copy-button svg{color:#5D6A77;}.css-18y02vl .datacamp-exercise{margin-top:30px;}.css-18y02vl .datalab-link{color:#E1E1E8;cursor:pointer;font-family:Studio-Feixen-Sans;font-size:10px;font-weight:800;height:29px;letter-spacing:1.5px;line-height:12.5px;padding:0;text-shadow:none;text-transform:uppercase;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}.css-18y02vl .datalab-link:active,.css-18y02vl .datalab-link:focus,.css-18y02vl .datalab-link:hover{color:#575653;-webkit-text-decoration:none;text-decoration:none;}.css-18y02vl .no-bottom-margin{margin-bottom:0!important;}.css-18y02vl .no-top-margin{margin-top:0!important;}.css-18y02vl .p-margin{margin-bottom:14px!important;}.css-18y02vl .parent{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-18y02vl center{margin-top:24px;margin-bottom:24px;}.css-18y02vl code{border-radius:4px;min-height:64px;padding:4px;}.css-18y02vl code[class*="language-"]{font-family:JetBrainsMonoNL;font-size:0.875rem;font-weight:normal;line-height:21px;white-space:pre-wrap;background-color:#EFEFF5;}.css-18y02vl code[class*=language-]::selection{background-color:#EFEFF5;border:2px solid #05192D;}.css-18y02vl div[data-datacamp-exercise]{display:inline-block;max-width:100%;overflow-x:auto;}.css-18y02vl div[data-start-time]{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;max-width:100%;}.css-18y02vl h2{font-size:1.75rem;font-weight:700;letter-spacing:-0.5px;line-height:1.2;margin-top:16px;margin-bottom:16px;scroll-margin-top:80px;}@media screen and (min-width: 992px){.css-18y02vl h2{font-size:2rem;margin-top:24px;margin-bottom:24px;}}.css-18y02vl h3{font-size:1.25rem;margin-top:8px;margin-bottom:8px;scroll-margin-top:80px;}@media screen and (min-width: 992px){.css-18y02vl h3{font-size:1.5rem;margin-top:16px;margin-bottom:16px;}}.css-18y02vl h4{font-size:1.125rem;margin-bottom:16px;margin-top:8px;scroll-margin-top:80px;}@media screen and (min-width: 992px){.css-18y02vl h4{font-size:1.25rem;}}.css-18y02vl iframe{display:block;margin-left:auto;margin-right:auto;}.css-18y02vl img{border:1px solid #05192d1a;height:auto;margin-top:24px;margin-bottom:24px;}.css-18y02vl li{font-size:1rem;margin-bottom:12px;}.css-18y02vl li>p{margin-bottom:0;}.css-18y02vl ol{display:inline-block;list-style:decimal;margin-bottom:24px;padding-left:32px;}.css-18y02vl p{font-size:1rem;max-width:100%;margin-bottom:24px;min-width:100%;}.css-18y02vl p *{font-family:Studio-Feixen-Sans!important;}.css-18y02vl pre{background-color:#EFEFF5;border-radius:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:24px;padding:0;position:relative;}.css-18y02vl pre code{background-color:#F7F7FC;color:#05192d;display:inline-block;max-width:100%;overflow-x:auto;padding:16px;text-shadow:none;}.css-18y02vl pre code::-webkit-scrollbar{background-color:transparent;height:8px;scroll-padding-bottom:8px;}.css-18y02vl pre code::-webkit-scrollbar-thumb{background-color:#848492;border-radius:20px;opacity:0.4;}.css-18y02vl pre code::-webkit-scrollbar-thumb:hover{background-color:#575653;}.css-18y02vl pre code[class*="language-"]{border-radius:0;white-space:pre;background-color:#EFEFF5;}.css-18y02vl pre[class*="language-"]::selection,.css-18y02vl pre[class*="language-"] ::selection,.css-18y02vl code[class*="language-"]::selection,.css-18y02vl code[class*="language-"] ::selection{background-color:#E1E1E8;}.css-18y02vl table{background-color:#EFEFF5!important;border:1px solid!important;border-color:#E8E8EE!important;border-radius:4px!important;border-spacing:0px;font-size:0.875rem;height:auto!important;margin-bottom:24px;width:100%!important;}.css-18y02vl table::-webkit-scrollbar{display:block;}.css-18y02vl table::-webkit-scrollbar-thumb{background-color:#EFEFF5;border:4px solid;border-color:#ffffff;border-radius:20px;opacity:0.4;}.css-18y02vl table::-webkit-scrollbar-thumb:hover{background-color:#575653;}.css-18y02vl table col{border:1px solid!important;border-color:transparent!important;}.css-18y02vl table colgroup{width:100%!important;}.css-18y02vl table tbody{width:100%!important;}.css-18y02vl table tbody tr th{text-align:initial;}.css-18y02vl table tbody tr th:only-of-type{text-align:center;}.css-18y02vl table tbody tr:first-child td{background-color:#EFEFF5!important;font-weight:700;}.css-18y02vl table tbody tr:last-child td{border-bottom:1px solid white!important;}.css-18y02vl table td{background-color:white!important;border:none;border-top:#E8E8EE 1px solid;height:auto!important;}.css-18y02vl table td,.css-18y02vl table th{padding:15px 12px;text-overflow:ellipsis;}.css-18y02vl table th{background-color:#EFEFF5!important;border:1px solid;border-color:#EFEFF5;}.css-18y02vl table thead +tbody tr:first-child td{background:white!important;font-weight:normal;}.css-18y02vl table thead tr{background-color:#EFEFF5!important;border:1px solid;border-color:#EFEFF5;}.css-18y02vl table thead tr th{border:none;text-align:initial;}.css-18y02vl table tr{border-bottom:1px solid!important;border-color:#E8E8EE!important;height:auto!important;}.css-18y02vl table+table{margin-top:24px;}.css-18y02vl ul{list-style:disc;margin-bottom:24px;padding-left:32px;}</style><style data-emotion="css 1rrpkkx">.css-1rrpkkx{box-sizing:border-box;margin:0;min-width:0;font-size:1.125rem;line-height:1.5;margin-top:0;margin-bottom:24px;}.css-1rrpkkx a{color:#0075AD;font-weight:700;-webkit-text-decoration:none;text-decoration:none;}.css-1rrpkkx a:active,.css-1rrpkkx a:focus,.css-1rrpkkx a:hover{-webkit-text-decoration:underline;text-decoration:underline;}.css-1rrpkkx a u{-webkit-text-decoration:none;text-decoration:none;}.css-1rrpkkx root{font-family:Studio-Feixen-Sans,Arial;margin:0;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;}.css-1rrpkkx *{max-width:100%;}.css-1rrpkkx *>code{background-color:#F7F7FC!important;color:#05192d;}.css-1rrpkkx .ai-code-explanation{background:#EFEFF5;color:#05192D;display:none;font-family:Studio-Feixen-Sans;font-size:14px;line-height:1.25;overflow-x:hidden;padding:8px 16px;white-space:pre-line;}.css-1rrpkkx .ai-code-explanation code{font-size:inherit;padding:4px;display:inline;background-color:#F7F7FC!important;}.css-1rrpkkx .ai-code-explanation li,.css-1rrpkkx .ai-code-explanation ul,.css-1rrpkkx .ai-code-explanation ol,.css-1rrpkkx .ai-code-explanation pre,.css-1rrpkkx .ai-code-explanation p{font-size:14px;margin:0;}.css-1rrpkkx .ai-code-explanation ul{padding-left:0;}.css-1rrpkkx .ai-code-explanation ol{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:0;}.css-1rrpkkx .ai-code-explanation p{margin-bottom:0!important;font-size:14px;}.css-1rrpkkx .ai-code-explanation pre{margin:-6px;}.css-1rrpkkx .ai-explain-code-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:transparent;border:2px solid #05192D;border-radius:4px;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:Studio-Feixen-Sans;font-size:12px;font-weight:800;height:32px;padding:8px;padding-top:0;padding-bottom:0;}.css-1rrpkkx .ai-explain-code-button::after{content:attr(data-value);margin-left:8px;}.css-1rrpkkx .ai-explain-code-button:active:not(:disabled),.css-1rrpkkx .ai-explain-code-button:hover{background-color:rgba(48, 57, 105, 0.06);-webkit-text-decoration:none;text-decoration:none;}.css-1rrpkkx .ai-explain-code-negative-feedback-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-top:4px;padding-bottom:4px;}.css-1rrpkkx .ai-explain-code-negative-feedback-button:hover{background:#E1E1E8;}.css-1rrpkkx .ai-explain-code-positive-feedback-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-top:4px;padding-bottom:4px;}.css-1rrpkkx .ai-explain-code-positive-feedback-button:hover{background:#E1E1E8;}.css-1rrpkkx .ai-feedback{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;background:#EFEFF5;border-top:1px solid #E1E1E8;color:#213147;display:none;font-family:Studio-Feixen-Sans;font-size:12px;line-height:1.5;overflow-x:hidden;padding:8px;padding-left:16px;padding-right:16px;padding-top:8px;padding-bottom:8px;width:100%;}.css-1rrpkkx .datalab-powered-text{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:Studio-Feixen-Sans;font-size:10px;font-weight:600;letter-spacing:1.5px;line-height:1.5;margin-left:auto;overflow-x:hidden;text-transform:uppercase;}.css-1rrpkkx .datalab-powered-text a{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;color:#5D6A77;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:4px;-webkit-text-decoration:none;text-decoration:none;}.css-1rrpkkx .datalab-powered-text a:hover{background-color:#E1E1E8;}.css-1rrpkkx .datalab-powered-text svg{height:17px;width:71px;}.css-1rrpkkx .ai-was-this-helpful{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:4px;}@media screen and (min-width: 480px){.css-1rrpkkx .ai-was-this-helpful{gap:8px;}}@media screen and (min-width: 768px){.css-1rrpkkx .ai-was-this-helpful{gap:16px;}}.css-1rrpkkx .atrule,.css-1rrpkkx .attr-value,.css-1rrpkkx .keyword{color:#008031;}.css-1rrpkkx .attrName,.css-1rrpkkx .builtin,.css-1rrpkkx .char,.css-1rrpkkx .inserted,.css-1rrpkkx .selector,.css-1rrpkkx .string,.css-1rrpkkx .url{color:#5646A5;}.css-1rrpkkx .boolean,.css-1rrpkkx .constant,.css-1rrpkkx .deleted,.css-1rrpkkx .number,.css-1rrpkkx .property,.css-1rrpkkx .symbol,.css-1rrpkkx .tag{color:#A85200;}.css-1rrpkkx .cdata,.css-1rrpkkx .comment,.css-1rrpkkx .doctype,.css-1rrpkkx .prolog{color:#5D6A77;}.css-1rrpkkx .className,.css-1rrpkkx .function{color:#0065D1;}.css-1rrpkkx .code-block-footer{background-color:#EFEFF5;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;min-width:-webkit-fit-content;min-width:-moz-fit-content;min-width:fit-content;padding-left:16px;padding-right:16px;padding-top:8px;padding-bottom:8px;width:100%;}.css-1rrpkkx .code-copy-button{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#EFEFF5;border-radius:50%;color:#5D6A77;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:Studio-Feixen-Sans;font-size:12px;font-weight:800;height:32px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1.5;padding:0;position:absolute;right:16px;text-shadow:none;top:16px;width:32px;}.css-1rrpkkx .code-copy-button svg{color:#5D6A77;}.css-1rrpkkx .datacamp-exercise{margin-top:30px;}.css-1rrpkkx .datalab-link{color:#E1E1E8;cursor:pointer;font-family:Studio-Feixen-Sans;font-size:10px;font-weight:800;height:29px;letter-spacing:1.5px;line-height:12.5px;padding:0;text-shadow:none;text-transform:uppercase;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}.css-1rrpkkx .datalab-link:active,.css-1rrpkkx .datalab-link:focus,.css-1rrpkkx .datalab-link:hover{color:#575653;-webkit-text-decoration:none;text-decoration:none;}.css-1rrpkkx .no-bottom-margin{margin-bottom:0!important;}.css-1rrpkkx .no-top-margin{margin-top:0!important;}.css-1rrpkkx .p-margin{margin-bottom:14px!important;}.css-1rrpkkx .parent{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1rrpkkx center{margin-top:24px;margin-bottom:24px;}.css-1rrpkkx code{border-radius:4px;min-height:64px;padding:4px;}.css-1rrpkkx code[class*="language-"]{font-family:JetBrainsMonoNL;font-size:0.875rem;font-weight:normal;line-height:21px;white-space:pre-wrap;background-color:#EFEFF5;}.css-1rrpkkx code[class*=language-]::selection{background-color:#EFEFF5;border:2px solid #05192D;}.css-1rrpkkx div[data-datacamp-exercise]{display:inline-block;max-width:100%;overflow-x:auto;}.css-1rrpkkx div[data-start-time]{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;max-width:100%;}.css-1rrpkkx h2{font-size:1.75rem;font-weight:700;letter-spacing:-0.5px;line-height:1.2;margin-top:16px;margin-bottom:16px;scroll-margin-top:80px;}@media screen and (min-width: 992px){.css-1rrpkkx h2{font-size:2rem;margin-top:24px;margin-bottom:24px;}}.css-1rrpkkx h3{font-size:1.25rem;margin-top:8px;margin-bottom:8px;scroll-margin-top:80px;}@media screen and (min-width: 992px){.css-1rrpkkx h3{font-size:1.5rem;margin-top:16px;margin-bottom:16px;}}.css-1rrpkkx h4{font-size:1.125rem;margin-bottom:16px;margin-top:8px;scroll-margin-top:80px;}@media screen and (min-width: 992px){.css-1rrpkkx h4{font-size:1.25rem;}}.css-1rrpkkx iframe{display:block;margin-left:auto;margin-right:auto;}.css-1rrpkkx img{border:1px solid #05192d1a;height:auto;margin-top:24px;margin-bottom:24px;}.css-1rrpkkx li{font-size:1rem;margin-bottom:12px;}.css-1rrpkkx li>p{margin-bottom:0;}.css-1rrpkkx ol{display:inline-block;list-style:decimal;margin-bottom:24px;padding-left:32px;}.css-1rrpkkx p{font-size:1rem;max-width:100%;margin-bottom:24px;min-width:100%;}.css-1rrpkkx p *{font-family:Studio-Feixen-Sans!important;}.css-1rrpkkx pre{background-color:#EFEFF5;border-radius:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:24px;padding:0;position:relative;}.css-1rrpkkx pre code{background-color:#F7F7FC;color:#05192d;display:inline-block;max-width:100%;overflow-x:auto;padding:16px;text-shadow:none;}.css-1rrpkkx pre code::-webkit-scrollbar{background-color:transparent;height:8px;scroll-padding-bottom:8px;}.css-1rrpkkx pre code::-webkit-scrollbar-thumb{background-color:#848492;border-radius:20px;opacity:0.4;}.css-1rrpkkx pre code::-webkit-scrollbar-thumb:hover{background-color:#575653;}.css-1rrpkkx pre code[class*="language-"]{border-radius:0;white-space:pre;background-color:#EFEFF5;}.css-1rrpkkx pre[class*="language-"]::selection,.css-1rrpkkx pre[class*="language-"] ::selection,.css-1rrpkkx code[class*="language-"]::selection,.css-1rrpkkx code[class*="language-"] ::selection{background-color:#E1E1E8;}.css-1rrpkkx table{background-color:#EFEFF5!important;border:1px solid!important;border-color:#E8E8EE!important;border-radius:4px!important;border-spacing:0px;font-size:0.875rem;height:auto!important;margin-bottom:24px;width:100%!important;}.css-1rrpkkx table::-webkit-scrollbar{display:block;}.css-1rrpkkx table::-webkit-scrollbar-thumb{background-color:#EFEFF5;border:4px solid;border-color:#ffffff;border-radius:20px;opacity:0.4;}.css-1rrpkkx table::-webkit-scrollbar-thumb:hover{background-color:#575653;}.css-1rrpkkx table col{border:1px solid!important;border-color:transparent!important;}.css-1rrpkkx table colgroup{width:100%!important;}.css-1rrpkkx table tbody{width:100%!important;}.css-1rrpkkx table tbody tr th{text-align:initial;}.css-1rrpkkx table tbody tr th:only-of-type{text-align:center;}.css-1rrpkkx table tbody tr:first-child td{background-color:#EFEFF5!important;font-weight:700;}.css-1rrpkkx table tbody tr:last-child td{border-bottom:1px solid white!important;}.css-1rrpkkx table td{background-color:white!important;border:none;border-top:#E8E8EE 1px solid;height:auto!important;}.css-1rrpkkx table td,.css-1rrpkkx table th{padding:15px 12px;text-overflow:ellipsis;}.css-1rrpkkx table th{background-color:#EFEFF5!important;border:1px solid;border-color:#EFEFF5;}.css-1rrpkkx table thead +tbody tr:first-child td{background:white!important;font-weight:normal;}.css-1rrpkkx table thead tr{background-color:#EFEFF5!important;border:1px solid;border-color:#EFEFF5;}.css-1rrpkkx table thead tr th{border:none;text-align:initial;}.css-1rrpkkx table tr{border-bottom:1px solid!important;border-color:#E8E8EE!important;height:auto!important;}.css-1rrpkkx table+table{margin-top:24px;}.css-1rrpkkx ul{list-style:disc;margin-bottom:24px;padding-left:32px;}</style><span class="listed-menu css-1rrpkkx"><h2 id="apache-spark-and-python-for-big-data-and-machine-learning-apach">Apache Spark and Python for Big Data and Machine Learning</h2>
<p class="p-margin">Apache Spark is known as a fast, easy-to-use and general engine for big data processing that has built-in modules for streaming, SQL, Machine Learning (ML) and graph processing. This technology is an in-demand skill for data engineers, but also data scientists can benefit from learning Spark when doing Exploratory Data Analysis (EDA), feature extraction and, of course, ML.</p>
<p class="p-margin">In this tutorial, you’ll interface Spark with Python through PySpark, the Spark Python API that exposes the Spark programming model to Python. More concretely, you’ll focus on:</p>
<nav>
<ul>
<li>Installing PySpark locally on your personal computer and setting it up so that you can work with the interactive Spark shell to do some quick, interactive analyses on your data. You’ll see how to do this with <code>pip</code>, Homebrew and via the Spark download page.</li>
<li>Learning how to work with the basics of Spark: you’ll see how you can create RDDs and perform basic operations on them.</li>
<li>Getting started with PySpark in Jupyter Notebook and loading in a real-life data set.</li>
<li>Exploring and preprocessing the data that you loaded in at the first step the help of DataFrames, which demands that you make use of Spark SQL, which allows you to query structured data inside Spark programs.</li>
<li>Creating a Linear Regression model with Spark ML to feed the data to it, after which you’ll be able to make predictions. And, lastly,</li>
<li>Evaluating the machine learning model that you made.</li>
</ul>
</nav>
<div id="scoped-content">
<p class="p-margin"><img alt="Apache Spark Tutorial" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Spark+ML/content_macro-1452986_960_720.jpg" style="width: 400px; height: 261px;"></p>
<p class="p-margin">If you're rather interested in using Spark with R, you should check out DataCamp’s free <a href="https://www.datacamp.com/courses/introduction-to-spark-in-r-using-sparklyr" target="_blank" rel="noopener">Introduction to Spark in R with sparklyr</a> or download the <a href="https://www.datacamp.com/cheat-sheet/pyspark-cheat-sheet-spark-dataframes-in-python" target="_blank" rel="noopener">PySpark SQL</a> cheat sheet.</p>
</div>
<h2 id="installing-apache-spark-insta">Installing Apache Spark</h2>
<p class="p-margin">Installing Spark and getting it to work can be a challenge. In this section, you’ll cover some steps that will show you how to get it installed on your pc.</p>
<p class="p-margin">First thing that you want to do is checking whether you meet the prerequisites. Spark is written in Scala Programming Language and runs on Java Virtual Machine (JVM) environment. That’s why you need to check if you have a <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">Java Development Kit (JDK)</a> installed. You do this because the JDK will provide you with one or more implementations of the JVM. Preferably, you want to pick the latest one, which, at the time of writing is the JDK8.</p>
<p class="p-margin">Next, you’re reading to download Spark!</p>
<div class="section level3" id="downloading-pyspark-with-pip">
<h3 id="downloading-pyspark-with-pip-then,">Downloading <code>pyspark</code> with <code>pip</code></h3>
<p class="p-margin">Then, you can download and install PySpark it with the help of <code>pip</code>. This is fairly easy and much like installing any other package. You just run the usual command and the heavy lifting gets done for you:</p>
<pre class="language-javascript"><code>$ pip install pyspark</code></pre>
<p class="p-margin">Alternatively, you can also go to the <a href="https://spark.apache.org/downloads.html" target="_blank" rel="noopener">Spark download page</a>. Keep the default options in the first three steps and you’ll find a downloadable link in step 4. Click on that link to download it. For this tutorial, you’ll download the 2.2.0 Spark Release and the “Pre-built for Apache Hadoop 2.7 and later” package type.</p>
<p class="p-margin"><strong>Note</strong> that the download can take some time to finish!</p>
</div>
<div class="section level3" id="downloading-spark-with-homebrew">
<h3 id="downloading-spark-with-homebrew-youca">Downloading Spark with Homebrew</h3>
<p class="p-margin">You can also install Spark with the <a href="https://brew.sh" target="_blank" rel="noopener">Homebrew</a>, a free and open-source package manager. This is especially handy if you’re working with macOS.</p>
<p class="p-margin">Simply run the following commands to search for Spark, to get more information and to finally install it on your personal computer:</p>
<pre class="language-javascript"><code class="lang-bash"># Search for spark
$ brew search spark

# Get more information on apache-spark
$ brew info apache-spark

# Install apache-spark
$ brew install apache-spark</code></pre>
</div>
<div class="section level3" id="download-and-set-up-spark">
<h3 id="download-and-set-up-spark-next,">Download and Set Up Spark</h3>
<p class="p-margin">Next, make sure that you untar the directory that appears in your <code>Downloads</code> folder. This can happen automatically for you, by double clicking the <code>spark-2.2.0-bin-hadoop2.7.tgz</code> archive or by opening up your Terminal and running the following command:</p>
<pre class="language-javascript"><code class="lang-bash">$ tar xvf spark-2.2.0-bin-hadoop2.7.tgz</code></pre>
<p class="p-margin">Next, move the untarred folder to <code>/usr/local/spark</code> by running the following line:</p>
<pre class="language-javascript"><code class="lang-bash">$ mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark</code></pre>
<p class="p-margin"><strong>Note</strong> that if you get an error that says that the permission is denied to move this folder to the new location, you should add <code>sudo</code> in front of this command. The line above will then become <code>$ sudo mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark</code>. You’ll be prompted to give your password, which is usually the one that you also use to unlock your pc when you start it up :)</p>
<p class="p-margin">Now that you’re all set to go, open the README file in the file path <code>/usr/local/spark</code>. You can do this by executing</p>
<pre class="language-javascript"><code class="lang-bash">$ cd /usr/local/spark</code></pre>
<p class="p-margin">This will brings you to the folder that you need to be. Then, you can start inspecting the folder and reading the README file that is incuded in it.</p>
<p class="p-margin">First, use <code>$ ls</code> to get a list of the files and folders that are in this <code>spark</code> folder. You’ll see that there’s a README.md file in there. You can open it by executing one of the following commands:</p>
<pre class="language-javascript"><code class="lang-bash"># Open and edit the file
$ nano README.md

# Just read the file 
$ cat README.md</code></pre>
<p class="p-margin"><strong>Tip</strong> use the tab button on your keyboard to autocomplete as you’re typing the file name :) This will save you some time.</p>
<p class="p-margin">You’ll see that this README provides you with some general information about Spark, online documentation, building Spark, the Interactive Scala and Python shells, example programs and much more.</p>
<p class="p-margin">The thing that could interest you most here is the section on how to build Spark but note that this will only be particularly relevant if you haven’t downloaded a pre-built version. For this tutorial, however, you downloaded a pre-built version. You can press CTRL + X to exit the README, which brings you back to the spark folder.</p>
<p class="p-margin">In case you selected a version that hasn’t been built yet, make sure you run the command that is listed in the README file. At the time of writing, this is the following:</p>
<pre class="language-javascript"><code class="lang-bash">$ build/mvn -DskipTests clean package run</code></pre>
<p class="p-margin">Note that this command can take a while to run.</p>
</div>
<h2 id="pyspark-basics:-rdds-nowth">PySpark Basics: RDDs</h2>
<p class="p-margin">Now that you’ve successfully installed Spark and PySpark, let’s first start off by exploring the interactive Spark Shell and by nailing down some of the basics that you will need when you want to get started. In the rest of this tutorial, however, you’ll work with PySpark in a Jupyter notebook.</p>
<div class="section level3" id="spark-applications-versus-spark-shell">
<h3 id="spark-applications-versus-spark-shell-thein">Spark Applications Versus Spark Shell</h3>
<p class="p-margin">The interactive shell is an example of a Read-Eval(uate)-Print-Loop (REPL) environment; That means that whatever you type in is read, evaluated and printed out to you so that you can continue your analysis. This might remind you of IPython, which is a powerful interactive Python shell that you might know from working with Jupyter. If you want to know more, consider reading DataCamp’s <a href="https://www.datacamp.com/blog/ipython-or-jupyter" target="_blank" rel="noopener">IPython or Jupyter</a> blog post.</p>
<p class="p-margin">This means that you can use the shell, which is available for Python as well as Scala, for all interactive work that you need to do.</p>
<p class="p-margin">Besides this shell, you can also write and deploy Spark applications. In contrast to writing Spark applications, the SparkSession has already been created for you so that you can just start working and not waste valuable time on creating one.</p>
<p class="p-margin">Now you might wonder: what is the SparkSession?</p>
<p class="p-margin">Well, it’s the main entry point for Spark functionality: it represents the connection to a Spark cluster and you can use it to create RDDs and to broadcast variables on that cluster. When you’re working with Spark, everything starts and ends with this SparkSession. <strong>Note</strong> that before Spark 2.0.0, the three main connection objects were SparkContext, SqlContext and HiveContext.</p>
<p class="p-margin">You’ll see more on this later on. For now, let’s just focus on the shell.</p>
</div>
<div class="section level3" id="the-python-spark-shell">
<h3 id="the-python-spark-shell-fromw">The Python Spark Shell</h3>
<p class="p-margin">From within the <code>spark</code> folder located at <code>/usr/local/spark</code>, you can run</p>
<pre class="language-javascript"><code>$ ./bin/pyspark</code></pre>
<p class="p-margin">At first, you’ll see some text appearing. And then, you’ll see “Spark” appearing, just like this:</p>
<pre class="language-javascript"><code class="lang-bash">Python 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016, 12:39:47) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/26 11:41:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/26 11:41:47 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/

Using Python version 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016 12:39:47)
SparkSession available as 'spark'.
&gt;&gt;&gt;</code></pre>
<p class="p-margin">When you see this, you know that you’re ready to start experimenting within the interactive shell!</p>
<p class="p-margin"><strong>Tip</strong>: if you prefer using the IPython shell instead of the Spark shell, you can do this by setting the following environment variable:</p>
<pre class="language-javascript"><code>export PYSPARK_DRIVER_PYTHON="/usr/local/ipython/bin/ipython"</code></pre>
<div class="section level3" id="creating-rdds">
<h3 id="creating-rdds-now,l">Creating RDDs</h3>
<p class="p-margin">Now, let’s start small and make an RDD, which is the most basic building block of Spark. An RDD simply represents data but it’s not one object, a collection of records, a result set or a data set. That is because it’s intended for data that resides on multiple computers: a single RDD could be spread over thousands of Java Virtual Machines (JVMs), because Spark automatically partitions the data under the hood to get this parallelism. Of course, you can adjust the parallelism to get more partitions. That’s why an RDD is actually a collection of partitions.</p>
<p class="p-margin">You can easily create a simple RDD by using the <code>parallelize()</code> function and by simply passing some data (an iterable, like a list, or a collection) to it:</p>
<pre class="language-javascript"><code class="lang-bash">&gt;&gt;&gt; rdd1 = spark.sparkContext.parallelize([('a',7),('a',2),('b',2)])
&gt;&gt;&gt; rdd2 = spark.sparkContext.parallelize([("a",["x","y","z"]), ("b",["p", "r"])])
&gt;&gt;&gt; rdd3 = spark.sparkContext.parallelize(range(100))</code></pre>
<p class="p-margin"><strong>Note</strong> that the SparkSession object has the SparkContext object, which you can access with <code>spark.sparkContext</code>. For backwards compatibility reasons, it’s also still possible to call the SparkContext with <code>sc</code>, as in <code>rdd1 = sc.parallelize(['a',7),('a',2),('b',2)])</code>.</p>
</div>
<div class="section level3" id="rdd-operations">
<h3 id="rdd-operations-nowth">RDD Operations</h3>
<p class="p-margin">Now that you have created the RDDs, you can use the distributed data in <code>rdd1</code> and <code>rdd2</code> to operate on in parallel. You have two types of operations: transformations and actions.</p>
<p class="p-margin">Now, to intuitively get the difference between these two, consider some of the most common transformations are <code>map()</code>, <code>filter()</code>, <code>flatMap()</code>, <code>sample()</code>, <code>randomSplit()</code>, <code>coalesce()</code> and <code>repartition()</code> and some of the most common actions are <code>reduce()</code>, <code>collect()</code>, <code>first()</code>, <code>take()</code>, <code>count()</code>, <code>saveAsHadoopFile()</code>.</p>
<p class="p-margin">Transformations are lazy operations on a RDD that create one or many new RDDs, while actions produce non-RDD values: they return a result set, a number, a file, …</p>
<p class="p-margin">You can, for example, aggregate all the elements of <code>rdd1</code> using the following, simple lambda function and return the results to the driver program:</p>
<pre class="language-javascript"><code class="lang-bash">&gt;&gt;&gt; rdd1.reduce(lambda a,b: a+b)</code></pre>
<p class="p-margin">Executing this line of code will give you the following result: <code>('a', 7, 'a', 2, 'b', 2)</code>. Another example of a transformation is <code>flatMapValues()</code>, which you run on key-value pair RDDs, such as <code>rdd2</code>. In this case, you pass each value in the key-value pair RDD <code>rdd2</code> through a flatMap function without changing the keys, which is the lambda function defined below and you perform an action after that by collecting hte results with <code>collect()</code>.</p>
<pre class="language-javascript"><code class="lang-bash">&gt;&gt;&gt; rdd2.flatMapValues(lambda x: x).collect()
[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]</code></pre>
</div>
</div>
<h2 id="the-data-nowth">The Data</h2>
<p class="p-margin">Now that you have covered some basics with the interactive shell, it’s time to get started with some real data. For this tutorial, you’ll make use of the <a href="http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html" target="_blank" rel="noopener">California Housing</a> data set. Note, of course, that this is actually ‘small’ data and that using Spark in this context might be overkill; This tutorial is for educational purposes only and is meant to give you an idea of how you can use PySpark to build a machine learning model.</p>
<h2 id="loading-and-exploring-your-data-event">Loading and Exploring your Data</h2>
<p class="p-margin">Even though you know a bit more about your data, you should take the time to go ahead and explore it more thoroughly; Before you do this, however, you will set up your Jupyter Notebook with Spark and you’ll take some first steps to defining the SparkContext.</p>
<div class="section level3" id="pyspark-in-jupyter-notebook">
<h3 id="pyspark-in-jupyter-notebook-forth">PySpark in Jupyter Notebook</h3>
<p class="p-margin">For this part of the tutorial, you won’t use the ishell but you’ll build your own application. You’ll do this in a Jupyter Notebook. You already have all the things that you need installed, so you don’t need to do much to get PySpark to work in Jupyter.</p>
<p class="p-margin">You can just launch the notebook application the same way like you always do, by running <code>$ jupyter notebook</code>. Then, you make a new notebook and you simply import the <code>findspark</code> library and use the <code>init()</code> function. In this case, you’re going to supply the path <code>/usr/local/spark</code> to <code>init()</code> because you’re certain that this is the path where you installed Spark.</p>
<pre class="language-javascript"><code class="lang-python"># Import findspark 
import findspark

# Initialize and provide path
findspark.init("/usr/local/spark")

# Or use this alternative
#findspark.init()</code></pre>
<p class="p-margin"><strong>Tip</strong>: if you have no idea whether your path is set correctly or where you have installed Spark on your pc, you can always use <code>findspark.find()</code> to automatically detect the location of where Spark is installed.</p>
<p class="p-margin">If you’re looking for alternative ways to work with Spark in Jupyter, consult our <a href="https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark" target="_blank" rel="noopener">Apache Spark in Python: Beginner’s Guide</a>.</p>
<p class="p-margin">Now that you have got all of that settled, you can finally start by creating your first Spark program!</p>
</div>
<div class="section level3" id="creating-your-first-spark-program">
<h3 id="creating-your-first-spark-program-whaty">Creating your First Spark Program</h3>
<p class="p-margin">What you first want to be doing is importing the <code>SparkContext</code> from the <code>pyspark</code> package and initializing it. <strong>Remember</strong> that you didn’t have to do this before because the interactive Spark shell automatically created and initialized it for you! Here, you’ll need to do a little bit more work yourself :)</p>
<p class="p-margin">Import the <code>SparkSession</code> module from <code>pyspark.sql</code> and build a SparkSession with the <code>builder()</code> method. Afterwards, you can set the master URL to connect to, the application name, add some additional configuration like the executor memory and then lastly, use <code>getOrCreate()</code> to either get the current Spark session or to create one if there is none running.</p>
<pre class="language-javascript"><code class="lang-python"># Import SparkSession
from pyspark.sql import SparkSession

# Build the SparkSession
spark = SparkSession.builder \
   .master("local") \
   .appName("Linear Regression Model") \
   .config("spark.executor.memory", "1gb") \
   .getOrCreate()
   
sc = spark.sparkContext</code></pre>
<p class="p-margin"><strong>Note</strong> that if you get an error where there’s a FileNotFoundError similar to this one: “No such file or directory: ‘/User/YourName/Downloads/spark-2.1.0-bin-hadoop2.7/./bin/spark-submit’”, you know that you have to (re)set your Spark PATH. Go to your home directory by executing <code>$ cd</code> and then edit the <code>.bash_profile</code> file by running <code>$ nano .bash_profile</code>.</p>
<p class="p-margin">Add something like the following to the bottom of the file</p>
<pre class="language-javascript"><code class="lang-bash">export SPARK_HOME="/usr/local/spark"</code></pre>
<p class="p-margin">Use CTRL + X to exit the file but make sure to save your adjustments by also entering <em>Y</em> to confirm the changes. Next, don’t forget to set the changes in motion by running <code>source .bash_profile</code>.</p>
<p class="p-margin"><strong>Tip</strong>: you can also set additional environment variables if you want; You probably don’t need them, but it’s definitely good to know that you can set them if desired. Consider the following examples:</p>
<pre class="language-javascript"><code class="lang-bash"># Set a fixed value for the hash seed secret
export PYTHONHASHSEED=0

# Set an alternate Python executable
export PYSPARK_PYTHON=/usr/local/ipython/bin/ipython

# Augment the default search path for shared libraries
export LD_LIBRARY_PATH=/usr/local/ipython/bin/ipython

# Augment the default search path for private libraries 
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH:$SPARK_HOME/python/</code></pre>
<p class="p-margin"><strong>Note</strong> also that now you have initialized a default SparkSession. However, in most cases, you’ll want to configure this further. You’ll see that this will be really needed when you start working with big data. If you want to know more about it, check <a href="https://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">this page</a>.</p>
</div>
<div class="section level3" id="loading-in-your-data">
<h3 id="loading-in-your-data-thist">Loading in your Data</h3>
<p class="p-margin">This tutorial makes use of the California Housing data set. It appeared in a 1997 paper titled <em>Sparse Spatial Autoregressions</em>, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.</p>
<p class="p-margin">The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. You’ll gather this information from <a href="http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html" target="_blank" rel="noopener">this web page</a> or by reading the paper which was mentioned above and which you can find <a href="http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/html/ms_sp_lets1.html" target="_blank" rel="noopener">here</a>.</p>
<p class="p-margin">These spatial data contain 20,640 observations on housing prices with 9 economic variables:</p>
<ul>
<li><em>Longitude</em> refers to the angular distance of a geographic place north or south of the earth’s equator for each block group;</li>
<li><em>Latitude</em> refers to the angular distance of a geographic place east or west of the earth’s equator for each block group;</li>
<li><em>Housing median age</em> is the median age of the people that belong to a block group. <strong>Note</strong> that the median is the value that lies at the midpoint of a frequency distribution of observed values;</li>
<li><em>Total rooms</em> is the total number of rooms in the houses per block group;</li>
<li><em>Total bedrooms</em> is the total number of bedrooms in the houses per block group;</li>
<li><em>Population</em> is the number of inhabitants of a block group;</li>
<li><em>Households</em> refers to units of houses and their occupants per block group;</li>
<li><em>Median income</em> is used to register the median income of people that belong to a block group; And,</li>
<li><em>Median house value</em> is the dependent variable and refers to the median house value per block group.</li>
</ul>
<p class="p-margin">What’s more, you also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.</p>
<p class="p-margin">The <em>Median house value</em> is the dependent variable and will be assigned the role of the target variable in your ML model.</p>
<p class="p-margin">You can download the data <a href="http://lib.stat.cmu.edu/datasets" target="_blank" rel="noopener">here</a>. Look for the <em>houses.zip</em> folder, download and untar it so that you can access the data folders.</p>
<p class="p-margin">Next, you’ll use the <code>textFile()</code> method to read in the data from the folder that you downloaded it to RDDs. This method takes an URI for the file, which is in this case the local path of your machine, and reads it as a collection of lines. For all convenience, you’ll not only read in the .data file, but also the .domain file that contains the header. This will allow you to double check the order of the variables.</p>
<pre class="language-javascript"><code class="lang-python"># Load in the data
rdd = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.data')

# Load in the header
header = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.domain')</code></pre>
<h2 id="data-exploration-youal">Data Exploration</h2>
<p class="p-margin">You already gathered a lot of information by just looking at the web page where you found the data set, but it’s always better to get hands-on and inspect your data with the help of Spark with Python, in this case.</p>
<p class="p-margin">Important to understand here is that, because Spark’s execution is “lazy” execution, nothing has been executed yet. Your data hasn’t been actually read in. The <code>rdd</code> and <code>header</code> variables are actually just concepts in your mind. You have to push Spark to work for you, so let’s use the <code>collect()</code> method to look at the <code>header</code>:</p>
<pre class="language-javascript"><code class="lang-python">header.collect()</code></pre>
<p class="p-margin">The <code>collect()</code> method brings the entire RDD to a single machine, and you’ll get to see the following result:</p>
<pre class="language-javascript"><code class="lang-python">[u'longitude: continuous.', u'latitude: continuous.', u'housingMedianAge: continuous. ', u'totalRooms: continuous. ', u'totalBedrooms: continuous. ', u'population: continuous. ', u'households: continuous. ', u'medianIncome: continuous. ', u'medianHouseValue: continuous. ']</code></pre>
<p class="p-margin"><strong>Tip</strong>: be careful when using <code>collect()</code>! Running this line of code can possibly cause the driver to run out of memory. That’s why the following approach with the <code>take()</code> method is a safer approach if you want to just print a few elements of the RDD. In general, it’s a good principle to limit your result set whenever possible, <a href="https://www.datacamp.com/tutorial/sql-tutorial-query" target="_blank" rel="noopener">just like when you’re using SQL</a>.</p>
<p class="p-margin">You learn that the order of the variables is the same as the one that you saw above in the presentation of the data set, and you also learn that all columns should have continuous values. Let’s force Spark to do some more work and take a look at the California housing data to confirm this.</p>
<p class="p-margin">Call the <code>take()</code> method on your RDD:</p>
<pre class="language-javascript"><code class="lang-python">rdd.take(2)</code></pre>
<p class="p-margin">By executing the previous line of code, you take the first 2 elements of the RDD. The result is as you expected: because you read in the files with the <code>textFile()</code> function, the lines are just all read in together. The entries are separated by a single comma and the rows themselves are also separated by a comma:</p>
<pre class="language-javascript"><code class="lang-python">[u'-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000', u'-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']</code></pre>
<p class="p-margin">You definitely need to solve this. Now, you don’t need to split the entries, but you definitely need to make sure that the rows of your data are separate elements. To solve this, you’ll use the <code>map()</code> function to which you pass a lambda function to split the line at the comma. Then, check your result by running the same line with the <code>take()</code> method, just like you did before:</p>
<p class="p-margin"><strong>Remember</strong> that lambda functions are anonymous functions which are created at runtime.</p>
<pre class="language-javascript"><code class="lang-python"># Split lines on commas
rdd = rdd.map(lambda line: line.split(","))

# Inspect the first 2 lines 
rdd.take(2)</code></pre>
<p class="p-margin">You’ll get the following result:</p>
<pre class="language-javascript"><code class="lang-python">[[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000'], [u'-122.220000', u'37.860000', u'21.000000', u'7099.000000', u'1106.000000', u'2401.000000', u'1138.000000', u'8.301400', u'358500.000000']]</code></pre>
<p class="p-margin">Alternatively, you can also use the following functions to inspect your data:</p>
<pre class="language-javascript"><code class="lang-python"># Inspect the first line 
rdd.first()

# Take top elements
rdd.top(2)</code></pre>
<p class="p-margin">If you’re used to working with Pandas or data frames in R, you’ll have probably also expected to see a header, but there is none. To make your life easier, you will move on from the RDD and convert it to a DataFrame. Dataframes are preferred over RDDs whenever you can use them. Especially when you’re working with Python, the performance of DataFrames is better than RDDs.</p>
<p class="p-margin">But what is the difference between the two?</p>
<p class="p-margin">You can use RDDs when you want to perform low-level transformations and actions on your unstructured data. This means that you don’t care about imposing a schema while processing or accessing the attributes by name or column. Tying in to what was said before about performance, by using RDDs, you don’t necessarily want the performance benefits that DataFrames can offer for (semi-) structured data. Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.</p>
<p class="p-margin">To recapitulate, you’ll switch to DataFrames now to use high-level expressions, to perform SQL queries to explore your data further and to gain columnar access.</p>
<p class="p-margin">So let’s do this.</p>
<p class="p-margin">The first step is to make a SchemaRDD or an RDD of Row objects with a schema. This is normal, because just like a DataFrame, you eventually want to come to a situation where you have rows and columns. Each entry is linked to a row and a certain column and columns have data types.</p>
<p class="p-margin">You’ll use the <code>map()</code> function again and another lambda function in which you’ll map each entry to a field in a Row. To make this more visual, consider this first line:</p>
<pre class="language-javascript"><code class="lang-python">[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000']</code></pre>
<p class="p-margin">The lambda function says that you’re going to construct a row in a SchemaRDD and that the element at index 0 will have the name “longitude”, and so on.</p>
<p class="p-margin">With this SchemaRDD in place, you can easily convert the RDD to a DataFrame with the <code>toDF()</code> method.</p>
<pre class="language-javascript"><code class="lang-python"># Import the necessary modules 
from pyspark.sql import Row

# Map the RDD to a DF
df = rdd.map(lambda line: Row(longitude=line[0], 
                              latitude=line[1], 
                              housingMedianAge=line[2],
                              totalRooms=line[3],
                              totalBedRooms=line[4],
                              population=line[5], 
                              households=line[6],
                              medianIncome=line[7],
                              medianHouseValue=line[8])).toDF()</code></pre>
<p class="p-margin">Now that you have your DataFrame <code>df</code>, you can inspect it with the methods that you have also used before, namely <code>first()</code> and <code>take()</code>, but also with <code>head()</code> and <code>show()</code>:</p>
<pre class="language-javascript"><code># Show the top 20 rows 
df.show()</code></pre>
<p class="p-margin">You’ll immediately see that this looks much different from the RDD that you were working with before:</p>
<div class="figure"><img alt="pyspark tutorial" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Spark+ML/Spark+DataFrame.png" loading="lazy"></div>
<br>
<p class="p-margin"><strong>Tip</strong>: use <code>df.columns</code> to return the columns of your DataFrame.</p>
<p class="p-margin">The data seems all nicely ordered into columns, but what about the data types? By reading in your data, Spark will try to infer a schema, but has this been successful here? Use either <code>df.dtypes</code> or <code>df.printSchema()</code> to get to know more about the data types that are contained within your DataFrame.</p>
<pre class="language-javascript"><code class="lang-python"># Print the data types of all `df` columns
# df.dtypes

# Print the schema of `df`
df.printSchema()</code></pre>
<p class="p-margin">Because you don’t execute the first line of code, you will only get back the following result:</p>
<pre class="language-javascript"><code class="lang-python">root
 |-- households: string (nullable = true)
 |-- housingMedianAge: string (nullable = true)
 |-- latitude: string (nullable = true)
 |-- longitude: string (nullable = true)
 |-- medianHouseValue: string (nullable = true)
 |-- medianIncome: string (nullable = true)
 |-- population: string (nullable = true)
 |-- totalBedRooms: string (nullable = true)
 |-- totalRooms: string (nullable = true)</code></pre>
<p class="p-margin">All columns are still of data type string… That’s disappointing!</p>
<p class="p-margin">If you want to continue with this DataFrame, you’ll need to rectify this situation and assign “better” or more accurate data types to all columns. Your performance will also benefit from this. Intuitively, you could go for a solution like the following, where you declare that each column of the DataFrame <code>df</code> should be cast to a <code>FloatType()</code>:</p>
<pre class="language-javascript"><code class="lang-python">from pyspark.sql.types import *

df = df.withColumn("longitude", df["longitude"].cast(FloatType())) \
   .withColumn("latitude", df["latitude"].cast(FloatType())) \
   .withColumn("housingMedianAge",df["housingMedianAge"].cast(FloatType())) \
   .withColumn("totalRooms", df["totalRooms"].cast(FloatType())) \ 
   .withColumn("totalBedRooms", df["totalBedRooms"].cast(FloatType())) \ 
   .withColumn("population", df["population"].cast(FloatType())) \ 
   .withColumn("households", df["households"].cast(FloatType())) \ 
   .withColumn("medianIncome", df["medianIncome"].cast(FloatType())) \ 
   .withColumn("medianHouseValue", df["medianHouseValue"].cast(FloatType()))</code></pre>
<p class="p-margin">But these repeated calls are quite obscure, error-proof and don’t really look nice. Why don’t you write a function that can do all of this for you in a more clean way?</p>
<p class="p-margin">The following User-Defined Function (UDF) takes a DataFrame, column names, and the new data type that you want the have the columns to have. You say that for every column name, you take the column and you cast it to a new data type. Then, you return the DataFrame:</p>
<pre class="language-javascript"><code class="lang-python"># Import all from `sql.types`
from pyspark.sql.types import *

# Write a custom function to convert the data type of DataFrame columns
def convertColumn(df, names, newType):
  for name in names: 
     df = df.withColumn(name, df[name].cast(newType))
  return df 

# Assign all column names to `columns`
columns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']

# Conver the `df` columns to `FloatType()`
df = convertColumn(df, columns, FloatType())</code></pre>
<p class="p-margin">That already looks much better! You can quickly inspect the data types of <code>df</code> with the <code>printSchema()</code> method, just like you have done before.</p>
<p class="p-margin">Now that you’ve got that all sorted out, it’s time to really get started on the data exploration. You have seen that columnar access and SQL queries were two advantages of using DataFrames. Well, now it’s time to dig a little bit further into that. Let’s start small and just select two columns from <code>df</code> of which you only want to see 10 rows:</p>
<pre class="language-javascript"><code>df.select('population','totalBedRooms').show(10)</code></pre>
<p class="p-margin">This query gives you the following result:</p>
<pre class="language-javascript"><code class="lang-python">+----------+-------------+
|population|totalBedRooms|
+----------+-------------+
|     322.0|        129.0|
|    2401.0|       1106.0|
|     496.0|        190.0|
|     558.0|        235.0|
|     565.0|        280.0|
|     413.0|        213.0|
|    1094.0|        489.0|
|    1157.0|        687.0|
|    1206.0|        665.0|
|    1551.0|        707.0|
+----------+-------------+
only showing top 10 rows</code></pre>
<p class="p-margin">You can also make your queries more complex, as you see in the following example:</p>
<pre class="language-javascript"><code class="lang-python">df.groupBy("housingMedianAge").count().sort("housingMedianAge",ascending=False).show()</code></pre>
<p class="p-margin">Which gives you the following result:</p>
<pre class="language-javascript"><code class="lang-python">+----------------+-----+                                                        
|housingMedianAge|count|
+----------------+-----+
|            52.0| 1273|
|            51.0|   48|
|            50.0|  136|
|            49.0|  134|
|            48.0|  177|
|            47.0|  198|
|            46.0|  245|
|            45.0|  294|
|            44.0|  356|
|            43.0|  353|
|            42.0|  368|
|            41.0|  296|
|            40.0|  304|
|            39.0|  369|
|            38.0|  394|
|            37.0|  537|
|            36.0|  862|
|            35.0|  824|
|            34.0|  689|
|            33.0|  615|
+----------------+-----+
only showing top 20 rows</code></pre>
<p class="p-margin">Besides querying, you can also choose to describe your data and get some summary statistics. This will most definitely help you after!</p>
<pre class="language-javascript"><code class="lang-python">df.describe().show()</code></pre>
<br>
<div class="figure"><img alt="PySpark Machine Learning" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Spark+ML/SummaryStatistics-PySpark.png" loading="lazy"></div>
<br>
<p class="p-margin">Look at the minimum and maximum values of all the (numerical) attributes. You see that multiple attributes have a wide range of values: you will need to normalize your dataset.</p>
<h2 id="data-preprocessing-witha">Data Preprocessing</h2>
<p class="p-margin">With all this information that you gathered from your small exploratory data analysis, you know enough to preprocess your data to feed it to the model.</p>
<ul>
<li>You shouldn’t care about missing values; all zero values have been excluded from the data set.</li>
<li>You should probably standardize your data, as you have seen that the range of minimum and maximum values is quite big.</li>
<li>There are possibbly some additional attributes that you could add, such as a feature that registers the number of bedrooms per room or the rooms per household.</li>
<li>Your dependent variable is also quite big; To make your life easier, you’ll have to adjust the values slightly.</li>
</ul>
<div class="section level3" id="preprocessing-the-target-values">
<h3 id="preprocessing-the-target-values-first">Preprocessing the Target Values</h3>
<p class="p-margin">First, let’s start with the <code>medianHouseValue</code>, your dependent variable. To facilitate your working with the target values, you will express the house values in units of 100,000. That means that a target such as <code>452600.000000</code> should become <code>4.526</code>:</p>
<pre class="language-javascript"><code class="lang-python"># Import all from `sql.functions` 
from pyspark.sql.functions import *

# Adjust the values of `medianHouseValue`
df = df.withColumn("medianHouseValue", col("medianHouseValue")/100000)

# Show the first 2 lines of `df`
df.take(2)</code></pre>
<p class="p-margin">You can clearly see that the values have been adjusted correctly when you look at the result of the <code>take()</code> method:</p>
<pre class="language-javascript"><code class="lang-python">[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0), Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]</code></pre>
</div>
<div class="section level3" id="feature-engineering">
<h3 id="feature-engineering-nowth">Feature Engineering</h3>
<p class="p-margin">Now that you have adjusted the values in <code>medianHouseValue</code>, you can also add the additional variables that you read about above. You’re going to add the following columns to the data set:</p>
<ul>
<li><em>Rooms per household</em> which refers to the number of rooms in households per block group;</li>
<li><em>Population per household</em>, which basically gives you an indication of how many people live in households per block group; And</li>
<li><em>Bedrooms per room</em> which will give you an idea about how many rooms are bedrooms per block group;</li>
</ul>
<br>
<p class="p-margin">As you’re working with DataFrames, you can best use the <code>select()</code> method to select the columns that you’re going to be working with, namely <code>totalRooms</code>, <code>households</code>, and <code>population</code>. Additionally, you have to indicate that you’re working with columns by adding the <code>col()</code> function to your code. Otherwise, you won’t be able to do element-wise operations like the division that you have in mind for these three variables:</p>
<pre class="language-javascript"><code class="lang-python"># Import all from `sql.functions` if you haven't yet
from pyspark.sql.functions import *

# Divide `totalRooms` by `households`
roomsPerHousehold = df.select(col("totalRooms")/col("households"))

# Divide `population` by `households`
populationPerHousehold = df.select(col("population")/col("households"))

# Divide `totalBedRooms` by `totalRooms`
bedroomsPerRoom = df.select(col("totalBedRooms")/col("totalRooms"))

# Add the new columns to `df`
df = df.withColumn("roomsPerHousehold", col("totalRooms")/col("households")) \
   .withColumn("populationPerHousehold", col("population")/col("households")) \
   .withColumn("bedroomsPerRoom", col("totalBedRooms")/col("totalRooms"))
   
# Inspect the result
df.first()</code></pre>
<p class="p-margin">You see that, for the first row, there are about 6.98 rooms per household, the households in the block group consist of about 2.5 people and the amount of bedrooms is quite low with 0.14:</p>
<pre class="language-javascript"><code class="lang-python">Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)</code></pre>
<p class="p-margin">Next, -and this is already forseeing an issue that you might have when you’ll standardize the values in your data set- you’ll also re-order the values. Since you don’t want to necessarily standardize your target values, you’ll want to make sure to isolate those in your data set.</p>
<p class="p-margin">In this case, you’ll need to do this by using the <code>select()</code> method and passing the column names in the order that is more appropriate. In this case, the target variable <code>medianHouseValue</code> is put first, so that it won’t be affected by the standardization.</p>
<p class="p-margin"><strong>Note</strong> also that this is the time to leave out variables that you might not want to consider in your analysis. In this case, let’s leave out variables such as <code>longitude</code>, <code>latitude</code>, <code>housingMedianAge</code> and <code>totalRooms</code>.</p>
<pre class="language-javascript"><code class="lang-python"># Re-order and select columns
df = df.select("medianHouseValue", 
              "totalBedRooms", 
              "population", 
              "households", 
              "medianIncome", 
              "roomsPerHousehold", 
              "populationPerHousehold", 
              "bedroomsPerRoom")</code></pre>
</div>
<div class="section level3" id="standardization">
<h3 id="standardization-nowth">Standardization</h3>
<p class="p-margin">Now that you have re-ordered the data, you’re ready to normalize the data. Or almost, at least. There is just one more step that you need to go through: separating the features from the target variable. In essence, this boils down to isolating the first column in your DataFrame from the rest of the columns.</p>
<p class="p-margin">In this case, you’ll use the <code>map()</code> function that you use with RDDs to perform this action. You also see that you make use of the <code>DenseVector()</code> function. A dense vector is a local vector that is backed by a double array that represents its entry values. In other words, it's used to store arrays of values for use in PySpark.</p>
<p class="p-margin">Next, you go back to making a DataFrame out of the <code>input_data</code> and you re-label the columns by passing a list as a second argument. This list consists of the column names <code>"label"</code> and <code>"features"</code>:</p>
<pre class="language-javascript"><code class="lang-python"># Import `DenseVector`
from pyspark.ml.linalg import DenseVector

# Define the `input_data` 
input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))

# Replace `df` with the new DataFrame
df = spark.createDataFrame(input_data, ["label", "features"])</code></pre>
<p class="p-margin">Next, you can finally scale the data. You can use Spark ML to do this: this library will make machine learning on big data scalable and easy. You’ll find tools such as ML algorithms and everything you need to build practical ML pipelines. In this case, you don’t need to do that much preprocessing so a pipeline would maybe be overkill, but if you want to look into it, definitely consider visiting the <a href="https://spark.apache.org/docs/latest/ml-pipeline.html" target="_blank" rel="noopener">this page</a>.</p>
<p class="p-margin">The input columns are the features, and the output column with the rescaled that will be included in the <code>scaled_df</code> will be named <code>"features_scaled"</code>:</p>
<pre class="language-javascript"><code class="lang-python"># Import `StandardScaler` 
from pyspark.ml.feature import StandardScaler

# Initialize the `standardScaler`
standardScaler = StandardScaler(inputCol="features", outputCol="features_scaled")

# Fit the DataFrame to the scaler
scaler = standardScaler.fit(df)

# Transform the data in `df` with the scaler
scaled_df = scaler.transform(df)

# Inspect the result
scaled_df.take(2)</code></pre>
<p class="p-margin">Let’s take a look at your DataFrame and the result. You see that, indeed, a third column <code>features_scaled</code> was added to your DataFrame, which you can use to compare with <code>features</code>:</p>
<pre class="language-javascript"><code class="lang-python">[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])), Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]</code></pre>
<p class="p-margin"><strong>Note</strong> that these lines of code are very similar to what you would be doing in Scikit-Learn.</p>
<h2 id="building-a-machine-learning-model-with-spark-ml-witha">Building a Machine Learning Model with Spark ML</h2>
<p class="p-margin">With all the preprocessing done, it’s finally time to start building your Linear Regression model! Just like always, you first need to split the data into training and test sets. Luckily, this is no issue with the&nbsp;<code>randomSplit()</code> method:</p>
<pre class="language-javascript"><code class="lang-python"># Split the data into train and test sets
train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)</code></pre>
<p class="p-margin">You pass in a list with two numbers that represent the size that you want your training and test sets to have and a seed, which is needed for reproducibility reasons. If you want to know more about this, consider DataCamp’s <a href="https://www.datacamp.com/tutorial/machine-learning-python" target="_blank" rel="noopener">Python Machine Learning Tutorial</a>.</p>
<p class="p-margin">Then, without further ado, you can make your model!</p>
<p class="p-margin"><strong>Note</strong> that the argument <code>elasticNetParam</code> corresponds to α or the vertical intercept and that the <code>regParam</code> or the regularization paramater corresponds to λ. Go <a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression" target="_blank" rel="noopener">here</a> for more information.</p>
<pre class="language-javascript"><code class="lang-python"># Import `LinearRegression`
from pyspark.ml.regression import LinearRegression

# Initialize `lr`
lr = LinearRegression(labelCol="label", maxIter=10, regParam=0.3, elasticNetParam=0.8)

# Fit the data to the model
linearModel = lr.fit(train_data)</code></pre>
<p class="p-margin">With your model in place, you can generate predictions for your test data: use the <code>transform()</code> method to predict the labels for your <code>test_data</code>. Then, you can use RDD operations to extract the predictions as well as the true labels from the DataFrame and zip these two values together in a list called <code>predictionAndLabel</code>.</p>
<p class="p-margin">Lastly, you can then inspect the predicted and real values by simply accessing the list with square brackets <code>[]</code>:</p>
<pre class="language-javascript"><code class="lang-python"># Generate predictions
predicted = linearModel.transform(test_data)

# Extract the predictions and the "known" correct labels
predictions = predicted.select("prediction").rdd.map(lambda x: x[0])
labels = predicted.select("label").rdd.map(lambda x: x[0])

# Zip `predictions` and `labels` into a list
predictionAndLabel = predictions.zip(labels).collect()

# Print out first 5 instances of `predictionAndLabel` 
predictionAndLabel[:5]</code></pre>
<p class="p-margin">You’ll see the following real and predicted values (in that order):</p>
<pre class="language-javascript"><code class="lang-python">[(1.4491508524918457, 0.14999), (1.5705029404692372, 0.14999), (2.148727956912464, 0.14999), (1.5831547768979277, 0.344), (1.5182107797955968, 0.398)]</code></pre>
<h2 id="evaluating-the-model-looki">Evaluating the Model</h2>
<p class="p-margin">Looking at predicted values is one thing, but another and better thing is looking at some metrics to get a better idea of how good your model actually is. You can first start by printing out the coefficients and the intercept of the model:</p>
<pre class="language-javascript"><code class="lang-python"># Coefficients for the model
linearModel.coefficients

# Intercept for the model
linearModel.intercept</code></pre>
<p class="p-margin">Which gives you the following result:</p>
<pre class="language-javascript"><code class="lang-python"># The coefficients
[0.0,0.0,0.0,0.276239709215,0.0,0.0,0.0]

# The intercept
0.990399577462</code></pre>
<p class="p-margin">Next, you can also use the <code>summary</code> attribute to pull up the <code>rootMeanSquaredError</code> and the <code>r2</code>:</p>
<pre class="language-javascript"><code class="lang-python"># Get the RMSE
linearModel.summary.rootMeanSquaredError

# Get the R2
linearModel.summary.r2</code></pre>
<ul>
<li>
<p class="p-margin">The RMSE measures how much error there is between two datasets comparing a predicted value and an observed or known value. The smaller an RMSE value, the closer predicted and observed values are.</p>
</li>
<li>
<p class="p-margin">The R2 (“R squared”) or the coefficient of determination is a measure that shows how close the data are to the fitted regression line. This score will always be between 0 and a 100% (or 0 to 1 in this case), where 0% indicates that the model explains none of the variability of the response data around its mean, and 100% indicates the opposite: it explains all the variability. That means that, in general, the higher the R-squared, the better the model fits your data.</p>
</li>
</ul>
<p class="p-margin">You'll get back the following result:</p>
<pre class="language-javascript"><code class="lang-python"># RMSE
0.8692118678997669

# R2
0.4240895287218379
</code></pre>
</div>
<div class="section level2" id="before-you-go">
<p class="p-margin">There's definitely some improvements needed to your model! If you want to continue with this model, you can play around with the parameters that you passed to your model, the variables that you included in your original DataFrame, .... But this is where the tutorial ends for now!&nbsp;</p>
<h2 id="before-you-go…-befor">Before you Go…</h2>
<p class="p-margin">Before you go, make sure to stop the SparkSession with the following line of code:</p>
<pre class="language-javascript"><code>spark.stop()</code></pre>
</div>
<div class="section level2" id="taking-big-data-further">
<h2 id="taking-big-data-further-congr">Taking Big Data Further</h2>
<p class="p-margin">Congrats! You have made it to the end of this tutorial, where you learned how to make a linear regression model with the help of Spark ML.</p>
<p class="p-margin">If you are interested in learning more about PySpark, consider taking DataCamp’s <a href="https://www.datacamp.com/courses/introduction-to-pyspark" target="_blank" rel="noopener">Introduction to PySpark</a> course and take a look at the <a href="https://www.datacamp.com/tutorial/apache-spark-tutorial-machine-learning" target="_blank" rel="noopener">Apache Spark Tutorial: ML with PySpark</a>.</p>
</div>
</div></span></div></div><style data-emotion="css v5yd45">.css-v5yd45{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:32px;}</style><style data-emotion="css 15nvpkw">.css-15nvpkw{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:32px;}</style><style data-emotion="css 1ydys7c">.css-1ydys7c{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:32px;}</style><div class="css-1ydys7c"><style data-emotion="css h8aqnd">.css-h8aqnd{color:#05192d;font-size:0.75rem;font-weight:800;letter-spacing:1.5px;margin-bottom:8px;text-transform:uppercase;}</style><style data-emotion="css 13e6sno">.css-13e6sno{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;color:#05192d;font-size:0.75rem;font-weight:800;letter-spacing:1.5px;margin-bottom:8px;text-transform:uppercase;}</style><span class="css-13e6sno">Topics</span><style data-emotion="css gu67so">.css-gu67so{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><style data-emotion="css 59n9ud">.css-59n9ud{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><style data-emotion="css 1sz6xm9">.css-1sz6xm9{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><div class="css-1sz6xm9"><a data-trackid="media-visit-category-tutorial-python" href="/tutorial/category/python"><style data-emotion="css f3ofnt">.css-f3ofnt{background-color:#EFEFF5;border-radius:4px;color:#6B6A67;cursor:pointer;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:0.875rem;font-weight:400;letter-spacing:0.5px;line-height:21px;margin-bottom:4px;margin-right:8px;padding-left:8px;padding-right:8px;padding-top:4px;padding-bottom:4px;width:-webkit-max-content;width:-moz-max-content;width:max-content;}.css-f3ofnt:active,.css-f3ofnt:focus,.css-f3ofnt:hover{background-color:#E1E1E8;}</style><style data-emotion="css 1ezc6sm">.css-1ezc6sm{box-sizing:border-box;margin:0;min-width:0;display:inline-block;vertical-align:baseline;font-size:0;font-weight:700;white-space:nowrap;padding-left:1px;padding-right:1px;border-radius:2px;color:#ffffff;background-color:#33e;background-color:#EFEFF5;border-radius:4px;color:#6B6A67;cursor:pointer;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-size:0.875rem;font-weight:400;letter-spacing:0.5px;line-height:21px;margin-bottom:4px;margin-right:8px;padding-left:8px;padding-right:8px;padding-top:4px;padding-bottom:4px;width:-webkit-max-content;width:-moz-max-content;width:max-content;}.css-1ezc6sm:active,.css-1ezc6sm:focus,.css-1ezc6sm:hover{background-color:#E1E1E8;}</style><div class="css-1ezc6sm">Python</div></a><a data-trackid="media-visit-category-tutorial-data-science" href="/tutorial/category/data-science"><div class="css-1ezc6sm">Data Science</div></a><a data-trackid="media-visit-category-tutorial-machine-learning" href="/tutorial/category/machine-learning"><div class="css-1ezc6sm">Machine Learning</div></a></div></div></div><style data-emotion="css 1fub4eg">.css-1fub4eg{-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;height:100%;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:start;justify-content:start;margin-bottom:0;margin-left:0;margin-right:0;overflow-x:hidden;overflow-y:auto;padding-left:0;position:static;padding-right:0;width:100%;}@media screen and (min-width: 1200px){.css-1fub4eg{margin-left:0;margin-right:16px;padding-left:16px;position:-webkit-sticky;position:sticky;padding-right:0;width:180px;}}</style><aside class="css-1fub4eg"><div class="css-1ng2sso"><div class="css-106n4fp"><div class="css-er0ssy"><style data-emotion="css 1haccz9">.css-1haccz9{border:1px solid;border-color:#05192d;display:block;margin-bottom:16px;opacity:0.1;}@media screen and (min-width: 1200px){.css-1haccz9{display:none;}}</style><style data-emotion="css 1bh1nbf">.css-1bh1nbf{box-sizing:border-box;margin:0;min-width:0;color:gray;margin:0;margin-top:2px;margin-bottom:2px;border:0;border-bottom:1px solid;border:1px solid;border-color:#05192d;display:block;margin-bottom:16px;opacity:0.1;}@media screen and (min-width: 1200px){.css-1bh1nbf{display:none;}}</style><hr class="css-1bh1nbf"/><style data-emotion="css 1yz7e9k">.css-1yz7e9k{box-sizing:border-box;margin:0;min-width:0;}</style><div class="css-1yz7e9k"><div data-trackid="media-visit-author-profile" class="css-1yz7e9k"><style data-emotion="css 6aowfx">.css-6aowfx{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:12px;}@media screen and (min-width: 992px){.css-6aowfx{-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}}</style><style data-emotion="css 12n25ok">.css-12n25ok{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:12px;}@media screen and (min-width: 992px){.css-12n25ok{-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}}</style><style data-emotion="css 1d9l0ti">.css-1d9l0ti{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:12px;}@media screen and (min-width: 992px){.css-1d9l0ti{-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}}</style><div class="css-1d9l0ti"><style data-emotion="css 1wi30uy">.css-1wi30uy{border-radius:50%;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:64px;margin-bottom:0;margin-top:0;overflow:hidden;width:64px;}</style><style data-emotion="css ww9z0u">.css-ww9z0u{box-sizing:border-box;margin:0;min-width:0;border-radius:50%;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:64px;margin-bottom:0;margin-top:0;overflow:hidden;width:64px;}</style><div class="hide-in-percy css-ww9z0u"><style data-emotion="css xah9so">.css-xah9so{object-fit:cover;}</style><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:64px;height:64px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="Karlijn Willems&#x27;s photo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="css-xah9so" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Karlijn Willems&#x27;s photo" loading="lazy" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-xah9so" srcSet="https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png 1x, https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png 2x" src="https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png"/></noscript></span></div><div class="css-er0ssy"><style data-emotion="css 90lrac">.css-90lrac{color:#65707C;cursor:pointer;font-size:0.875rem;font-weight:800;margin-bottom:4px;-webkit-text-decoration:none;text-decoration:none;}</style><style data-emotion="css 19xhyk3">.css-19xhyk3{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;color:#65707C;cursor:pointer;font-size:0.875rem;font-weight:800;margin-bottom:4px;-webkit-text-decoration:none;text-decoration:none;}.css-19xhyk3:active,.css-19xhyk3:focus,.css-19xhyk3:hover{-webkit-text-decoration:underline;text-decoration:underline;}</style><a class="css-19xhyk3" data-trackid="media-visit-author-profile" href="/portfolio/karlijn" target="_self">Karlijn Willems</a><style data-emotion="css 18d9s7l">.css-18d9s7l{color:#65707C;font-size:0.875rem;font-weight:400;line-height:1.3;margin-bottom:0;}@media screen and (min-width: 992px){.css-18d9s7l{margin-bottom:40px;}}</style><style data-emotion="css w8mez2">.css-w8mez2{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;color:#65707C;font-size:0.875rem;font-weight:400;line-height:1.3;margin-bottom:0;}@media screen and (min-width: 992px){.css-w8mez2{margin-bottom:40px;}}</style><span class="css-w8mez2">Former Data Journalist at DataCamp | Manager at NextWave Consulting</span></div></div><style data-emotion="css z72akh">.css-z72akh{border:1px solid;border-color:#05192d;display:block;margin-bottom:24px;margin-top:10px;opacity:0.1;}@media screen and (min-width: 1200px){.css-z72akh{display:none;}}</style><style data-emotion="css 196ty0e">.css-196ty0e{box-sizing:border-box;margin:0;min-width:0;color:gray;margin:0;margin-top:2px;margin-bottom:2px;border:0;border-bottom:1px solid;border:1px solid;border-color:#05192d;display:block;margin-bottom:24px;margin-top:10px;opacity:0.1;}@media screen and (min-width: 1200px){.css-196ty0e{display:none;}}</style><hr class="css-196ty0e"/></div></div><style data-emotion="css 7pzpbu">.css-7pzpbu{display:none;}@media screen and (min-width: 1200px){.css-7pzpbu{display:block;}}</style><style data-emotion="css 1wtm0ss">.css-1wtm0ss{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:40px;}</style><style data-emotion="css drply8">.css-drply8{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:40px;}</style><style data-emotion="css 16fimdw">.css-16fimdw{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:40px;}</style><div class="css-16fimdw"><style data-emotion="css 1upa8m5">.css-1upa8m5{color:#05192d;font-size:0.75rem;font-weight:800;letter-spacing:1.5px;margin-bottom:20px;text-transform:uppercase;}</style><style data-emotion="css qb47je">.css-qb47je{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;color:#05192d;font-size:0.75rem;font-weight:800;letter-spacing:1.5px;margin-bottom:20px;text-transform:uppercase;}</style><span class="css-qb47je">Topics</span><style data-emotion="css 1giq0d6">.css-1giq0d6{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><style data-emotion="css 3gib0z">.css-3gib0z{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><style data-emotion="css l6r8ug">.css-l6r8ug{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><div class="css-l6r8ug"><a data-trackid="media-visit-category-tutorial-python" href="/tutorial/category/python"><div class="css-1ezc6sm">Python</div></a><a data-trackid="media-visit-category-tutorial-data-science" href="/tutorial/category/data-science"><div class="css-1ezc6sm">Data Science</div></a><a data-trackid="media-visit-category-tutorial-machine-learning" href="/tutorial/category/machine-learning"><div class="css-1ezc6sm">Machine Learning</div></a></div></div><style data-emotion="css 1kxjyw9">.css-1kxjyw9{display:none;margin-bottom:16px;}@media screen and (min-width: 1200px){.css-1kxjyw9{display:block;}}</style><style data-emotion="css 1cvapwe">.css-1cvapwe{box-sizing:border-box;margin:0;min-width:0;display:none;margin-bottom:16px;}@media screen and (min-width: 1200px){.css-1cvapwe{display:block;}}</style><div class="css-1cvapwe"><style data-emotion="css 9v5gwp">.css-9v5gwp{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding:0;}</style><style data-emotion="css rnnxue">.css-rnnxue{box-sizing:border-box;margin:0;min-width:0;width:100%;max-width:container;margin-left:auto;margin-right:auto;max-width:1172px;padding-left:16px;padding-right:16px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding:0;}</style><div class="css-rnnxue" data-testid="further-resource-recommendation-aside"><style data-emotion="css b2wpop">.css-b2wpop{-webkit-column-gap:24px;column-gap:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:repeat(1, 1fr);margin-bottom:32px;row-gap:20px;}.css-b2wpop div:nth-of-type(4){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}.css-b2wpop div:nth-of-type(5){display:none;}.css-b2wpop div:nth-of-type(6){display:none;}@media screen and (min-width: 768px){.css-b2wpop{grid-template-columns:repeat(2, 1fr);}}@media screen and (min-width: 1200px){.css-b2wpop{grid-template-columns:repeat(3, 1fr);}}</style><style data-emotion="css 7ad6ds">.css-7ad6ds{box-sizing:border-box;margin:0;min-width:0;-webkit-column-gap:24px;column-gap:24px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:repeat(1, 1fr);margin-bottom:32px;row-gap:20px;}.css-7ad6ds div:nth-of-type(4){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}.css-7ad6ds div:nth-of-type(5){display:none;}.css-7ad6ds div:nth-of-type(6){display:none;}@media screen and (min-width: 768px){.css-7ad6ds{grid-template-columns:repeat(2, 1fr);}}@media screen and (min-width: 1200px){.css-7ad6ds{grid-template-columns:repeat(3, 1fr);}}</style><div class="css-7ad6ds"><style data-emotion="css 1kgv0f6">.css-1kgv0f6{background-color:inherit;border-radius:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:none;grid-template-rows:none;}.css-1kgv0f6:hover{cursor:pointer;}@media screen and (min-width: 600px){.css-1kgv0f6{background-color:#ffffff;}}</style><style data-emotion="css 1o693db">.css-1o693db{box-sizing:border-box;margin:0;min-width:0;background-color:inherit;border-radius:4px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:none;grid-template-rows:none;}.css-1o693db:hover{cursor:pointer;}@media screen and (min-width: 600px){.css-1o693db{background-color:#ffffff;}}</style><div data-trackid="media-visit-new-resource-rhs" class="css-1o693db"><style data-emotion="css 3dqd93">.css-3dqd93{border-radius:4px;grid-column-end:2;grid-column-start:1;grid-row-end:3;grid-row-start:1;height:62px;margin-bottom:8px;overflow:hidden;width:100px;}@media screen and (min-width: 600px){.css-3dqd93{border-radius:0;}}</style><style data-emotion="css snetr3">.css-snetr3{box-sizing:border-box;margin:0;min-width:0;border-radius:4px;grid-column-end:2;grid-column-start:1;grid-row-end:3;grid-row-start:1;height:62px;margin-bottom:8px;overflow:hidden;width:100px;}@media screen and (min-width: 600px){.css-snetr3{border-radius:0;}}</style><div class="hide-in-percy css-snetr3"><style data-emotion="css 1qucg2m">.css-1qucg2m{border-radius:4px;height:62px;}@media screen and (min-width: 600px){.css-1qucg2m{border-radius:0;height:auto;}}</style><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#EFEBE4" height="180" width="336"></rect><rect fill="#DD3400" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#FFEC3C" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#DD3400" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#FFEC3C" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-pyspark-cheat-sheet-spark-dataframes-in-python" href="/cheat-sheet/pyspark-cheat-sheet-spark-dataframes-in-python" class="css-xlmvza"><style data-emotion="css 1j3bu79">.css-1j3bu79{color:#5D6A77;font-size:14px;font-weight:400;line-height:1.3;-webkit-text-decoration:none;text-decoration:none;}.css-1j3bu79:active,.css-1j3bu79:focus,.css-1j3bu79:hover{color:#444C54;-webkit-text-decoration:none;text-decoration:none;}</style><style data-emotion="css 3mmz8b">.css-3mmz8b{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;color:#5D6A77;font-size:14px;font-weight:400;line-height:1.3;-webkit-text-decoration:none;text-decoration:none;}.css-3mmz8b:active,.css-3mmz8b:focus,.css-3mmz8b:hover{color:#444C54;-webkit-text-decoration:none;text-decoration:none;}</style><h3 class="css-3mmz8b">PySpark Cheat Sheet: Spark DataFrames in Python</h3></a></div><div data-trackid="media-visit-new-resource-rhs" class="css-1o693db"><div class="hide-in-percy css-snetr3"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#EFEBE4" height="180" width="336"></rect><rect fill="#FF931E" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#F7F3EB" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#000820" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#D87300" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-pyspark-cheat-sheet-spark-in-python" href="/cheat-sheet/pyspark-cheat-sheet-spark-in-python" class="css-xlmvza"><h3 class="css-3mmz8b">PySpark Cheat Sheet: Spark in Python </h3></a></div><div data-trackid="media-visit-new-resource-rhs" class="css-1o693db"><div class="hide-in-percy css-snetr3"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#05192D" height="180" width="336"></rect><rect fill="#B08EFF" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#7933FF" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#72E5FE" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#5EB1FF" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#B08EFF" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#72E5FE" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-pyspark-tutorial-getting-started-with-pyspark" href="/tutorial/pyspark-tutorial-getting-started-with-pyspark" class="css-xlmvza"><h3 class="css-3mmz8b">Pyspark Tutorial: Getting Started with Pyspark </h3></a></div><div data-trackid="media-visit-new-resource-rhs" class="css-1o693db"><div class="hide-in-percy css-snetr3"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#F7F3EB" height="180" width="336"></rect><rect fill="#000820" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#00C74E" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#D87300" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#00C74E" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-installation-of-pyspark" href="/tutorial/installation-of-pyspark" class="css-xlmvza"><h3 class="css-3mmz8b">Installation of PySpark (All operating systems)</h3></a></div><div data-trackid="media-visit-new-resource-rhs" class="css-1o693db"><div class="hide-in-percy css-snetr3"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#72E5FE" height="180" width="336"></rect><rect fill="#7933FF" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#5EB1FF" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#7933FF" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#5EB1FF" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-snowpark-snoflake-introduction" href="/tutorial/snowpark-snoflake-introduction" class="css-xlmvza"><h3 class="css-3mmz8b">Snowflake Snowpark: A Comprehensive Introduction</h3></a></div><div data-trackid="media-visit-new-resource-rhs" class="css-1o693db"><div class="hide-in-percy css-snetr3"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#EFEBE4" height="180" width="336"></rect><rect fill="#72E5FE" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#5EB1FF" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#000820" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#D87300" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#5646A5" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-machine-learning-python" href="/tutorial/machine-learning-python" class="css-xlmvza"><h3 class="css-3mmz8b">Python Machine Learning: Scikit-Learn Tutorial</h3></a></div></div></div></div></div></div></div></aside></div><style data-emotion="css 14k1vhd">.css-14k1vhd{margin-bottom:40px;margin-top:32px;overflow-x:hidden;}@media screen and (min-width: 768px){.css-14k1vhd{margin-bottom:unset;}}</style><style data-emotion="css 9mbwio">.css-9mbwio{box-sizing:border-box;margin:0;min-width:0;margin-bottom:40px;margin-top:32px;overflow-x:hidden;}@media screen and (min-width: 768px){.css-9mbwio{margin-bottom:unset;}}</style><section class="css-9mbwio"><style data-emotion="css viwwn9">.css-viwwn9{padding:0;position:relative;}</style><style data-emotion="css kwdxbc">.css-kwdxbc{box-sizing:border-box;margin:0;min-width:0;width:100%;max-width:container;margin-left:auto;margin-right:auto;max-width:1172px;padding-left:16px;padding-right:16px;padding:0;position:relative;}</style><div class="css-kwdxbc"><style data-emotion="css 146qejk">.css-146qejk{margin-bottom:28px;}</style><style data-emotion="css 1x15cwx">.css-1x15cwx{box-sizing:border-box;margin:0;min-width:0;margin-bottom:28px;}</style><div class="css-1x15cwx"><style data-emotion="css 1h9yah9">.css-1h9yah9{margin-bottom:8px;}</style><style data-emotion="css i9ahla">.css-i9ahla{box-sizing:border-box;margin:0;min-width:0;font-size:1.75rem;font-weight:700;letter-spacing:-0.5px;line-height:1.2;margin-top:0;margin-bottom:8px;}@media screen and (min-width: 992px){.css-i9ahla{font-size:2rem;}}</style><span class="css-i9ahla" id="courses-carousel-Wkew2JwuhX"><p class="p-margin">Learn more about Python and PySpark</p></span></div><style data-emotion="css 1sjaeuc">.css-1sjaeuc{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:24px;}</style><style data-emotion="css 12bfbc4">.css-12bfbc4{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:24px;}</style><style data-emotion="css 1cfi7ez">.css-1cfi7ez{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:24px;}</style><div class="css-1cfi7ez"><style data-emotion="css z2psi8">.css-z2psi8{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:nowrap;-webkit-flex-wrap:nowrap;-ms-flex-wrap:nowrap;flex-wrap:nowrap;gap:16px;scroll-snap-type:x mandatory;}.css-z2psi8 article{border:1px solid rgba(5, 25, 45, 0.1);box-shadow:0 1px 3px 0 rgb(0 0 0 / 15%);-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;gap:24px;max-width:100%;min-width:0;margin-top:4px;-webkit-transform:translateY(-4px);-moz-transform:translateY(-4px);-ms-transform:translateY(-4px);transform:translateY(-4px);width:100%;}@media screen and (min-width: 600px){.css-z2psi8 article{max-width:326px;}}@media screen and (min-width: 768px){.css-z2psi8 article{max-width:326px;}}@media screen and (min-width: 992px){.css-z2psi8 article{max-width:374px;}}@media screen and (min-width: 768px){.css-z2psi8{gap:24px;}}</style><style data-emotion="css 16czm15">.css-16czm15{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:nowrap;-webkit-flex-wrap:nowrap;-ms-flex-wrap:nowrap;flex-wrap:nowrap;gap:16px;scroll-snap-type:x mandatory;}.css-16czm15 article{border:1px solid rgba(5, 25, 45, 0.1);box-shadow:0 1px 3px 0 rgb(0 0 0 / 15%);-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;gap:24px;max-width:100%;min-width:0;margin-top:4px;-webkit-transform:translateY(-4px);-moz-transform:translateY(-4px);-ms-transform:translateY(-4px);transform:translateY(-4px);width:100%;}@media screen and (min-width: 600px){.css-16czm15 article{max-width:326px;}}@media screen and (min-width: 768px){.css-16czm15 article{max-width:326px;}}@media screen and (min-width: 992px){.css-16czm15 article{max-width:374px;}}@media screen and (min-width: 768px){.css-16czm15{gap:24px;}}</style><style data-emotion="css pgc8ay">.css-pgc8ay{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:nowrap;-webkit-flex-wrap:nowrap;-ms-flex-wrap:nowrap;flex-wrap:nowrap;gap:16px;scroll-snap-type:x mandatory;}.css-pgc8ay article{border:1px solid rgba(5, 25, 45, 0.1);box-shadow:0 1px 3px 0 rgb(0 0 0 / 15%);-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;-webkit-flex-basis:100%;-ms-flex-preferred-size:100%;flex-basis:100%;gap:24px;max-width:100%;min-width:0;margin-top:4px;-webkit-transform:translateY(-4px);-moz-transform:translateY(-4px);-ms-transform:translateY(-4px);transform:translateY(-4px);width:100%;}@media screen and (min-width: 600px){.css-pgc8ay article{max-width:326px;}}@media screen and (min-width: 768px){.css-pgc8ay article{max-width:326px;}}@media screen and (min-width: 992px){.css-pgc8ay article{max-width:374px;}}@media screen and (min-width: 768px){.css-pgc8ay{gap:24px;}}</style><div class="css-pgc8ay"><style data-emotion="css 17olifz">.css-17olifz{background:#FFFFFF;border:1px;border-radius:4px;box-shadow:0px 0px 2px 0px rgba(5, 25, 45, 0.3);cursor:pointer;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;max-height:360px;min-width:286px;padding:24px;position:relative;}@media screen and (min-width: 768px){.css-17olifz{min-width:312px;}}</style><style data-emotion="css 1er034h">.css-1er034h{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background:#FFFFFF;border:1px;border-radius:4px;box-shadow:0px 0px 2px 0px rgba(5, 25, 45, 0.3);cursor:pointer;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;max-height:360px;min-width:286px;padding:24px;position:relative;}@media screen and (min-width: 768px){.css-1er034h{min-width:312px;}}</style><style data-emotion="css 13wxmaz">.css-13wxmaz{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background:#FFFFFF;border:1px;border-radius:4px;box-shadow:0px 0px 2px 0px rgba(5, 25, 45, 0.3);cursor:pointer;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;max-height:360px;min-width:286px;padding:24px;position:relative;}@media screen and (min-width: 768px){.css-13wxmaz{min-width:312px;}}</style><article data-testid="course-card" data-trackid="introduction-to-pyspark" class="css-13wxmaz"><style data-emotion="css 1vb6nck">.css-1vb6nck{overflow:hidden;}</style><span class="css-1vb6nck"><style data-emotion="css elqpnp">.css-elqpnp{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;gap:6px;margin-bottom:16px;text-transform:uppercase;}</style><style data-emotion="css 1rcv7o">.css-1rcv7o{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;gap:6px;margin-bottom:16px;text-transform:uppercase;}</style><style data-emotion="css yh6ywx">.css-yh6ywx{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;gap:6px;margin-bottom:16px;text-transform:uppercase;}</style><div class="css-yh6ywx"><svg aria-hidden="true" height="14" width="14" viewBox="0 0 18 18" class="css-6su6fj"><path fill="currentColor" d="M4 12h10V2H4v10Zm12 2v4H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h12v14Zm-2 0H4v2h10v-2ZM7 4h4a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2Z"></path></svg><style data-emotion="css 1hqnoz0">.css-1hqnoz0{color:#5D6A77;}</style><style data-emotion="css 1vy5y1i">.css-1vy5y1i{box-sizing:border-box;margin:0;min-width:0;font-size:0.75rem;font-weight:700;letter-spacing:2px;line-height:1.2;margin-top:0;text-transform:uppercase;color:#5D6A77;}</style><p class="css-1vy5y1i">Course</p></div><style data-emotion="css 1fbuti">.css-1fbuti{margin-bottom:12px;}</style><style data-emotion="css 1w2p6ll">.css-1w2p6ll{box-sizing:border-box;margin:0;min-width:0;font-size:1.25rem;font-weight:700;letter-spacing:-0.5px;line-height:1.2;margin-top:0;margin-bottom:12px;}</style><h3 class="css-1w2p6ll"><style data-emotion="css 1531qan">.css-1531qan{-webkit-text-decoration:none;text-decoration:none;color:inherit;}</style><a href="/courses/introduction-to-pyspark" class="css-1531qan">Introduction to PySpark</a></h3><style data-emotion="css set6so">.css-set6so{-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:16px;margin-bottom:12px;}</style><style data-emotion="css l8w85a">.css-l8w85a{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:16px;margin-bottom:12px;}</style><style data-emotion="css a2vx78">.css-a2vx78{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:16px;margin-bottom:12px;}</style><div class="css-a2vx78"><style data-emotion="css i7f8ey">.css-i7f8ey{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:4px;}</style><style data-emotion="css 1p8di5r">.css-1p8di5r{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:4px;}</style><style data-emotion="css 4yrfe3">.css-4yrfe3{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:4px;}</style><div class="css-4yrfe3"><style data-emotion="css 1nidj1s">.css-1nidj1s{color:#848492;}</style><svg viewBox="0 0 18 18" aria-hidden="true" class="css-1nidj1s" height="12" role="img" width="12"><path fill="currentColor" d="M7.98 9.022L8 4.995a1 1 0 012 .01l-.016 3.022H12a1 1 0 010 2H8.98a1 1 0 01-1-1.005zM9 16A7 7 0 109 2a7 7 0 000 14zm0 2A9 9 0 119 0a9 9 0 010 18z" fill-rule="evenodd"></path></svg><style data-emotion="css 1cvgzax">.css-1cvgzax{color:#5D6A77;font-size:14px;text-align:center;}</style><style data-emotion="css 1tlx3u4">.css-1tlx3u4{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;color:#5D6A77;font-size:14px;text-align:center;}</style><span class="css-1tlx3u4">4 hr</span></div><div class="css-4yrfe3"><svg viewBox="0 0 18 18" aria-hidden="true" class="css-1nidj1s" height="12" role="img" width="12"><path fill="currentColor" d="M9 17v-5a1 1 0 00-1-1H4a1 1 0 00-1 1v5c0 .667-.333 1-1 1s-1-.333-1-1v-5a3 3 0 013-3h4a3 3 0 013 3v5c0 .667-.333 1-1 1-.666 0-1-.333-1-1zm6 0v-3a1 1 0 00-1-1h-1c-.667 0-1-.333-1-1s.333-1 1-1h1a3 3 0 013 3v3c0 .667-.333 1-1 1-.666 0-1-.333-1-1zM6 6a2 2 0 100-4 2 2 0 000 4zm0 2a4 4 0 110-8 4 4 0 010 8zm8 0a1 1 0 100-2 1 1 0 000 2zm0 2a3 3 0 110-6 3 3 0 010 6z" fill-rule="evenodd"></path></svg><span class="css-1tlx3u4">134.8K</span></div></div><style data-emotion="css 1pu6a0g">.css-1pu6a0g{overflow:hidden;text-overflow:ellipsis;}</style><style data-emotion="css klgjx2">.css-klgjx2{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;overflow:hidden;text-overflow:ellipsis;}</style><span class="css-klgjx2">Learn to implement distributed data management and machine learning in Spark using the PySpark package.</span></span><style data-emotion="css 1yxizsn">.css-1yxizsn{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;margin-top:12px;}</style><style data-emotion="css setllo">.css-setllo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;margin-top:12px;}</style><style data-emotion="css ckijet">.css-ckijet{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;margin-top:12px;}</style><div class="css-ckijet"><style data-emotion="css 1wlwn5i">.css-1wlwn5i{-webkit-text-decoration:none;text-decoration:none;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;}.css-1wlwn5i:active,.css-1wlwn5i:focus,.css-1wlwn5i:hover{color:#5D6A77;-webkit-text-decoration:underline;text-decoration:underline;}</style><a data-trackid="inline-course-card-view-details" class="css-1wlwn5i" href="/courses/introduction-to-pyspark"><style data-emotion="css 1c24ws2">.css-1c24ws2{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;gap:7px;}</style><style data-emotion="css bj7zux">.css-bj7zux{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;gap:7px;}</style><style data-emotion="css 1il7fq1">.css-1il7fq1{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#5D6A77;gap:7px;}</style><div class="css-1il7fq1"><style data-emotion="css 1awoypf">.css-1awoypf{color:#5D6A77;font-weight:700;}</style><style data-emotion="css e1mss3">.css-e1mss3{box-sizing:border-box;margin:0;min-width:0;font-size:0.875rem;line-height:1.5;margin-top:0;color:#5D6A77;font-weight:700;}</style><span class="css-e1mss3">See Details</span><svg viewBox="0 0 18 18" aria-hidden="false" height="18" role="img" width="18"><title>Right Arrow</title><path fill="currentColor" d="M13.58 10L2 10.002a1 1 0 010-2L13.59 8l-3.285-3.296a.998.998 0 111.417-1.41l4.93 4.948a.998.998 0 01-.012 1.527l-4.933 4.938a1 1 0 01-1.414 0 1.002 1.002 0 010-1.416l3.287-3.29z" fill-rule="evenodd"></path></svg></div></a><style data-emotion="css 1dsfrjc">.css-1dsfrjc{border:2px solid #848492;-webkit-transform:scale(0.85);-moz-transform:scale(0.85);-ms-transform:scale(0.85);transform:scale(0.85);}@media screen and (min-width: 768px){.css-1dsfrjc{-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}</style><style data-emotion="css 1b8uj2l">.css-1b8uj2l{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--main, #05192D);font-size:var(--wf-button--medium, 14px);height:36px;min-width:36px;width:auto;padding-left:16px;padding-right:16px;border-color:var(--wf-border-color--interactive, rgba(48, 57, 105, 0.6));border:2px solid #848492;-webkit-transform:scale(0.85);-moz-transform:scale(0.85);-ms-transform:scale(0.85);transform:scale(0.85);}.css-1b8uj2l::before{border-radius:2px;content:"";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-1b8uj2l::before{border-radius:2px;margin:0;}.css-1b8uj2l:active{background-color:transparent;}.css-1b8uj2l:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-1b8uj2l:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-1b8uj2l >*{z-index:1;}@media screen and (min-width: 768px){.css-1b8uj2l{-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}</style><a data-trackid="inline-course-card-start-course" href="/users/sign_up?redirect=%2Fcourses%2Fintroduction-to-pyspark%2Fcontinue" class="css-1b8uj2l"><style data-emotion="css 61bni1">.css-61bni1{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:100%;gap:8px;}</style><span class="css-61bni1">Start Course</span></a></div></article><article data-testid="course-card" data-trackid="machine-learning-with-pyspark" class="css-13wxmaz"><span class="css-1vb6nck"><div class="css-yh6ywx"><svg aria-hidden="true" height="14" width="14" viewBox="0 0 18 18" class="css-6su6fj"><path fill="currentColor" d="M4 12h10V2H4v10Zm12 2v4H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h12v14Zm-2 0H4v2h10v-2ZM7 4h4a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2Z"></path></svg><p class="css-1vy5y1i">Course</p></div><h3 class="css-1w2p6ll"><a href="/courses/machine-learning-with-pyspark" class="css-1531qan">Machine Learning with PySpark</a></h3><div class="css-a2vx78"><div class="css-4yrfe3"><svg viewBox="0 0 18 18" aria-hidden="true" class="css-1nidj1s" height="12" role="img" width="12"><path fill="currentColor" d="M7.98 9.022L8 4.995a1 1 0 012 .01l-.016 3.022H12a1 1 0 010 2H8.98a1 1 0 01-1-1.005zM9 16A7 7 0 109 2a7 7 0 000 14zm0 2A9 9 0 119 0a9 9 0 010 18z" fill-rule="evenodd"></path></svg><span class="css-1tlx3u4">4 hr</span></div><div class="css-4yrfe3"><svg viewBox="0 0 18 18" aria-hidden="true" class="css-1nidj1s" height="12" role="img" width="12"><path fill="currentColor" d="M9 17v-5a1 1 0 00-1-1H4a1 1 0 00-1 1v5c0 .667-.333 1-1 1s-1-.333-1-1v-5a3 3 0 013-3h4a3 3 0 013 3v5c0 .667-.333 1-1 1-.666 0-1-.333-1-1zm6 0v-3a1 1 0 00-1-1h-1c-.667 0-1-.333-1-1s.333-1 1-1h1a3 3 0 013 3v3c0 .667-.333 1-1 1-.666 0-1-.333-1-1zM6 6a2 2 0 100-4 2 2 0 000 4zm0 2a4 4 0 110-8 4 4 0 010 8zm8 0a1 1 0 100-2 1 1 0 000 2zm0 2a3 3 0 110-6 3 3 0 010 6z" fill-rule="evenodd"></path></svg><span class="css-1tlx3u4">22K</span></div></div><span class="css-klgjx2">Learn how to make predictions from data with Apache Spark, using decision trees, logistic regression, linear regression, ensembles, and pipelines.</span></span><div class="css-ckijet"><a data-trackid="inline-course-card-view-details" class="css-1wlwn5i" href="/courses/machine-learning-with-pyspark"><div class="css-1il7fq1"><span class="css-e1mss3">See Details</span><svg viewBox="0 0 18 18" aria-hidden="false" height="18" role="img" width="18"><title>Right Arrow</title><path fill="currentColor" d="M13.58 10L2 10.002a1 1 0 010-2L13.59 8l-3.285-3.296a.998.998 0 111.417-1.41l4.93 4.948a.998.998 0 01-.012 1.527l-4.933 4.938a1 1 0 01-1.414 0 1.002 1.002 0 010-1.416l3.287-3.29z" fill-rule="evenodd"></path></svg></div></a><a data-trackid="inline-course-card-start-course" href="/users/sign_up?redirect=%2Fcourses%2Fmachine-learning-with-pyspark%2Fcontinue" class="css-1b8uj2l"><span class="css-61bni1">Start Course</span></a></div></article><article data-testid="course-card" data-trackid="cleaning-data-with-pyspark" class="css-13wxmaz"><span class="css-1vb6nck"><div class="css-yh6ywx"><svg aria-hidden="true" height="14" width="14" viewBox="0 0 18 18" class="css-6su6fj"><path fill="currentColor" d="M4 12h10V2H4v10Zm12 2v4H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h12v14Zm-2 0H4v2h10v-2ZM7 4h4a1 1 0 0 1 0 2H7a1 1 0 1 1 0-2Z"></path></svg><p class="css-1vy5y1i">Course</p></div><h3 class="css-1w2p6ll"><a href="/courses/cleaning-data-with-pyspark" class="css-1531qan">Cleaning Data with PySpark</a></h3><div class="css-a2vx78"><div class="css-4yrfe3"><svg viewBox="0 0 18 18" aria-hidden="true" class="css-1nidj1s" height="12" role="img" width="12"><path fill="currentColor" d="M7.98 9.022L8 4.995a1 1 0 012 .01l-.016 3.022H12a1 1 0 010 2H8.98a1 1 0 01-1-1.005zM9 16A7 7 0 109 2a7 7 0 000 14zm0 2A9 9 0 119 0a9 9 0 010 18z" fill-rule="evenodd"></path></svg><span class="css-1tlx3u4">4 hr</span></div><div class="css-4yrfe3"><svg viewBox="0 0 18 18" aria-hidden="true" class="css-1nidj1s" height="12" role="img" width="12"><path fill="currentColor" d="M9 17v-5a1 1 0 00-1-1H4a1 1 0 00-1 1v5c0 .667-.333 1-1 1s-1-.333-1-1v-5a3 3 0 013-3h4a3 3 0 013 3v5c0 .667-.333 1-1 1-.666 0-1-.333-1-1zm6 0v-3a1 1 0 00-1-1h-1c-.667 0-1-.333-1-1s.333-1 1-1h1a3 3 0 013 3v3c0 .667-.333 1-1 1-.666 0-1-.333-1-1zM6 6a2 2 0 100-4 2 2 0 000 4zm0 2a4 4 0 110-8 4 4 0 010 8zm8 0a1 1 0 100-2 1 1 0 000 2zm0 2a3 3 0 110-6 3 3 0 010 6z" fill-rule="evenodd"></path></svg><span class="css-1tlx3u4">24.8K</span></div></div><span class="css-klgjx2">Learn how to clean data with Apache Spark in Python.</span></span><div class="css-ckijet"><a data-trackid="inline-course-card-view-details" class="css-1wlwn5i" href="/courses/cleaning-data-with-pyspark"><div class="css-1il7fq1"><span class="css-e1mss3">See Details</span><svg viewBox="0 0 18 18" aria-hidden="false" height="18" role="img" width="18"><title>Right Arrow</title><path fill="currentColor" d="M13.58 10L2 10.002a1 1 0 010-2L13.59 8l-3.285-3.296a.998.998 0 111.417-1.41l4.93 4.948a.998.998 0 01-.012 1.527l-4.933 4.938a1 1 0 01-1.414 0 1.002 1.002 0 010-1.416l3.287-3.29z" fill-rule="evenodd"></path></svg></div></a><a data-trackid="inline-course-card-start-course" href="/users/sign_up?redirect=%2Fcourses%2Fcleaning-data-with-pyspark%2Fcontinue" class="css-1b8uj2l"><span class="css-61bni1">Start Course</span></a></div></article></div><style data-emotion="css 1o82mok">.css-1o82mok{margin-left:unset;-webkit-text-decoration:none;text-decoration:none;width:100%;}@media screen and (min-width: 768px){.css-1o82mok{margin-left:auto;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;}}</style><a class="css-1o82mok" href="/courses-all"><style data-emotion="css 7qxidy">.css-7qxidy{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:2px solid #EFEFF5;border-radius:4px;color:#65707C;gap:7px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:4px;}@media screen and (min-width: 768px){.css-7qxidy{border:none;}}</style><style data-emotion="css qn0963">.css-qn0963{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:2px solid #EFEFF5;border-radius:4px;color:#65707C;gap:7px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:4px;}@media screen and (min-width: 768px){.css-qn0963{border:none;}}</style><style data-emotion="css 58bv3">.css-58bv3{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:2px solid #EFEFF5;border-radius:4px;color:#65707C;gap:7px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding:4px;}@media screen and (min-width: 768px){.css-58bv3{border:none;}}</style><div class="css-58bv3"><style data-emotion="css 12w9r2c">.css-12w9r2c{font-size:14px;font-weight:800;}.css-12w9r2c:active,.css-12w9r2c:focus,.css-12w9r2c:hover{-webkit-text-decoration:underline;text-decoration:underline;}</style><style data-emotion="css khv0gd">.css-khv0gd{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;font-size:14px;font-weight:800;}.css-khv0gd:active,.css-khv0gd:focus,.css-khv0gd:hover{-webkit-text-decoration:underline;text-decoration:underline;}</style><span class="css-khv0gd">See More</span><svg viewBox="0 0 18 18" aria-hidden="false" height="18" role="img" width="18"><title>Right Arrow</title><path fill="currentColor" d="M13.58 10L2 10.002a1 1 0 010-2L13.59 8l-3.285-3.296a.998.998 0 111.417-1.41l4.93 4.948a.998.998 0 01-.012 1.527l-4.933 4.938a1 1 0 01-1.414 0 1.002 1.002 0 010-1.416l3.287-3.29z" fill-rule="evenodd"></path></svg></div></a></div></div></section><style data-emotion="css b3xwak">.css-b3xwak{max-width:1200px;margin-left:auto;margin-right:auto;}</style><style data-emotion="css 7zqrlm">.css-7zqrlm{box-sizing:border-box;margin:0;min-width:0;max-width:1200px;margin-left:auto;margin-right:auto;}</style><div class="css-7zqrlm"><div class="css-rnnxue" data-testid="further-resource-recommendation-footer"><style data-emotion="css xjwxos">.css-xjwxos{font-size:32px;font-weight:800;letter-spacing:-1px;margin-bottom:15px;}</style><style data-emotion="css bebloz">.css-bebloz{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;font-size:32px;font-weight:800;letter-spacing:-1px;margin-bottom:15px;}</style><span class="css-bebloz">Related</span><style data-emotion="css 1hqeb79">.css-1hqeb79{-webkit-column-gap:24px;column-gap:24px;display:grid;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:repeat(1, 1fr);row-gap:0;}.css-1hqeb79 div:nth-of-type(4){display:none;}@media screen and (min-width: 1200px){.css-1hqeb79 div:nth-of-type(4){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}.css-1hqeb79 div:nth-of-type(5){display:none;}@media screen and (min-width: 768px){.css-1hqeb79 div:nth-of-type(5){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}.css-1hqeb79 div:nth-of-type(6){display:none;}@media screen and (min-width: 1200px){.css-1hqeb79 div:nth-of-type(6){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}@media screen and (min-width: 600px){.css-1hqeb79{row-gap:24px;}}@media screen and (min-width: 768px){.css-1hqeb79{grid-template-columns:repeat(2, 1fr);}}@media screen and (min-width: 1200px){.css-1hqeb79{grid-template-columns:repeat(3, 1fr);}}</style><style data-emotion="css 35esya">.css-35esya{box-sizing:border-box;margin:0;min-width:0;-webkit-column-gap:24px;column-gap:24px;display:grid;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:repeat(1, 1fr);row-gap:0;}.css-35esya div:nth-of-type(4){display:none;}@media screen and (min-width: 1200px){.css-35esya div:nth-of-type(4){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}.css-35esya div:nth-of-type(5){display:none;}@media screen and (min-width: 768px){.css-35esya div:nth-of-type(5){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}.css-35esya div:nth-of-type(6){display:none;}@media screen and (min-width: 1200px){.css-35esya div:nth-of-type(6){display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}@media screen and (min-width: 600px){.css-35esya{row-gap:24px;}}@media screen and (min-width: 768px){.css-35esya{grid-template-columns:repeat(2, 1fr);}}@media screen and (min-width: 1200px){.css-35esya{grid-template-columns:repeat(3, 1fr);}}</style><div class="css-35esya"><style data-emotion="css 5ca0w2">.css-5ca0w2{-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;background-color:inherit;border:none;border-radius:4px;display:grid;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;grid-template-columns:100px 230px;grid-template-rows:repeat(2, 1fr);-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;padding:0;}.css-5ca0w2:hover{border-color:#05192D2E;cursor:pointer;}@media screen and (min-width: 600px){.css-5ca0w2{background-color:#ffffff;border:1px solid #05192D1A;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:100%;grid-template-rows:none;height:430px;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;padding:24px;}}</style><style data-emotion="css 1n9t734">.css-1n9t734{box-sizing:border-box;margin:0;min-width:0;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;background-color:inherit;border:none;border-radius:4px;display:grid;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;grid-template-columns:100px 230px;grid-template-rows:repeat(2, 1fr);-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;padding:0;}.css-1n9t734:hover{border-color:#05192D2E;cursor:pointer;}@media screen and (min-width: 600px){.css-1n9t734{background-color:#ffffff;border:1px solid #05192D1A;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;grid-template-columns:100%;grid-template-rows:none;height:430px;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;padding:24px;}}</style><div data-trackid="media-visit-new-resource-footer" class="css-1n9t734"><style data-emotion="css 8s32ys">.css-8s32ys{border-radius:4px;grid-column-end:2;grid-column-start:1;grid-row-end:3;grid-row-start:1;height:62px;margin-bottom:32px;overflow:hidden;width:100px;}@media screen and (min-width: 600px){.css-8s32ys{border-radius:0;height:180px;margin-bottom:20px;width:325px;}}</style><style data-emotion="css 1l22q7j">.css-1l22q7j{box-sizing:border-box;margin:0;min-width:0;border-radius:4px;grid-column-end:2;grid-column-start:1;grid-row-end:3;grid-row-start:1;height:62px;margin-bottom:32px;overflow:hidden;width:100px;}@media screen and (min-width: 600px){.css-1l22q7j{border-radius:0;height:180px;margin-bottom:20px;width:325px;}}</style><div class="hide-in-percy css-1l22q7j"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#EFEBE4" height="180" width="336"></rect><rect fill="#DD3400" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#FFEC3C" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#DD3400" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#FFEC3C" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><style data-emotion="css ijor27">.css-ijor27{height:46px;}.css-ijor27:active,.css-ijor27:focus,.css-ijor27:hover{-webkit-text-decoration:none;text-decoration:none;}@media screen and (min-width: 600px){.css-ijor27{height:auto;}}</style><style data-emotion="css 1hx470u">.css-1hx470u{box-sizing:border-box;margin:0;min-width:0;color:#05192d;font-weight:700;line-height:1.5;-webkit-text-decoration:none;text-decoration:none;height:46px;}.css-1hx470u:active,.css-1hx470u:focus,.css-1hx470u:hover{-webkit-text-decoration:underline;text-decoration:underline;}.css-1hx470u:active,.css-1hx470u:focus,.css-1hx470u:hover{-webkit-text-decoration:none;text-decoration:none;}@media screen and (min-width: 600px){.css-1hx470u{height:auto;}}</style><a data-trackid="media-recommendation-pyspark-cheat-sheet-spark-dataframes-in-python" href="/cheat-sheet/pyspark-cheat-sheet-spark-dataframes-in-python" class="css-1hx470u"><style data-emotion="css 1x2gbiy">.css-1x2gbiy{margin-bottom:8px;margin-left:16px;margin-top:0;}@media screen and (min-width: 600px){.css-1x2gbiy{margin-left:0;}}</style><style data-emotion="css btqi7q">.css-btqi7q{box-sizing:border-box;margin:0;min-width:0;margin-bottom:8px;margin-left:16px;margin-top:0;}@media screen and (min-width: 600px){.css-btqi7q{margin-left:0;}}</style><div class="css-btqi7q"><style data-emotion="css nfnflu">.css-nfnflu{font-weight:800;letter-spacing:1.5px;line-height:1.25;text-transform:uppercase;color:var(--wf-text--main, #05192D);font-size:12px;}</style><style data-emotion="css wrijbb">.css-wrijbb{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;font-weight:800;letter-spacing:1.5px;line-height:1.25;text-transform:uppercase;color:var(--wf-text--main, #05192D);font-size:12px;}</style><p class="css-wrijbb">cheat-sheet</p></div><style data-emotion="css 2m9w8c">.css-2m9w8c{display:-webkit-box;font-size:14px;font-weight:400;line-height:1.2;max-width:230px;margin-bottom:-10px;margin-left:16px;overflow:hidden;text-overflow:ellipsis;-webkit-box-orient:vertical;-webkit-line-clamp:2;}@media screen and (min-width: 480px){.css-2m9w8c{max-width:310px;}}@media screen and (min-width: 600px){.css-2m9w8c{font-size:20px;font-weight:800;max-width:auto;margin-bottom:12px;margin-left:0;}}</style><style data-emotion="css 1h0qt4y">.css-1h0qt4y{box-sizing:border-box;margin:0;min-width:0;color:#05192D;font-size:16px;line-height:1.5;display:-webkit-box;font-size:14px;font-weight:400;line-height:1.2;max-width:230px;margin-bottom:-10px;margin-left:16px;overflow:hidden;text-overflow:ellipsis;-webkit-box-orient:vertical;-webkit-line-clamp:2;}@media screen and (min-width: 480px){.css-1h0qt4y{max-width:310px;}}@media screen and (min-width: 600px){.css-1h0qt4y{font-size:20px;font-weight:800;max-width:auto;margin-bottom:12px;margin-left:0;}}</style><h3 class="css-1h0qt4y">PySpark Cheat Sheet: Spark DataFrames in Python</h3></a><style data-emotion="css 1csvksd">.css-1csvksd{display:none;line-height:1.5;margin-bottom:16px;min-height:72px;overflow:hidden;text-overflow:ellipsis;-webkit-box-orient:vertical;-webkit-line-clamp:3;}@media screen and (min-width: 600px){.css-1csvksd{display:-webkit-box;}}</style><style data-emotion="css pzdbi3">.css-pzdbi3{box-sizing:border-box;margin:0;min-width:0;display:none;line-height:1.5;margin-bottom:16px;min-height:72px;overflow:hidden;text-overflow:ellipsis;-webkit-box-orient:vertical;-webkit-line-clamp:3;}@media screen and (min-width: 600px){.css-pzdbi3{display:-webkit-box;}}</style><div class="css-pzdbi3">This PySpark SQL cheat sheet is your handy companion to Apache Spark DataFrames in Python and includes code samples.</div><style data-emotion="css jd07pa">.css-jd07pa{-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;-webkit-align-self:grid-end;-ms-flex-item-align:grid-end;align-self:grid-end;display:none;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;grid-column-end:3;grid-column-start:2;margin-left:16px;}@media screen and (min-width: 600px){.css-jd07pa{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;grid-column-end:2;grid-column-start:1;min-height:40px;margin-left:0;margin-top:auto;}}</style><style data-emotion="css 1li6zqs">.css-1li6zqs{box-sizing:border-box;margin:0;min-width:0;-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;-webkit-align-self:grid-end;-ms-flex-item-align:grid-end;align-self:grid-end;display:none;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;grid-column-end:3;grid-column-start:2;margin-left:16px;}@media screen and (min-width: 600px){.css-1li6zqs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;grid-column-end:2;grid-column-start:1;min-height:40px;margin-left:0;margin-top:auto;}}</style><div class="css-1li6zqs"><style data-emotion="css 13da2po">.css-13da2po{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:0;margin-top:-4px;width:100%;}@media screen and (min-width: 600px){.css-13da2po{gap:8px;margin-top:8px;}}</style><style data-emotion="css 2of637">.css-2of637{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:0;margin-top:-4px;width:100%;}@media screen and (min-width: 600px){.css-2of637{gap:8px;margin-top:8px;}}</style><style data-emotion="css gy1ihg">.css-gy1ihg{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:0;margin-top:-4px;width:100%;}@media screen and (min-width: 600px){.css-gy1ihg{gap:8px;margin-top:8px;}}</style><div class="css-gy1ihg"><style data-emotion="css 1b71khg">.css-1b71khg{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;margin-top:4px;}.css-1b71khg:not(:last-child):after{content:'•';}@media screen and (min-width: 600px){.css-1b71khg{margin-top:unset;}}</style><style data-emotion="css r12rjb">.css-r12rjb{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;margin-top:4px;}.css-r12rjb:not(:last-child):after{content:'•';}@media screen and (min-width: 600px){.css-r12rjb{margin-top:unset;}}</style><style data-emotion="css 1p88eqm">.css-1p88eqm{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:8px;margin-top:4px;}.css-1p88eqm:not(:last-child):after{content:'•';}@media screen and (min-width: 600px){.css-1p88eqm{margin-top:unset;}}</style><div class="css-1p88eqm"><style data-emotion="css 1hg22e0">.css-1hg22e0{display:none;}@media screen and (min-width: 600px){.css-1hg22e0{display:block;}}</style><style data-emotion="css m5jzxa">.css-m5jzxa{box-sizing:border-box;margin:0;min-width:0;display:none;}@media screen and (min-width: 600px){.css-m5jzxa{display:block;}}</style><div class="hide-in-percy css-m5jzxa"><style data-emotion="css h5q76b">.css-h5q76b{border-radius:50%;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:24px;margin-bottom:0;margin-top:0;overflow:hidden;width:24px;}</style><style data-emotion="css 2r5hwm">.css-2r5hwm{box-sizing:border-box;margin:0;min-width:0;border-radius:50%;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:24px;margin-bottom:0;margin-top:0;overflow:hidden;width:24px;}</style><div class="hide-in-percy css-2r5hwm"><a class="css-xlmvza" data-trackid="media-visit-author-profile" href="/portfolio/karlijn" target="_self"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:24px;height:24px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="Karlijn Willems&#x27;s photo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="css-xah9so" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Karlijn Willems&#x27;s photo" loading="lazy" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-xah9so" srcSet="https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png 1x, https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png 2x" src="https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png"/></noscript></span></a></div></div><style data-emotion="css 1f2ft3b">.css-1f2ft3b{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:8px;}</style><style data-emotion="css 1qfpo4x">.css-1qfpo4x{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:8px;}</style><style data-emotion="css 1ovs79h">.css-1ovs79h{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;gap:8px;}</style><div class="css-1ovs79h"><style data-emotion="css dsv21c">.css-dsv21c{color:#65707C;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><style data-emotion="css 198tbf7">.css-198tbf7{box-sizing:border-box;margin:0;min-width:0;font-size:0.875rem;line-height:1.5;margin-top:0;color:#65707C;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}</style><p class="css-198tbf7">Karlijn Willems<!-- --> </p><style data-emotion="css icz25b">.css-icz25b{color:#65707C;display:none;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}.css-icz25b:before{content:'•';margin-right:8px;}@media screen and (min-width: 600px){.css-icz25b{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 1ba3pj6">.css-1ba3pj6{box-sizing:border-box;margin:0;min-width:0;font-size:0.875rem;line-height:1.5;margin-top:0;color:#65707C;display:none;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}.css-1ba3pj6:before{content:'•';margin-right:8px;}@media screen and (min-width: 600px){.css-1ba3pj6{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><p class="css-1ba3pj6">5 min</p></div></div></div></div></div><div data-trackid="media-visit-new-resource-footer" class="css-1n9t734"><div class="hide-in-percy css-1l22q7j"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#EFEBE4" height="180" width="336"></rect><rect fill="#FF931E" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#F7F3EB" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#000820" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#D87300" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-pyspark-cheat-sheet-spark-in-python" href="/cheat-sheet/pyspark-cheat-sheet-spark-in-python" class="css-1hx470u"><div class="css-btqi7q"><p class="css-wrijbb">cheat-sheet</p></div><h3 class="css-1h0qt4y">PySpark Cheat Sheet: Spark in Python </h3></a><div class="css-pzdbi3">This PySpark cheat sheet with code samples covers the basics like initializing Spark in Python, loading data, sorting, and repartitioning.</div><div class="css-1li6zqs"><div class="css-gy1ihg"><div class="css-1p88eqm"><div class="hide-in-percy css-m5jzxa"><div class="hide-in-percy css-2r5hwm"><a class="css-xlmvza" data-trackid="media-visit-author-profile" href="/portfolio/karlijn" target="_self"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:24px;height:24px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="Karlijn Willems&#x27;s photo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="css-xah9so" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Karlijn Willems&#x27;s photo" loading="lazy" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-xah9so" srcSet="https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png 1x, https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png 2x" src="https://images.datacamp.com/image/upload/v1658157569/karlijn_5fd8178e25.png"/></noscript></span></a></div></div><div class="css-1ovs79h"><p class="css-198tbf7">Karlijn Willems<!-- --> </p><p class="css-1ba3pj6">6 min</p></div></div></div></div></div><div data-trackid="media-visit-new-resource-footer" class="css-1n9t734"><div class="hide-in-percy css-1l22q7j"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#05192D" height="180" width="336"></rect><rect fill="#B08EFF" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#7933FF" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#72E5FE" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#5EB1FF" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#B08EFF" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#72E5FE" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-pyspark-tutorial-getting-started-with-pyspark" href="/tutorial/pyspark-tutorial-getting-started-with-pyspark" class="css-1hx470u"><div class="css-btqi7q"><p class="css-wrijbb">tutorial</p></div><h3 class="css-1h0qt4y">Pyspark Tutorial: Getting Started with Pyspark </h3></a><div class="css-pzdbi3">Discover what Pyspark is and how it can be used while giving examples.</div><div class="css-1li6zqs"><div class="css-gy1ihg"><div class="css-1p88eqm"><div class="hide-in-percy css-m5jzxa"><div class="hide-in-percy css-2r5hwm"><a class="css-xlmvza" data-trackid="media-visit-author-profile" href="/portfolio/natassha6789" target="_self"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:24px;height:24px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="Natassha Selvaraj&#x27;s photo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="css-xah9so" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Natassha Selvaraj&#x27;s photo" loading="lazy" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-xah9so" srcSet="https://images.datacamp.com/image/upload/v1665761694/nattasha_167ec05e37.png 1x, https://images.datacamp.com/image/upload/v1665761694/nattasha_167ec05e37.png 2x" src="https://images.datacamp.com/image/upload/v1665761694/nattasha_167ec05e37.png"/></noscript></span></a></div></div><div class="css-1ovs79h"><p class="css-198tbf7">Natassha Selvaraj<!-- --> </p><p class="css-1ba3pj6">10 min</p></div></div></div></div></div><div data-trackid="media-visit-new-resource-footer" class="css-1n9t734"><div class="hide-in-percy css-1l22q7j"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#F7F3EB" height="180" width="336"></rect><rect fill="#000820" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#00C74E" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#FF931E" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#D87300" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#00C74E" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-installation-of-pyspark" href="/tutorial/installation-of-pyspark" class="css-1hx470u"><div class="css-btqi7q"><p class="css-wrijbb">tutorial</p></div><h3 class="css-1h0qt4y">Installation of PySpark (All operating systems)</h3></a><div class="css-pzdbi3">This tutorial will demonstrate the installation of PySpark and hot to manage the environment variables in Windows, Linux, and Mac Operating System.</div><div class="css-1li6zqs"><div class="css-gy1ihg"><div class="css-1p88eqm"><div class="hide-in-percy css-m5jzxa"><style data-emotion="css 16d77sy">.css-16d77sy{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;overflow:hidden;background:var(--wf-green--main, #03EF62);border-radius:24px;color:var(--wf-green--text-on-color, #05192D);font-size:12px;height:24px;line-height:24px;width:24px;}</style><div class="css-16d77sy"><style data-emotion="css 3xjkdl">.css-3xjkdl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-variant-numeric:tabular-nums;height:100%;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:100%;}.css-3xjkdl>*{margin:auto 0;}.css-3xjkdl>:not(svg){min-width:100%;}.css-3xjkdl>*{min-height:14px;}.css-3xjkdl>svg{width:14px;}</style><div class="css-3xjkdl"><svg aria-hidden="true" height="16" width="16" viewBox="0 0 18 18" class="css-6su6fj"><path fill="currentColor" d="M8.122 4.454v4.36l-4.627 2.64V1.799l4.627 2.655Zm1.497 9.161v-3.93l6.195-3.535-1.507-.863L9.62 7.96V4.018a.754.754 0 0 0-.376-.65L3.599.133a1.08 1.08 0 0 0-1.6.95L2 12.168a1.082 1.082 0 0 0 1.555.976l.042-.022 4.522-2.581v3.516c0 .27.144.517.377.651L14.298 18l1.507-.864-6.186-3.52Z"></path></svg></div></div></div><div class="css-1ovs79h"><p class="css-198tbf7">Olivia Smith<!-- --> </p><p class="css-1ba3pj6">8 min</p></div></div></div></div></div><div data-trackid="media-visit-new-resource-footer" class="css-1n9t734"><div class="hide-in-percy css-1l22q7j"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#72E5FE" height="180" width="336"></rect><rect fill="#7933FF" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#5EB1FF" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#7933FF" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#5EB1FF" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-snowpark-snoflake-introduction" href="/tutorial/snowpark-snoflake-introduction" class="css-1hx470u"><div class="css-btqi7q"><p class="css-wrijbb">tutorial</p></div><h3 class="css-1h0qt4y">Snowflake Snowpark: A Comprehensive Introduction</h3></a><div class="css-pzdbi3">Take the first steps to master in-database machine learning using Snowflake Snowpark.</div><div class="css-1li6zqs"><div class="css-gy1ihg"><div class="css-1p88eqm"><div class="hide-in-percy css-m5jzxa"><div class="hide-in-percy css-2r5hwm"><a class="css-xlmvza" data-trackid="media-visit-author-profile" href="/portfolio/bexgboost" target="_self"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:24px;height:24px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="Bex Tuychiev&#x27;s photo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="css-xah9so" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Bex Tuychiev&#x27;s photo" loading="lazy" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-xah9so" srcSet="https://images.datacamp.com/image/upload/v1686304521/Bex_Tuychiev_1fa6cc0c26.jpg 1x, https://images.datacamp.com/image/upload/v1686304521/Bex_Tuychiev_1fa6cc0c26.jpg 2x" src="https://images.datacamp.com/image/upload/v1686304521/Bex_Tuychiev_1fa6cc0c26.jpg"/></noscript></span></a></div></div><div class="css-1ovs79h"><p class="css-198tbf7">Bex Tuychiev<!-- --> </p><p class="css-1ba3pj6">19 min</p></div></div></div></div></div><div data-trackid="media-visit-new-resource-footer" class="css-1n9t734"><div class="hide-in-percy css-1l22q7j"><svg class="css-1qucg2m" fill="none" viewBox="0 0 336 180" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_4804_129801)"><rect fill="#EFEBE4" height="180" width="336"></rect><rect fill="#72E5FE" height="106.6" transform="rotate(-30 -208.588 220.18)" width="456.125" x="-208.588" y="220.18"></rect><rect fill="#000820" height="35.875" transform="rotate(-30 -208.588 220.18)" width="347.576" x="-208.588" y="220.18"></rect><rect fill="#5EB1FF" height="35.875" transform="rotate(-30 -226.525 189.111)" width="347.576" x="-226.525" y="189.111"></rect><rect fill="#000820" height="106.6" transform="rotate(-30 85.5879 256.055)" width="456.125" x="85.5879" y="256.055"></rect><rect fill="#D87300" height="35.875" transform="rotate(-30 85.5879 256.055)" width="184.5" x="85.5879" y="256.055"></rect><rect fill="#5646A5" height="35.875" transform="rotate(-30 227.432 132.736)" width="271.625" x="227.432" y="132.736"></rect></g><defs><clipPath id="clip0_4804_129801"><rect fill="white" height="180" width="336"></rect></clipPath></defs></svg></div><a data-trackid="media-recommendation-machine-learning-python" href="/tutorial/machine-learning-python" class="css-1hx470u"><div class="css-btqi7q"><p class="css-wrijbb">tutorial</p></div><h3 class="css-1h0qt4y">Python Machine Learning: Scikit-Learn Tutorial</h3></a><div class="css-pzdbi3">An easy-to-follow scikit-learn tutorial that will help you get started with Python machine learning. </div><div class="css-1li6zqs"><div class="css-gy1ihg"><div class="css-1p88eqm"><div class="hide-in-percy css-m5jzxa"><div class="hide-in-percy css-2r5hwm"><a class="css-xlmvza" data-trackid="media-visit-author-profile" href="/portfolio/kurtispykes" target="_self"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:24px;height:24px;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><img alt="Kurtis Pykes &#x27;s photo" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="fixed" class="css-xah9so" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Kurtis Pykes &#x27;s photo" loading="lazy" decoding="async" data-nimg="fixed" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-xah9so" srcSet="https://images.datacamp.com/image/upload/v1658156357/Kurtis_e60df9583d.jpg 1x, https://images.datacamp.com/image/upload/v1658156357/Kurtis_e60df9583d.jpg 2x" src="https://images.datacamp.com/image/upload/v1658156357/Kurtis_e60df9583d.jpg"/></noscript></span></a></div></div><div class="css-1ovs79h"><p class="css-198tbf7">Kurtis Pykes <!-- --> </p><p class="css-1ba3pj6">12 min</p></div></div></div></div></div></div><style data-emotion="css bh4hm1">.css-bh4hm1{-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;border-color:05192D1A;color:black;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin-bottom:40px;margin-top:20px;padding:8px;width:100%;}@media screen and (min-width: 600px){.css-bh4hm1{width:auto;}}@media screen and (min-width: 768px){.css-bh4hm1{display:none;}}</style><style data-emotion="css 1qgbyiy">.css-1qgbyiy{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--main, #05192D);font-size:16px;height:48px;min-width:48px;width:auto;padding-left:24px;padding-right:24px;border-color:var(--wf-border-color--interactive, rgba(48, 57, 105, 0.6));-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;border-color:05192D1A;color:black;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin-bottom:40px;margin-top:20px;padding:8px;width:100%;}.css-1qgbyiy::before{border-radius:2px;content:"";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-1qgbyiy::before{border-radius:2px;margin:0;}.css-1qgbyiy:active{background-color:transparent;}.css-1qgbyiy:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-1qgbyiy:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-1qgbyiy >*{z-index:1;}@media screen and (min-width: 600px){.css-1qgbyiy{width:auto;}}@media screen and (min-width: 768px){.css-1qgbyiy{display:none;}}</style><a data-trackid="media-category-python" href="/tutorial/category/python" class="css-1qgbyiy"><span class="css-61bni1">See More<svg aria-hidden="true" height="18" width="18" viewBox="0 0 18 18" class="css-6su6fj"><path fill="currentColor" d="M13.58 10 2 10.002a1 1 0 0 1 0-2L13.59 8l-3.285-3.296a.998.998 0 1 1 1.417-1.41l4.93 4.948a.998.998 0 0 1-.012 1.527l-4.933 4.938a1 1 0 0 1-1.414 0 1.002 1.002 0 0 1 0-1.416l3.287-3.29Z"></path></svg></span></a><style data-emotion="css i1eyol">.css-i1eyol{-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;border-color:05192D1A;color:black;display:none;margin-bottom:40px;margin-top:20px;padding:0;width:100%;}@media screen and (min-width: 600px){.css-i1eyol{width:auto;}}@media screen and (min-width: 768px){.css-i1eyol{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css fs03fc">.css-fs03fc{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--link, #0065D1);font-size:16px;height:48px;min-width:48px;width:auto;padding-left:24px;padding-right:24px;-webkit-align-self:flex-end;-ms-flex-item-align:flex-end;align-self:flex-end;border-color:05192D1A;color:black;display:none;margin-bottom:40px;margin-top:20px;padding:0;width:100%;}.css-fs03fc::before{border-radius:2px;content:"";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-fs03fc:active{background-color:transparent;}.css-fs03fc:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-fs03fc:hover{border-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-fs03fc:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-fs03fc >*{z-index:1;}@media screen and (min-width: 600px){.css-fs03fc{width:auto;}}@media screen and (min-width: 768px){.css-fs03fc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><a data-trackid="media-category-python" href="/tutorial/category/python" class="css-fs03fc"><span class="css-61bni1">See More<svg aria-hidden="true" height="18" width="18" viewBox="0 0 18 18" class="css-6su6fj"><path fill="currentColor" d="M13.58 10 2 10.002a1 1 0 0 1 0-2L13.59 8l-3.285-3.296a.998.998 0 1 1 1.417-1.41l4.93 4.948a.998.998 0 0 1-.012 1.527l-4.933 4.938a1 1 0 0 1-1.414 0 1.002 1.002 0 0 1 0-1.416l3.287-3.29Z"></path></svg></span></a></div></div></div><div class="css-1yz7e9k"><style data-emotion="css wzppyk">.css-wzppyk{background-color:#000820;}</style><style data-emotion="css 101nefz">.css-101nefz{box-sizing:border-box;margin:0;min-width:0;background-color:#000820;}</style><footer class="css-101nefz"><style data-emotion="css 1bnylqr">.css-1bnylqr{background-color:#213147;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:0;padding-left:16px;padding-right:16px;padding-top:64px;padding-bottom:64px;}@media screen and (min-width: 1200px){.css-1bnylqr{margin-bottom:40px;}}</style><style data-emotion="css 1tg237r">.css-1tg237r{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background-color:#213147;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:0;padding-left:16px;padding-right:16px;padding-top:64px;padding-bottom:64px;}@media screen and (min-width: 1200px){.css-1tg237r{margin-bottom:40px;}}</style><style data-emotion="css y62ecw">.css-y62ecw{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;background-color:#213147;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-bottom:0;padding-left:16px;padding-right:16px;padding-top:64px;padding-bottom:64px;}@media screen and (min-width: 1200px){.css-y62ecw{margin-bottom:40px;}}</style><div class="css-y62ecw"><style data-emotion="css 6192d3">.css-6192d3{color:#ffffff;text-align:center;}</style><style data-emotion="css 3lw860">.css-3lw860{box-sizing:border-box;margin:0;min-width:0;font-size:1.75rem;font-weight:700;letter-spacing:-0.5px;line-height:1.2;margin-top:0;color:#ffffff;text-align:center;}@media screen and (min-width: 992px){.css-3lw860{font-size:2rem;}}</style><h2 class="css-3lw860">Grow your data skills with DataCamp for Mobile</h2><style data-emotion="css 14psqi5">.css-14psqi5{color:#ffffff;margin-bottom:16px;margin-top:8px;text-align:center;}</style><style data-emotion="css 1hp1864">.css-1hp1864{box-sizing:border-box;margin:0;min-width:0;font-size:1rem;line-height:1.5;margin-top:0;color:#ffffff;margin-bottom:16px;margin-top:8px;text-align:center;}</style><p class="css-1hp1864">Make progress on the go with our mobile courses and daily 5-minute coding challenges.</p></div><style data-emotion="css cmnpne">.css-cmnpne{padding-bottom:16px;}@media screen and (min-width: 992px){.css-cmnpne{padding-top:28px;}}@media screen and (min-width: 1200px){.css-cmnpne{padding-bottom:48px;}}</style><style data-emotion="css 137p4nw">.css-137p4nw{box-sizing:border-box;margin:0;min-width:0;width:100%;max-width:container;margin-left:auto;margin-right:auto;max-width:1172px;padding-left:16px;padding-right:16px;padding-bottom:16px;}@media screen and (min-width: 992px){.css-137p4nw{padding-top:28px;}}@media screen and (min-width: 1200px){.css-137p4nw{padding-bottom:48px;}}</style><div class="css-137p4nw"><style data-emotion="css h15bf0">@media screen and (min-width: 1200px){.css-h15bf0{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css 17nkhmo">.css-17nkhmo{box-sizing:border-box;margin:0;min-width:0;}@media screen and (min-width: 1200px){.css-17nkhmo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><div class="css-17nkhmo"><style data-emotion="css k2tqm9">@media screen and (min-width: 480px){.css-k2tqm9{-webkit-column-gap:32px;column-gap:32px;display:grid;grid-template-columns:repeat(2, 1fr);}}@media screen and (min-width: 768px){.css-k2tqm9{grid-template-columns:repeat(3, 1fr);}}@media screen and (min-width: 992px){.css-k2tqm9{grid-template-columns:repeat(4, 1fr);}}@media screen and (min-width: 1200px){.css-k2tqm9{grid-template-columns:repeat(6, 1fr);}}</style><style data-emotion="css masp8k">.css-masp8k{box-sizing:border-box;margin:0;min-width:0;}@media screen and (min-width: 480px){.css-masp8k{-webkit-column-gap:32px;column-gap:32px;display:grid;grid-template-columns:repeat(2, 1fr);}}@media screen and (min-width: 768px){.css-masp8k{grid-template-columns:repeat(3, 1fr);}}@media screen and (min-width: 992px){.css-masp8k{grid-template-columns:repeat(4, 1fr);}}@media screen and (min-width: 1200px){.css-masp8k{grid-template-columns:repeat(6, 1fr);}}</style><div class="css-masp8k"><div class="css-1yz7e9k"><div class="css-1yz7e9k"><style data-emotion="css p5z12p">.css-p5z12p{color:#ffffff;display:block;padding-bottom:12px;padding-top:32px;}</style><style data-emotion="css jfffg7">.css-jfffg7{box-sizing:border-box;margin:0;min-width:0;font-size:1rem;font-weight:700;letter-spacing:2px;line-height:1.2;margin-top:0;text-transform:uppercase;color:#ffffff;display:block;padding-bottom:12px;padding-top:32px;}</style><strong class="css-jfffg7">Learn</strong><div class="css-1yz7e9k"><style data-emotion="css 1qttubv">.css-1qttubv{display:block;line-height:1.2;padding-top:11px;padding-bottom:11px;}</style><style data-emotion="css rar64w">.css-rar64w{box-sizing:border-box;margin:0;min-width:0;border:0;color:#ffffff;font-weight:400;opacity:0.75;-webkit-text-decoration:none;text-decoration:none;display:block;line-height:1.2;padding-top:11px;padding-bottom:11px;}.css-rar64w:active,.css-rar64w:focus,.css-rar64w:hover{border:0;color:#ffffff;opacity:1;}</style><a class="css-rar64w" href="/blog/how-to-learn-python-expert-guide">Learn Python</a><a class="css-rar64w" href="/learn/r">Learn R</a><a class="css-rar64w" href="/blog/how-to-learn-ai">Learn AI</a><a class="css-rar64w" href="/learn/sql">Learn SQL</a><a class="css-rar64w" href="/learn/power-bi">Learn Power BI</a><a class="css-rar64w" href="/learn/tableau">Learn Tableau</a><a class="css-rar64w" href="/category/data-engineering">Learn Data Engineering</a><a class="css-rar64w" href="/signal">Assessments</a><a class="css-rar64w" href="/tracks/career">Career Tracks</a><a class="css-rar64w" href="/tracks/skill">Skill Tracks</a><a class="css-rar64w" href="/courses-all">Courses</a><a class="css-rar64w" href="/blog/data-science-roadmap">Data Science Roadmap</a></div></div></div><div class="css-1yz7e9k"><div class="css-1yz7e9k"><strong class="css-jfffg7">Data Courses</strong><div class="css-1yz7e9k"><a class="css-rar64w" href="/category/python">Python Courses</a><a class="css-rar64w" href="/category/r">R Courses</a><a class="css-rar64w" href="/category/sql">SQL Courses</a><a class="css-rar64w" href="/category/power-bi">Power BI Courses</a><a class="css-rar64w" href="/category/tableau">Tableau Courses</a><a class="css-rar64w" href="/category/alteryx">Alteryx Courses</a><a class="css-rar64w" href="/category/azure">Azure Courses</a><a class="css-rar64w" href="/category/google-sheets">Google Sheets Courses</a><a class="css-rar64w" href="/category/artificial-intelligence">AI Courses</a><a class="css-rar64w" href="/category/data-analysis">Data Analysis Courses</a><a class="css-rar64w" href="/category/data-visualization">Data Visualization Courses</a><a class="css-rar64w" href="/category/machine-learning">Machine Learning Courses</a><a class="css-rar64w" href="/category/data-engineering">Data Engineering Courses</a><a class="css-rar64w" href="/category/probability-and-statistics">Probability &amp; Statistics Courses</a></div></div></div><div class="css-1yz7e9k"><div class="css-1yz7e9k"><strong class="css-jfffg7">DataLab</strong><div class="css-1yz7e9k"><a target="_blank" class="css-rar64w" href="/datalab">Get Started</a><a target="_blank" class="css-rar64w" href="/datalab/pricing">Pricing</a><a target="_blank" class="css-rar64w" href="/datalab/security">Security</a><a target="_blank" class="css-rar64w" href="https://datalab-docs.datacamp.com">Documentation</a></div></div><div class="css-1yz7e9k"><strong class="css-jfffg7">Certification</strong><div class="css-1yz7e9k"><a class="css-rar64w" href="/certification">Certifications</a><a class="css-rar64w" href="/certification/data-scientist">Data Scientist</a><a class="css-rar64w" href="/certification/data-analyst">Data Analyst</a><a class="css-rar64w" href="/certification/data-engineer">Data Engineer</a><a class="css-rar64w" href="/certification/sql-associate">SQL Associate</a><a class="css-rar64w" href="/certification/data-analyst-in-power-bi">Power BI Data Analyst</a><a class="css-rar64w" href="/certification/data-analyst-in-tableau">Tableau Certified Data Analyst</a><a class="css-rar64w" href="/certification/azure-fundamentals">Azure Fundamentals</a><a class="css-rar64w" href="/certification/ai-fundamentals">AI Fundamentals</a></div></div></div><div class="css-1yz7e9k"><div class="css-1yz7e9k"><strong class="css-jfffg7">Resources</strong><div class="css-1yz7e9k"><a class="css-rar64w" href="/resources">Resource Center</a><a class="css-rar64w" href="/webinars">Upcoming Events</a><a class="css-rar64w" href="/blog">Blog</a><a class="css-rar64w" href="/code-along">Code-Alongs</a><a class="css-rar64w" href="/tutorial">Tutorials</a><a class="css-rar64w" href="/open-source">Open Source</a><a class="css-rar64w" href="https://www.rdocumentation.org">RDocumentation</a><a class="css-rar64w" href="/learn/create">Course Editor</a><a class="css-rar64w" href="/business/demo">Book a Demo with DataCamp for Business</a><a class="css-rar64w" href="/data-portfolio">Data Portfolio</a><a class="css-rar64w" href="/data-portfolio/leaderboard">Portfolio Leaderboard</a></div></div></div><div class="css-1yz7e9k"><div class="css-1yz7e9k"><strong class="css-jfffg7">Plans</strong><div class="css-1yz7e9k"><a class="css-rar64w" href="/pricing">Pricing</a><a class="css-rar64w" href="/business">For Business</a><a class="css-rar64w" href="/universities">For Universities</a><a class="css-rar64w" href="/promo">Discounts, Promos &amp; Sales</a><style data-emotion="css 1fqnvuc">.css-1fqnvuc{display:block;padding-top:9px;padding-bottom:9px;}</style><style data-emotion="css ynalry">.css-ynalry{box-sizing:border-box;margin:0;min-width:0;border:0;color:#ffffff;font-weight:400;opacity:0.75;-webkit-text-decoration:none;text-decoration:none;display:block;padding-top:9px;padding-bottom:9px;}.css-ynalry:active,.css-ynalry:focus,.css-ynalry:hover{border:0;color:#ffffff;opacity:1;}</style><a rel="noopener noreferrer" target="_blank" class="css-ynalry" href="/donates">DataCamp Donates</a></div></div><div class="css-1yz7e9k"><strong class="css-jfffg7">For Business</strong><div class="css-1yz7e9k"><a class="css-rar64w" href="/business/compare-plans">Business Pricing</a><a class="css-rar64w" href="/business/learn-teams">Teams Plan</a><a class="css-rar64w" href="/business/data-unlimited">Data &amp; AI Unlimited Plan</a><a class="css-rar64w" href="/business/customer-stories">Customer Stories</a><a class="css-rar64w" href="/business/partner-program">Partner Program</a></div></div></div><div class="css-1yz7e9k"><div class="css-1yz7e9k"><strong class="css-jfffg7">About</strong><div class="css-1yz7e9k"><a class="css-rar64w" href="/about">About Us</a><a class="css-rar64w" href="/stories">Learner Stories</a><a class="css-rar64w" href="/careers">Careers</a><a class="css-rar64w" href="/learn/create">Become an Instructor</a><a class="css-rar64w" href="/press">Press</a><a class="css-rar64w" href="/about/leadership">Leadership</a><a class="css-rar64w" href="https://support.datacamp.com/hc/en-us/articles/360021185634">Contact Us</a><a href="/es" class="css-rar64w">DataCamp Español</a><a href="/pt" class="css-rar64w">DataCamp Português</a></div></div><div class="css-1yz7e9k"><strong class="css-jfffg7">Support</strong><div class="css-1yz7e9k"><a class="css-rar64w" href="https://support.datacamp.com/hc/en-us">Help Center</a><a class="css-rar64w" href="/affiliates">Become an Affiliate</a></div></div></div></div></div><style data-emotion="css ybpf8h">.css-ybpf8h{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;margin-top:40px;}@media screen and (min-width: 600px){.css-ybpf8h{margin-left:48px;}}@media screen and (min-width: 1200px){.css-ybpf8h{margin-left:0;}}</style><svg height="85" width="139" xmlns="http://www.w3.org/2000/svg" class="css-ybpf8h"><g fill="none" fill-rule="evenodd"><path d="M100.251 44.684v5.109h5.004v2.713h-5.004v10.5c0 1.62 1.057 2.184 2.431 2.184 1.163 0 2.326-.599 2.608-.81v2.96c-.458.175-1.621.845-2.925.845-2.924 0-5.25-1.903-5.25-5.074V52.506h-3.1v-2.713h3.1v-5.109zm-27.057-2.079v25.368h-3.136v-2.96c-1.409 1.939-3.558 3.172-6.201 3.172-4.263 0-7.857-4.087-7.857-9.302 0-5.214 3.594-9.301 7.857-9.301 2.643 0 4.792 1.268 6.201 3.241V42.605zm40.904 6.977c2.854 0 5.003 1.268 6.342 3.311v-3.1h3.136v18.18h-3.136v-3.1c-1.339 2.043-3.488 3.312-6.342 3.312h-.036c-4.262 0-7.68-4.017-7.68-9.302 0-5.214 3.418-9.301 7.68-9.301zm-31.402 0c2.854 0 5.003 1.268 6.343 3.311v-3.1h3.135v18.18H89.04v-3.1c-1.34 2.043-3.489 3.312-6.343 3.312h-.035c-4.263 0-7.68-4.017-7.68-9.302 0-5.214 3.417-9.301 7.68-9.301h.035zm-17.923 3.1c-3.312 0-5.637 2.713-5.637 6.201s2.325 6.237 5.637 6.237 5.285-2.749 5.285-6.237-1.973-6.201-5.285-6.201zm18.98-.035h-.035c-3.488 0-5.602 2.642-5.602 6.236 0 3.664 2.15 6.237 5.602 6.237h.035c3.136 0 5.286-2.749 5.286-6.237s-2.15-6.236-5.286-6.236zm31.402 0h-.035c-3.488 0-5.602 2.642-5.602 6.236 0 3.664 2.15 6.237 5.602 6.237h.035c3.136 0 5.285-2.749 5.285-6.237s-2.149-6.236-5.285-6.236zm-1.057-24.405c2.854 0 5.003 1.27 6.342 3.312v-3.1h3.136v18.18h-3.136v-3.1c-1.339 2.043-3.488 3.311-6.342 3.311h-.036c-4.263 0-7.68-4.016-7.68-9.3 0-5.215 3.417-9.303 7.68-9.303zm17.526 0c3.805.036 6.13 3.1 6.13 5.744h-3.03c0-.74-.317-1.516-.951-2.08-.529-.493-1.269-.81-2.185-.81-1.832 0-3.1.951-3.1 2.255 0 1.269.775 1.903 2.36 2.255l2.114.458c3.524.775 5.215 2.713 5.215 5.285 0 3.066-2.466 5.497-6.553 5.497-4.405 0-6.66-3.383-6.66-6.13h2.96c0 .88.423 1.76 1.163 2.36.634.563 1.515.916 2.537.916 2.184 0 3.453-1.163 3.453-2.608 0-1.48-1.022-2.113-3.313-2.642l-1.972-.458c-2.255-.528-4.405-1.832-4.405-4.897 0-2.784 2.502-5.18 6.237-5.144zm-16.469 3.065h-.035c-3.488 0-5.602 2.643-5.602 6.237 0 3.664 2.15 6.236 5.602 6.236h.035c3.136 0 5.285-2.748 5.285-6.236s-2.149-6.237-5.285-6.237zM72.627 6.976c5.108 0 7.61 4.087 7.857 6.236h-3.312c-.141-.493-.599-1.48-1.62-2.255-.67-.528-1.657-.916-2.925-.916-3.383 0-5.708 2.819-5.708 6.237 0 3.417 2.36 6.236 5.708 6.236a5.05 5.05 0 003.136-1.057c.88-.705 1.303-1.621 1.41-2.08h3.24c-.282 1.974-2.924 6.202-7.786 6.202-5.003 0-8.844-4.158-8.844-9.301 0-5.144 3.84-9.302 8.844-9.302zm23.505 0c4.933 0 8.491 4.122 8.491 9.02 0 .458-.035.916-.07 1.303H90.706c.317 3.065 2.255 5.285 5.567 5.285 2.96 0 4.475-2.008 4.756-2.96h3.277c-.211 1.516-2.501 5.955-8.033 5.955-5.25 0-8.773-4.052-8.773-9.301 0-5.18 3.63-9.302 8.632-9.302zm17.966 0c2.854 0 5.003 1.268 6.342 3.311v-3.1h3.136v18.18h-3.136v-3.1c-1.339 2.043-3.488 3.312-6.342 3.312h-.036c-4.262 0-7.68-4.017-7.68-9.302 0-5.214 3.418-9.301 7.68-9.301zM85.617 0v25.367H82.48V0h3.136zm47.653 6.976c.775 0 1.41.07 1.762.106v3.065c-.247-.07-1.304-.106-1.692-.106-2.537 0-4.72 1.586-4.72 5.144v10.182h-3.137V7.187h3.136v2.572c1.057-1.656 2.607-2.783 4.65-2.783zm-18.115 3.065h-.035c-3.488 0-5.602 2.643-5.602 6.236 0 3.665 2.15 6.237 5.602 6.237h.035c3.136 0 5.285-2.748 5.285-6.237 0-3.488-2.149-6.236-5.285-6.236zm-19.023-.07c-2.889 0-4.827 2.008-5.355 4.756h10.675c-.458-2.678-2.36-4.756-5.32-4.756z" fill="#fff"></path><path d="M29.685 67.065V51.748L53.84 37.971l-5.87-3.366-18.284 10.427v-15.36a2.925 2.925 0 00-1.465-2.532L6.23 14.52a4.186 4.186 0 00-4.253.126A4.198 4.198 0 000 18.223v43.2c0 1.464.74 2.802 1.98 3.578a4.184 4.184 0 004.245.128l17.633-10.057v13.69c0 1.048.564 2.02 1.474 2.537l22.596 12.862 5.875-3.368zm-5.827-35.703v16.994L5.828 58.64V21.015z" fill="#03ef62"></path></g></svg><style data-emotion="css 1kwx0ad">.css-1kwx0ad{grid-column-gap:0;grid-row-gap:0;grid-template-columns:75% 25%;grid-template-rows:auto auto;}@media screen and (min-width: 1200px){.css-1kwx0ad{display:grid;padding-top:0;}}</style><style data-emotion="css 88jn27">.css-88jn27{box-sizing:border-box;margin:0;min-width:0;grid-column-gap:0;grid-row-gap:0;grid-template-columns:75% 25%;grid-template-rows:auto auto;}@media screen and (min-width: 1200px){.css-88jn27{display:grid;padding-top:0;}}</style><div class="css-88jn27"><style data-emotion="css fkog9e">.css-fkog9e{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;margin-left:-16px;martin-top:48px;}.css-fkog9e a{border-bottom:0;}@media screen and (min-width: 1200px){.css-fkog9e{-webkit-box-pack:end;-ms-flex-pack:end;-webkit-justify-content:flex-end;justify-content:flex-end;margin-top:0;}}.css-fkog9e a{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-bottom:0;color:#ffffff;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:44px;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:44px;}.css-fkog9e a:active svg,.css-fkog9e a:focus svg,.css-fkog9e a:hover svg{fill:#03ef62;}</style><div class="css-fkog9e"><a rel="noopener" target="_blank" href="https://www.facebook.com/datacampinc/" class="css-xlmvza"><i aria-hidden="true"><svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" width="10" height="16"><path d="M6.164 15.875V8.691h2.514l.376-2.8h-2.89V4.102c0-.81.233-1.362 1.448-1.362l1.545-.001V.234A22.319 22.319 0 006.903.125c-2.23 0-3.756 1.305-3.756 3.701v2.065H.625v2.8h2.522v7.184h3.016z"></path></svg></i><style data-emotion="css kek3bb">.css-kek3bb{clip:rect(0 0 0 0);-webkit-clip-path:inset(50%);clip-path:inset(50%);height:1px;overflow:hidden;position:absolute;white-space:nowrap;width:1px;}</style><span class="css-kek3bb">Facebook</span></a><a rel="noopener" target="_blank" href="https://twitter.com/datacamp" class="css-xlmvza"><i aria-hidden="true"><svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" width="17" height="17"><path d="M15.75 3.27a6.445 6.445 0 01-1.855.512 3.254 3.254 0 001.42-1.794 6.519 6.519 0 01-2.053.788 3.214 3.214 0 00-2.359-1.026 3.24 3.24 0 00-3.23 3.245c0 .254.029.503.084.74a9.163 9.163 0 01-6.659-3.392 3.24 3.24 0 00-.437 1.632c0 1.126.571 2.12 1.437 2.701A3.217 3.217 0 01.634 6.27v.04a3.248 3.248 0 002.592 3.183 3.225 3.225 0 01-1.459.054 3.233 3.233 0 003.017 2.254 6.464 6.464 0 01-4.013 1.386c-.26 0-.518-.015-.771-.043a9.11 9.11 0 004.952 1.46c5.944 0 9.192-4.946 9.192-9.236l-.01-.42a6.456 6.456 0 001.616-1.676z"></path></svg></i><span class="css-kek3bb">Twitter</span></a><a rel="noopener" target="_blank" href="https://www.linkedin.com/school/datacampinc/" class="css-xlmvza"><i aria-hidden="true"><svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" width="17" height="17"><path d="M15.75 9.227v5.823h-3.376V9.617c0-1.365-.488-2.296-1.71-2.296-.932 0-1.487.628-1.732 1.235-.09.217-.112.519-.112.822v5.672H5.442s.047-9.202 0-10.155H8.82v1.44l-.023.032h.023v-.032c.448-.691 1.25-1.678 3.042-1.678 2.223 0 3.888 1.451 3.888 4.57zM1.911 0C.756 0 0 .758 0 1.754 0 2.73.734 3.51 1.866 3.51h.022c1.178 0 1.91-.78 1.91-1.755C3.776.758 3.066 0 1.911 0zM.201 15.05h3.375V4.895H.2V15.05z"></path><path d="M.125.125h15.75v15.75H.125z" fill="none" fill-rule="evenodd"></path></svg></i><span class="css-kek3bb">LinkedIn</span></a><a rel="noopener" target="_blank" href="https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A" class="css-xlmvza"><i aria-hidden="true"> <svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" width="17" height="17"><path d="M5.904 11.375l5.25-3.045-5.25-3.08v6.125zm1.963-8.75c.983 0 1.932.013 2.849.04a56.45 56.45 0 012.015.086l.64.036.149.015c.094.009.161.017.202.026l.207.041c.096.018.18.042.25.072.07.03.152.07.246.117.093.048.183.108.272.176.148.117.279.254.391.407.055.074.14.251.254.528.113.276.192.58.232.911a28.771 28.771 0 01.159 2.258v1.587c.005.875-.047 1.75-.158 2.619-.04.33-.114.63-.22.898-.104.267-.199.453-.281.554l-.123.154c-.082.09-.166.171-.254.24-.088.07-.179.126-.272.17-.081.04-.163.078-.246.114-.08.033-.165.057-.25.072l-.21.04a5.053 5.053 0 01-.347.042c-1.47.114-3.305.171-5.505.171a158.147 158.147 0 01-3.157-.059 64.523 64.523 0 01-1.76-.067l-.43-.036-.316-.036c-.161-.02-.32-.05-.478-.09a2.677 2.677 0 01-.448-.19 1.777 1.777 0 01-.633-.538c-.055-.074-.14-.25-.254-.528a3.538 3.538 0 01-.233-.911c-.05-.41-.086-.822-.11-1.234C.027 9.97.01 9.628 0 9.286V7.7a19.95 19.95 0 01.158-2.619c.041-.33.114-.63.22-.898.104-.267.199-.453.281-.554l.123-.155c.082-.091.166-.172.254-.24.088-.07.179-.129.272-.176.094-.049.175-.087.246-.117.08-.034.165-.058.25-.073.097-.017.166-.031.207-.04.041-.01.108-.018.201-.027l.15-.014c1.47-.108 3.304-.162 5.505-.162z"></path></svg></i><span class="css-kek3bb">YouTube</span></a><a rel="noopener" target="_blank" href="https://www.instagram.com/datacamp/" class="css-xlmvza"><i aria-hidden="true"><svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" width="20" height="20"><path fill-rule="evenodd" d="M10.28.374c2.359 0 2.7.013 3.688.058 1.024.047 1.724.21 2.336.447.643.242 1.225.62 1.705 1.11.49.481.869 1.063 1.11 1.705.239.613.401 1.312.448 2.337.047 1.026.058 1.354.058 3.968 0 2.615-.011 2.942-.058 3.969-.047 1.025-.21 1.724-.447 2.337a4.927 4.927 0 01-2.815 2.815c-.613.238-1.312.4-2.337.447-.874.04-1.242.054-2.947.057H8.979c-1.706-.003-2.073-.017-2.948-.057-1.025-.047-1.724-.21-2.336-.447a4.705 4.705 0 01-1.705-1.11 4.709 4.709 0 01-1.11-1.705c-.239-.613-.401-1.312-.448-2.337-.047-1.026-.058-1.354-.058-3.969v-.28c0-2.359.013-2.7.058-3.688.047-1.025.21-1.724.447-2.337a4.722 4.722 0 011.11-1.705A4.722 4.722 0 013.694.879c.613-.238 1.312-.4 2.337-.447C7.057.385 7.385.374 9.999.374zm.523 1.735H9.196c-1.846.002-2.188.014-3.086.055-.939.043-1.448.2-1.787.332-.419.154-.797.4-1.107.72-.32.31-.566.688-.72 1.107-.132.339-.289.848-.332 1.787-.039.859-.052 1.209-.055 2.853v2.073c.003 1.644.016 1.994.055 2.853.043.939.2 1.448.332 1.788.154.418.4.796.72 1.106.31.32.688.566 1.106.72.34.132.85.289 1.788.332.859.039 1.209.052 2.853.055h2.073c1.644-.003 1.994-.016 2.853-.055.938-.043 1.448-.2 1.787-.332a3.185 3.185 0 001.827-1.826c.132-.34.289-.85.332-1.788.04-.898.053-1.24.055-3.086V9.196c-.002-1.846-.015-2.188-.055-3.086-.043-.939-.2-1.448-.332-1.787a2.972 2.972 0 00-.72-1.107 2.972 2.972 0 00-1.107-.72c-.339-.132-.849-.289-1.787-.332-.898-.04-1.24-.053-3.086-.055zM10 5.057a4.943 4.943 0 110 9.885 4.943 4.943 0 010-9.885zm0 1.734a3.208 3.208 0 100 6.417 3.21 3.21 0 10-.001-6.417zm5.138-3.085a1.156 1.156 0 110 2.311 1.156 1.156 0 010-2.311z"></path></svg></i><span class="css-kek3bb">Instagram</span></a></div><style data-emotion="css 14kygk">.css-14kygk{-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;grid-area:1/1/2/2;margin-left:-12px;margin-top:20px;}@media screen and (min-width: 480px){.css-14kygk{-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}}@media screen and (min-width: 1200px){.css-14kygk{-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;}}</style><style data-emotion="css bcch0t">.css-bcch0t{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;grid-area:1/1/2/2;margin-left:-12px;margin-top:20px;}@media screen and (min-width: 480px){.css-bcch0t{-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}}@media screen and (min-width: 1200px){.css-bcch0t{-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;}}</style><style data-emotion="css y9gunu">.css-y9gunu{box-sizing:border-box;margin:0;min-width:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;grid-area:1/1/2/2;margin-left:-12px;margin-top:20px;}@media screen and (min-width: 480px){.css-y9gunu{-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}}@media screen and (min-width: 1200px){.css-y9gunu{-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;}}</style><div class="css-y9gunu"><style data-emotion="css xkpyuy">.css-xkpyuy{font-size:0.875rem;padding-left:12px;padding-right:12px;padding-top:9px;padding-bottom:9px;}</style><style data-emotion="css 9dz0qq">.css-9dz0qq{box-sizing:border-box;margin:0;min-width:0;border:0;color:#ffffff;font-weight:400;opacity:0.75;-webkit-text-decoration:none;text-decoration:none;font-size:0.875rem;padding-left:12px;padding-right:12px;padding-top:9px;padding-bottom:9px;}.css-9dz0qq:active,.css-9dz0qq:focus,.css-9dz0qq:hover{border:0;color:#ffffff;opacity:1;}</style><a class="css-9dz0qq" href="/privacy-policy">Privacy Policy</a><a class="css-9dz0qq" href="/cookie-notice">Cookie Notice</a><a class="css-9dz0qq" href="/do-not-sell-my-personal-information">Do Not Sell My Personal Information</a><a class="css-9dz0qq" href="/accessibility">Accessibility</a><a class="css-9dz0qq" href="/security">Security</a><a class="css-9dz0qq" href="/terms-of-use">Terms of Use</a></div><style data-emotion="css qen30u">.css-qen30u{color:#ffffff;font-size:0.75rem;grid-area:2/1/3/2;margin-top:16px;opacity:0.5;}@media screen and (min-width: 480px){.css-qen30u{text-align:center;}}@media screen and (min-width: 1200px){.css-qen30u{margin-top:0;padding-top:25px;text-align:left;}}</style><p class="css-qen30u">© 2024 DataCamp, Inc. All Rights Reserved.</p></div></div></footer></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"carouselComponents":[{"__typename":"ComponentDataCampProductsCoursesOrTracksCarousel","id":"229","title":"\u003cp\u003eLearn more about Python and PySpark\u003c/p\u003e","description":"\u003cp\u003eLearn more about Python and PySpark\u003c/p\u003e","seeMoreUrl":null,"carousel":[{"algoliaItem":"4452__Introduction to PySpark__introduction-to-pyspark","description":"\u003cp\u003eIn this course, you'll learn how to use Spark from Python! Spark is a tool for doing parallel computation with large datasets and it integrates well with Python. PySpark is the Python package that makes the magic happen. You'll use this package to work with data about flights from Portland and Seattle. You'll learn to wrangle this data and build a whole machine learning pipeline to predict whether or not flights will be delayed. Get ready to put some Spark in your Python code and dive into the world of high-performance machine learning!\u003c/p\u003e"},{"algoliaItem":"14989__Machine Learning with PySpark__machine-learning-with-pyspark","description":"\u003cp\u003eSpark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you'll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you'll analyse a large dataset of flight delays and spam text messages. With this background you'll be ready to harness the power of Spark and apply it on your own Machine Learning projects!\u003c/p\u003e"},{"algoliaItem":"16473__Cleaning Data with PySpark__cleaning-data-with-pyspark","description":"\u003cp\u003eWorking with data is tricky - working with millions or even billions of rows is worse. Did you receive some data processing code written on a laptop with fairly pristine data? Chances are you\u0026rsquo;ve probably been put in charge of moving a basic data process from prototype to production. You may have worked with real world datasets, with missing fields, bizarre formatting, and orders of magnitude more data. Even if this is all new to you, this course helps you learn what\u0026rsquo;s needed to prepare data processes using Python with Apache Spark. You\u0026rsquo;ll learn terminology, methods, and some best practices to create a performant, maintainable, and understandable data processing platform.\u003c/p\u003e"},{"algoliaItem":"6249__Feature Engineering with PySpark__feature-engineering-with-pyspark","description":"\u003cp\u003eThe real world is messy and your job is to make sense of it. Toy datasets like MTCars and Iris are the result of careful curation and cleaning, even so the data needs to be transformed for it to be useful for powerful machine learning algorithms to extract meaning, forecast, classify or cluster. This course will cover the gritty details that data scientists are spending 70-80% of their time on; data wrangling and feature engineering. With size of datasets now becoming ever larger, let's use PySpark to cut this Big Data problem down to size!\u003c/p\u003e"},{"algoliaItem":"13023__Big Data Fundamentals with PySpark__big-data-fundamentals-with-pyspark","description":"\u003cp\u003eThere's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is a \"lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. You\u0026rsquo;ll use PySpark, a Python package for Spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc. You will explore the works of William Shakespeare, analyze Fifa 2018 data and perform clustering on genomic datasets. At the end of this course, you will have gained an in-depth understanding of PySpark and its application to general Big Data analysis.\u003c/p\u003e"},{"algoliaItem":"7049__Building Recommendation Engines with PySpark__recommendation-engines-in-pyspark","description":"\u003cp\u003eThis course will show you how to build recommendation engines using Alternating Least Squares in PySpark. Using the popular MovieLens dataset and the Million Songs dataset, this course will take you step by step through the intuition of the Alternating Least Squares algorithm as well as the code to train, test and implement ALS models on various types of customer data.\u003c/p\u003e"}],"cardResponses":[{"slug":"introduction-to-pyspark","title":"Introduction to PySpark","contentType":"course","numberOfUsers":134846,"shortDescription":"Learn to implement distributed data management and machine learning in Spark using the PySpark package.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"machine-learning-with-pyspark","title":"Machine Learning with PySpark","contentType":"course","numberOfUsers":22006,"shortDescription":"Learn how to make predictions from data with Apache Spark, using decision trees, logistic regression, linear regression, ensembles, and pipelines.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"cleaning-data-with-pyspark","title":"Cleaning Data with PySpark","contentType":"course","numberOfUsers":24770,"shortDescription":"Learn how to clean data with Apache Spark in Python.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"feature-engineering-with-pyspark","title":"Feature Engineering with PySpark","contentType":"course","numberOfUsers":13781,"shortDescription":"Learn the gritty details that data scientists are spending 70-80% of their time on; data wrangling and feature engineering.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"big-data-fundamentals-with-pyspark","title":"Big Data Fundamentals with PySpark","contentType":"course","numberOfUsers":46985,"shortDescription":"Learn the fundamentals of working with big data with PySpark.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"recommendation-engines-in-pyspark","title":"Building Recommendation Engines with PySpark","contentType":"course","numberOfUsers":11686,"shortDescription":"Learn tools and techniques to leverage your own big data to facilitate positive experiences for your users.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false}]}],"categories":[{"id":20,"tag":"Scala","slug":"scala","categoryShortDescription":[{"id":18,"shortDescription":"Discover more about Scala in our podcast, including the latest updates and use cases. Learn more about importing, cleaning, processing data, \u0026 more with Scala.","contentType":"podcast"},{"id":40,"shortDescription":"Learn more about Scala using our blog. We share use cases \u0026 free resources to grow your programming skills. We'll keep you up to date with the latest techniques \u0026 news.","contentType":"blog"},{"id":54,"shortDescription":"Discover more about Scala with our tutorials, including the latest updates and use cases. Learn more about importing, cleaning, processing data, \u0026 more with Scala.","contentType":"tutorial"},{"id":97,"shortDescription":"Discover more about Scala with our cheat sheets, including the latest updates and use cases. Learn more about importing, cleaning, processing data, \u0026 more with Scala.","contentType":"cheatSheet"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":17,"tag":"DataLab","slug":"datalab","categoryShortDescription":[{"id":10,"shortDescription":"From upskilling to portfolio curation, discover how our learners are utilizing DataLab workbooks to upskill themselves \u0026 their teams. ","contentType":"podcast"},{"id":44,"shortDescription":"From upskilling to portfolio curation, discover how our learners are utilizing DataLab to upskill themselves \u0026 their teams. ","contentType":"blog"},{"id":78,"shortDescription":"From upskilling to portfolio curation, our tutorials will show you how to get the most out of DataLab.","contentType":"tutorial"},{"id":102,"shortDescription":"From upskilling to portfolio curation, our tutorials will show you how to get the most out of DataLab.","contentType":"cheatSheet"},{"id":160,"shortDescription":"From upskilling to portfolio curation, discover how our learners are coding along using DataLab to upskill themselves \u0026 their teams. ","contentType":"codeAlong"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":10,"tag":"Data Engineering","slug":"data-engineering","categoryShortDescription":[{"id":4,"shortDescription":"Listen to our data engineering podcast episodes to gain extra insight into how to build the tools, infrastructure, \u0026 frameworks to support data fluency in your business.","contentType":"podcast"},{"id":25,"shortDescription":"Read our data engineering blog to gain extra insight into how to build the tools, infrastructure, \u0026 frameworks to support data fluency in your business.","contentType":"blog"},{"id":63,"shortDescription":"Read our data engineering blog to gain extra insight into how to build the tools, infrastructure, \u0026 frameworks to support data fluency in your business.","contentType":"tutorial"},{"id":85,"shortDescription":"Read our data engineering cheat sheets to gain extra insight into how to build the tools, infrastructure, \u0026 frameworks to support data fluency in your business.","contentType":"cheatSheet"},{"id":151,"shortDescription":"Join us for our data engineering code alongs to gain extra insight into how to build the tools, infrastructure, \u0026 frameworks to support data fluency in your business.","contentType":"codeAlong"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":24,"tag":"Git","slug":"git","categoryShortDescription":[{"id":16,"shortDescription":"Our Git podcast talks version control for data science. Learn more about Git, including how-tos, workflows, \u0026 news, as well as features of the latest release. ","contentType":"podcast"},{"id":30,"shortDescription":"Our Git blog talks version control for data science. Learn more about Git, including how-tos, workflows, \u0026 news, as well as features of the latest release. ","contentType":"blog"},{"id":70,"shortDescription":"Our Git tutorials provide version control tips for data science. Learn more about Git, including how-tos, workflows, \u0026 features of the latest release. ","contentType":"tutorial"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":196,"tag":"ChatGPT","slug":"chatgpt","categoryShortDescription":[{"id":546,"shortDescription":"Keep up to date with the latest news, techniques, and resources for ChatGPT. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"},{"id":547,"shortDescription":"Keep up to date with the latest news, techniques, and resources for ChatGPT. Our blog is full of practical articles \u0026 use cases you can use to upskill.","contentType":"blog"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":26,"tag":"Power BI","slug":"power-bi","categoryShortDescription":[{"id":15,"shortDescription":"Learn more about analyzing data in our Power BI podcast. Hear about use cases, discover techniques, and gain insights into creating databases, transforming data, \u0026 more.","contentType":"podcast"},{"id":36,"shortDescription":"Learn more about analyzing data in our Power BI blog. Read use cases, discover techniques, and gain insights into creating databases, transforming data, \u0026 more.","contentType":"blog"},{"id":71,"shortDescription":"Learn more about analyzing data in our Power BI tutorials. Read use cases, discover techniques, and gain insights into creating databases, transforming data, \u0026 more.","contentType":"tutorial"},{"id":94,"shortDescription":"Learn more about analyzing data in our Power BI cheat sheets. Read use cases, discover techniques, and gain insights into creating databases, transforming data, \u0026 more.","contentType":"cheatSheet"},{"id":114,"shortDescription":"Learn more about analyzing data in our Power BI webinars. Read use cases, discover techniques, and gain insights into creating databases, transforming data, \u0026 more.","contentType":"webinar"},{"id":156,"shortDescription":"Learn more about analyzing data in our Power BI code alongs. Try use cases, discover techniques, and gain insights into creating databases, transforming data, \u0026 more.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":15,"tag":"Big Data","slug":"big-data","categoryShortDescription":[{"id":173,"shortDescription":"Learn all about the techniques \u0026 tools that data scientists are using to handle, process, \u0026 analyse big data. Gain insight to help upskill your career \u0026 team.","contentType":"podcast"},{"id":174,"shortDescription":"Learn all about the techniques \u0026 tools that data scientists are using to handle, process, \u0026 analyze big data. Gain insight to help upskill your career \u0026 team.","contentType":"blog"},{"id":176,"shortDescription":"Learn how to implement the tools that data scientists are using to handle, process, \u0026 analyse big data. Gain insight to help upskill your career \u0026 team.","contentType":"tutorial"},{"id":175,"shortDescription":"Get the basic tools that data scientists are using to handle, process, \u0026 analyse big data. Gain insight to help upskill your career \u0026 team.","contentType":"cheatSheet"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":204,"tag":"Large Language Models","slug":"large-language-models","categoryShortDescription":[{"id":565,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Large Language Models. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"},{"id":564,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Large Language Models. Our blog talks about data science, uses, \u0026 responsible AI practices.","contentType":"blog"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":34,"tag":"Career Services","slug":"career-services","categoryShortDescription":[{"id":103,"shortDescription":"Source of job hunting help","contentType":"blog"},{"id":104,"shortDescription":"Source of job hunting help","contentType":"podcast"},{"id":105,"shortDescription":"Source of job hunting help","contentType":"tutorial"},{"id":106,"shortDescription":"Source of job hunting help","contentType":"cheatSheet"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":205,"tag":"Generative AI","slug":"generative-ai","categoryShortDescription":[{"id":566,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Generative AI. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"},{"id":567,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Generative AI. Our blog talks about data science, uses, \u0026 responsible AI practices.","contentType":"blog"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":9,"tag":"Data Analysis","slug":"data-analysis","categoryShortDescription":[{"id":3,"shortDescription":"Get insights on the latest tools \u0026 methods that you can use to analyze your data more effectively. Our data analysis podcast will help you develop your skills.","contentType":"podcast"},{"id":22,"shortDescription":"Get insights on the latest tools \u0026 methods that you can use to analyze your data more effectively. Our data analysis blog will help you develop your skills.","contentType":"blog"},{"id":56,"shortDescription":"Get insights on the latest tools \u0026 methods that you can use to analyze your data more effectively. Our data analysis tutorials will help you develop your skills.","contentType":"tutorial"},{"id":82,"shortDescription":"Get insights on the latest tools \u0026 methods that you can use to analyze your data more effectively. Our data analysis cheat sheets will help you develop your skills.","contentType":"cheatSheet"},{"id":120,"shortDescription":"Get insights on the latest tools \u0026 methods that you can use to analyze your data more effectively. Our data analysis webinars will help you develop your skills.","contentType":"webinar"},{"id":149,"shortDescription":"Get insights on the latest tools \u0026 methods that you can use to analyze your data more effectively. Our data analysis code alongs will help you develop your skills.","contentType":"codeAlong"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":216,"tag":"Excel","slug":"excel","categoryShortDescription":[{"id":589,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Excel. Our blog talks about data science, uses, \u0026 responsible AI practices.","contentType":"blog"},{"id":588,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Excel. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":8,"tag":"Data Science","slug":"data-science","categoryShortDescription":[{"id":5,"shortDescription":"Get advice, learn new skills, \u0026 advance your data career in our data science podcast. We talk about programming languages, industry news, \u0026 much more.","contentType":"podcast"},{"id":26,"shortDescription":"Read advice, learn new skills, \u0026 advance your data career in our data science blog. We talk about programming languages, industry news, \u0026 much more.","contentType":"blog"},{"id":55,"shortDescription":"Advance your data career with our data science tutorials. We walk you through challenging data science functions and models step-by-step.  ","contentType":"tutorial"},{"id":87,"shortDescription":"Advance your data career with our data science cheat sheets. We walk you through challenging data science functions and models step-by-step.  ","contentType":"cheatSheet"},{"id":121,"shortDescription":"Advance your data career with our data science webinars. We walk you through challenging data science functions and models step-by-step.  ","contentType":"webinar"},{"id":152,"shortDescription":"Work your way through an entire data science project with the help of a screencast, so you never get stuck.","contentType":"codeAlong"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":21,"tag":"R Programming","slug":"r-programming","categoryShortDescription":[{"id":13,"shortDescription":"In our R podcast, we share practical tips, use cases, \u0026 free resources to grow your programming skills. We'll keep you up to date with the latest techniques \u0026 news.","contentType":"podcast"},{"id":39,"shortDescription":"In our R blog, we share practical tips, use cases, \u0026 free resources to grow your programming skills. We'll keep you up to date with the latest techniques \u0026 news.","contentType":"blog"},{"id":73,"shortDescription":"In our R tutorials, you'll find helpful tips and use cases to grow your programming skills. We'll keep you up to date with the latest techniques.","contentType":"tutorial"},{"id":96,"shortDescription":"In our R cheat sheets, you'll find helpful tips and use cases to grow your programming skills. We'll keep you up to date with the latest techniques.","contentType":"cheatSheet"},{"id":117,"shortDescription":"In our R webinars, you'll find helpful tips and use cases to grow your programming skills. We'll keep you up to date with the latest techniques.","contentType":"webinar"},{"id":158,"shortDescription":"In our R code-alongs, we share practical tips, use cases, \u0026 free resources to grow your programming skills. We'll keep you up to date with the latest techniques \u0026 news.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":22,"tag":"Tableau","slug":"tableau","categoryShortDescription":[{"id":14,"shortDescription":"Learn more about data visualization using our Tableau podcast. We'll share resources, use cases, techniques, and news for all sorts of data \u0026 visualizations.","contentType":"podcast"},{"id":42,"shortDescription":"Learn more about data visualization using our Tableau blog. We'll share resources, use cases, techniques, and news for all sorts of data \u0026 visualizations.","contentType":"blog"},{"id":77,"shortDescription":"Learn more about data visualization using our Tableau tutorials. We'll share use cases and techniques for all sorts of data \u0026 visualizations.","contentType":"tutorial"},{"id":100,"shortDescription":"Learn more about data visualization using our Tableau cheat sheets. We'll share use cases and techniques for all sorts of data \u0026 visualizations.","contentType":"cheatSheet"},{"id":113,"shortDescription":"Learn more about data visualization using our Tableau webinars. We'll share use cases and techniques for all sorts of data \u0026 visualizations.","contentType":"webinar"},{"id":541,"shortDescription":"Get insights on the latest tools \u0026 methods that you can use to analyze your data more effectively. Our Tableau code alongs will help you develop your skills.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":14,"tag":"Deep Learning","slug":"deep-learning","categoryShortDescription":[{"id":8,"shortDescription":"Learn how to use AI to speed up data analysis and processes from articles in our deep learning podcast. Discover techniques like using Keras and GPT-3 to upskill.","contentType":"podcast"},{"id":28,"shortDescription":"Learn how to use AI to speed up data analysis and processes from articles in our deep learning blog. Discover techniques like using Keras and GPT-3 to upskill.","contentType":"blog"},{"id":67,"shortDescription":"Learn how to use AI to speed up data analysis and processes in our deep learning tutorials. Upskill with our deep learning tips, tricks, and techniques. ","contentType":"tutorial"},{"id":89,"shortDescription":"Learn how to use AI to speed up data analysis and processes in our deep learning cheat sheets. Upskill with our deep learning tips, tricks, and techniques. ","contentType":"cheatSheet"},{"id":154,"shortDescription":"Learn how to use AI to speed up data analysis and processes in our deep learning code alongs. Discover techniques like using Keras and GPT-3 to upskill.","contentType":"codeAlong"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":195,"tag":"AWS","slug":"aws","categoryShortDescription":[{"id":545,"shortDescription":"Keep up to date with the latest news, techniques, and resources for AWS. Our blog is full of practical articles \u0026 use cases you can use to upskill.","contentType":"blog"},{"id":544,"shortDescription":"Get insights on the latest AWS tools and techniques. Our AWS tutorials will help you develop your skills.","contentType":"tutorial"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":16,"tag":"Artificial Intelligence (AI)","slug":"ai","categoryShortDescription":[{"id":1,"shortDescription":"Keep up to date with the latest techniques, tools, and research in artificial intelligence. The DataFramed podcast talks about data science, uses, \u0026 responsible AI practices.","contentType":"podcast"},{"id":19,"shortDescription":"Keep up to date with the latest techniques, tools, and research in artificial intelligence. Our AI blog talks about data science, uses, \u0026 responsible AI practices.","contentType":"blog"},{"id":58,"shortDescription":"Keep up to date with the latest techniques, tools, and research in artificial intelligence. Out AI tutorials will walk you though challenging machine learning models.  ","contentType":"tutorial"},{"id":79,"shortDescription":"Keep up to date with the latest techniques, tools, and research in artificial intelligence. Our AI cheat sheets summarise about data science, uses, \u0026 responsible AI practices.","contentType":"cheatSheet"},{"id":150,"shortDescription":"Keep up to date with the latest techniques, tools, and research in artificial intelligence. Our AI code alongs focus on data science, uses, \u0026 responsible AI practices.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":200,"tag":"OpenAI","slug":"OpenAI","categoryShortDescription":[{"id":555,"shortDescription":"Keep up to date with the latest news, techniques, and resources for OpenAI. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"},{"id":554,"shortDescription":"Keep up to date with the latest news, techniques, and resources for OpenAI. Our blog is full of practical articles \u0026 use cases you can use to upskill.","contentType":"blog"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":25,"tag":"Spreadsheets","slug":"spreadsheets","categoryShortDescription":[{"id":17,"shortDescription":"Learn more about spreadsheets for data analysis, visualisation, and processing. Our spreadsheet podcast has tips \u0026 tricks for various spreadsheets applications.","contentType":"podcast"},{"id":41,"shortDescription":"Learn more about spreadsheets for data analysis, visualisation, and processing. Our spreadsheet blog has tips \u0026 tricks for various spreadsheet applications.","contentType":"blog"},{"id":74,"shortDescription":"Learn more about spreadsheets for data analysis, visualisation, and processing. Our spreadsheet tutorials include tips \u0026 tricks for various spreadsheets applications.","contentType":"tutorial"},{"id":98,"shortDescription":"Learn more about spreadsheets for data analysis, visualisation, and processing. Our spreadsheet cheat sheets include tips \u0026 tricks for various spreadsheets applications.","contentType":"cheatSheet"},{"id":543,"shortDescription":"Work your way through an entire spreadsheets project with the help of a screencast, so you never get stuck.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":39,"tag":"Business Intelligence","slug":"learn-business-intelligence","categoryShortDescription":[{"id":181,"shortDescription":"Find extra resources to help upskill you and/or your team. Find out all about business intelligence.","contentType":"webinar"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":38,"tag":"Julia","slug":"julia","categoryShortDescription":[{"id":110,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Julia programming. Our blog is full of practical articles \u0026 use cases you can use to upskill.\n","contentType":"blog"},{"id":111,"shortDescription":"Keep up to date with the latest techniques and resources for Julia programming. Our cheat sheets are full of free Julia training tips to help you learn, build, \u0026 grow.","contentType":"cheatSheet"},{"id":108,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Julia programming. Our podcast is full of practical articles \u0026 use cases you can use to upskill.","contentType":"podcast"},{"id":109,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Julia programming. Our tutorials are full of practical walk-throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"},{"id":112,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Julia programming. Our webinars are full of practical walk-throughs \u0026 use cases you can use to upskill.","contentType":"webinar"},{"id":542,"shortDescription":"Work your way through an entire Julia project with the help of a screencast, so you never get stuck.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":11,"tag":"Data Literacy","slug":"data-literacy","categoryShortDescription":[{"id":6,"shortDescription":"Find extra resources to help upskill you and/or your team. Hear all about data literacy, what it is, \u0026 how to better achieve it to think critically about your data.","contentType":"podcast"},{"id":64,"shortDescription":"Find extra resources to help upskill you and/or your team. Read all about data literacy, what it is, \u0026 how to better achieve it to think critically about your data.","contentType":"blog"},{"id":65,"shortDescription":"Find extra resources to help upskill you and/or your team. Our data literacy tutorials will help you think critically about your data.","contentType":"tutorial"},{"id":86,"shortDescription":"Find extra resources to help upskill you and/or your team. Our data literacy tutorials will help you think critically about your data.","contentType":"cheatSheet"},{"id":119,"shortDescription":"Find extra resources to help upskill you and/or your team. Our data literacy webinars will help you think critically about your data.","contentType":"webinar"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":209,"tag":"Microsoft Azure","slug":"microsoft-azure","categoryShortDescription":[{"id":574,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Microsoft Azure. Our blog talks about data science, uses, \u0026 responsible AI practices.","contentType":"blog"},{"id":575,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Microsoft Azure. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":19,"tag":"SQL","slug":"sql","categoryShortDescription":[{"id":12,"shortDescription":"Keep up to date with the latest news, techniques, and resources for SQL Server. Our podcast is full of free SQL Server training tips to help you learn, build, \u0026 grow. ","contentType":"podcast"},{"id":75,"shortDescription":"Keep up to date with the latest techniques and resources for SQL Server. Our tutorials are full of free SQL Server training tips to help you learn, build, \u0026 grow. ","contentType":"tutorial"},{"id":76,"shortDescription":"Keep up to date with the latest news, techniques, and resources for SQL Server. Our blog is full of free SQL Server training tips to help you learn, build, \u0026 grow. ","contentType":"blog"},{"id":99,"shortDescription":"Keep up to date with the latest techniques and resources for SQL Server. Our cheat sheets are full of free SQL Server training tips to help you learn, build, \u0026 grow. ","contentType":"cheatSheet"},{"id":115,"shortDescription":"Keep up to date with the latest techniques and resources for SQL Server. Our webinars are full of free SQL Server training tips to help you learn, build, \u0026 grow. ","contentType":"webinar"},{"id":159,"shortDescription":"Keep up to date with the latest news, techniques, and resources for SQL Server. Our code-alongs are full of free SQL Server training tips to help you learn, build, \u0026 grow. ","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":229,"tag":"MLOps","slug":"mlops","categoryShortDescription":[{"id":614,"shortDescription":"Keep up to date with the latest techniques, tools, and research in MLOps. Our blog talks about data science, uses, \u0026 responsible AI practices.","contentType":"blog"},{"id":615,"shortDescription":"Keep up to date with the latest news, techniques, and resources for MLOps. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":127,"tag":"Snowflake","slug":"snowflake","categoryShortDescription":[{"id":340,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Snowflake. The DataFramed podcast talks about data science, uses, \u0026 responsible AI practices.","contentType":"podcast"},{"id":343,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Snowflake.","contentType":"blog"},{"id":342,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Snowflake. Our tutorials will walk you though challenging machine learning models.  ","contentType":"tutorial"},{"id":344,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Snowflake. Our cheat sheets summarise about data science, uses, \u0026 responsible AI practices.","contentType":"cheatSheet"},{"id":339,"shortDescription":"Keep up to date with the latest techniques, tools, and research in Snowflake. Our code alongs focus on data science, uses, \u0026 responsible AI practices.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":18,"tag":"Python","slug":"python","categoryShortDescription":[{"id":11,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our podcast is full of practical articles \u0026 use cases you can use to upskill.","contentType":"podcast"},{"id":38,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our blog is full of practical articles \u0026 use cases you can use to upskill.","contentType":"blog"},{"id":72,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"},{"id":95,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our cheat sheets are full of practical articles \u0026 use cases you can use to upskill.","contentType":"cheatSheet"},{"id":116,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our webinars are full of practical articles \u0026 use cases you can use to upskill.","contentType":"webinar"},{"id":157,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our code alongs are full of practical use cases you can use to upskill.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},{"id":12,"tag":"Data Visualization","slug":"data-visualization","categoryShortDescription":[{"id":7,"shortDescription":"Learn more about turning your data \u0026 information into insightful infographics \u0026 visualizations. Discover new ways to storytell, build dashboards, \u0026 more.","contentType":"podcast"},{"id":27,"shortDescription":"Learn more about turning your data \u0026 information into insightful infographics \u0026 visualizations. Discover new storytelling techniques, build dashboards, \u0026 more.","contentType":"blog"},{"id":66,"shortDescription":"Learn more about turning your data \u0026 information into insightful infographics \u0026 visualizations with our tutorials. Discover new ways to storytell, build dashboards, \u0026 more.","contentType":"tutorial"},{"id":88,"shortDescription":"Learn more about turning your data \u0026 information into insightful infographics \u0026 visualizations with our cheat sheets. Discover new ways to storytell, build dashboards, \u0026 more.","contentType":"cheatSheet"},{"id":153,"shortDescription":"Learn more about turning your data \u0026 information into insightful infographics \u0026 visualizations. Discover new storytelling techniques, build dashboards, \u0026 more.","contentType":"codeAlong"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}},{"id":13,"tag":"Machine Learning","slug":"machine-learning","categoryShortDescription":[{"id":9,"shortDescription":"Get insights \u0026 best practices into AI \u0026 machine learning to drive data transformation, upskill, and build data cultures. Discover how you can use ML in your work.","contentType":"podcast"},{"id":34,"shortDescription":"Get insights \u0026 best practices into AI \u0026 machine learning to drive data transformation, upskill, and build data cultures. Discover how you can use ML in your work.","contentType":"blog"},{"id":68,"shortDescription":"Get insights \u0026 best practices into AI \u0026 machine learning, upskill, and build data cultures. Learn how to get the most out of machine learning models with our tutorials.","contentType":"tutorial"},{"id":93,"shortDescription":"Get insights \u0026 best practices into AI \u0026 machine learning, upskill, and build data cultures. Learn how to get the most out of machine learning models with our cheat sheets.","contentType":"cheatSheet"},{"id":155,"shortDescription":"Get insights \u0026 best practices into AI \u0026 machine learning to drive data transformation, upskill, and build data cultures. Discover how you can use ML in your work.","contentType":"codeAlong"}],"categoryGroup":{"id":2,"description":"Discover content by data science topics","subtitle":"Other topics:","tag":"Topics"}}],"categoryData":{"id":18,"tag":"Python","slug":"python","categoryShortDescription":[{"id":11,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our podcast is full of practical articles \u0026 use cases you can use to upskill.","contentType":"podcast"},{"id":38,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our blog is full of practical articles \u0026 use cases you can use to upskill.","contentType":"blog"},{"id":72,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our tutorials are full of practical walk throughs \u0026 use cases you can use to upskill.","contentType":"tutorial"},{"id":95,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our cheat sheets are full of practical articles \u0026 use cases you can use to upskill.","contentType":"cheatSheet"},{"id":116,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our webinars are full of practical articles \u0026 use cases you can use to upskill.","contentType":"webinar"},{"id":157,"shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our code alongs are full of practical use cases you can use to upskill.","contentType":"codeAlong"}],"categoryGroup":{"id":3,"description":"Discover content by tools and technology","subtitle":"Other technologies:","tag":"Technologies"}},"jobs":[],"page":{"localizations":[],"aiCodeBlockExplanations":[{"id":1346866,"codeBlock":{"code":"$ pip install pyspark","language":""},"completion":"This code installs the PySpark package using the pip package manager. PySpark is a Python library used for processing large datasets in a distributed computing environment using Apache Spark. The pip command is used to install Python packages from the Python Package Index (PyPI) repository. When executed, this command will download and install the latest version of the PySpark package and its dependencies on the local machine."},{"id":1346863,"codeBlock":{"code":"# Search for spark\n$ brew search spark\n\n# Get more information on apache-spark\n$ brew info apache-spark\n\n# Install apache-spark\n$ brew install apache-spark","language":"bash"},"completion":"This code is written in Bash, a Unix shell and command language. \n\nThe code is a set of commands that can be executed in a terminal to install Apache Spark, a popular big data processing framework. \n\nThe first command, `brew search spark`, searches for the keyword \"spark\" using Homebrew, a package manager for macOS. This command returns a list of packages related to Spark that can be installed using Homebrew.\n\nThe second command, `brew info apache-spark`, provides more information about the Apache Spark package, including its version, dependencies, and installation path.\n\nFinally, the third command, `brew install apache-spark`, installs the Apache Spark package using Homebrew. This command downloads and installs all the necessary dependencies and sets up the environment variables required to run Spark."},{"id":1346878,"codeBlock":{"code":"$ tar xvf spark-2.2.0-bin-hadoop2.7.tgz","language":"bash"},"completion":"This code is a Bash command that extracts the contents of a tar archive file named \"spark-2.2.0-bin-hadoop2.7.tgz\". The \"xvf\" options used with the \"tar\" command specify that the archive should be extracted and that the process should be verbose, displaying the names of the files as they are extracted."},{"id":1346868,"codeBlock":{"code":"$ mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark","language":"bash"},"completion":"This code is a Bash command that moves the directory \"spark-2.1.0-bin-hadoop2.7\" to the directory \"/usr/local/spark\". The \"mv\" command stands for \"move\" and is used to move or rename files and directories. In this case, it is moving the Spark directory to a specific location on the system. The \"/usr/local\" directory is a common location for installing software on Unix-based systems."},{"id":1346877,"codeBlock":{"code":"$ cd /usr/local/spark","language":"bash"},"completion":"This code is a command in the Bash shell. It changes the current working directory to `/usr/local/spark`. The `$` symbol at the beginning of the line indicates that this is a command to be executed in the terminal."},{"id":1346860,"codeBlock":{"code":"# Open and edit the file\n$ nano README.md\n\n# Just read the file \n$ cat README.md","language":"bash"},"completion":"This code snippet is written in Bash, a Unix shell and command language. \n\nThe first command `nano README.md` opens the file named \"README.md\" in the nano text editor, allowing the user to edit the file. \n\nThe second command `cat README.md` simply displays the contents of the file \"README.md\" in the terminal, without allowing the user to edit it."},{"id":1346869,"codeBlock":{"code":"$ build/mvn -DskipTests clean package run","language":"bash"},"completion":"This code is a command line instruction that uses Maven, a build automation tool for Java projects. \n\nThe command starts by calling the `mvn` executable located in the `build` directory. The `-DskipTests` flag tells Maven to skip running any tests during the build process. The `clean` command tells Maven to remove any previously generated files before building the project. The `package` command tells Maven to create a distributable package of the project, such as a JAR file. Finally, the `run` command tells Maven to execute the project after it has been built.\n\nOverall, this command is used to build and run a Java project using Maven."},{"id":1346864,"codeBlock":{"code":"$ ./bin/pyspark","language":""},"completion":"This code is a command line instruction to run the PySpark shell. The `$` symbol indicates the command line prompt, and `./bin/pyspark` is the command to run the PySpark shell. The `./` indicates that the command is being run from the current directory, and `bin/pyspark` is the path to the PySpark executable file. This code is typically run in a Unix or Linux environment."},{"id":1346859,"codeBlock":{"code":"Python 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016, 12:39:47) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n17/07/26 11:41:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/07/26 11:41:47 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0\n      /_/\n\nUsing Python version 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016 12:39:47)\nSparkSession available as 'spark'.\n\u0026gt;\u0026gt;\u0026gt;","language":"bash"},"completion":"This code is not actually a code snippet, but rather the output of running a Python 2.7.13 shell with Spark 2.2.0 installed. The output shows the version of Python and Spark being used, as well as some warnings and information about the default log level. The last line indicates that a SparkSession is available as 'spark', which means that the user can now interact with Spark using Python code."},{"id":1346890,"codeBlock":{"code":"export PYSPARK_DRIVER_PYTHON=\"/usr/local/ipython/bin/ipython\"","language":""},"completion":"This code sets the environment variable `PYSPARK_DRIVER_PYTHON` to the path of the IPython executable `/usr/local/ipython/bin/ipython`. This is typically used in a PySpark environment to specify the Python interpreter that should be used as the driver program."},{"id":1346876,"codeBlock":{"code":"\u0026gt;\u0026gt;\u0026gt; rdd1 = spark.sparkContext.parallelize([('a',7),('a',2),('b',2)])\n\u0026gt;\u0026gt;\u0026gt; rdd2 = spark.sparkContext.parallelize([(\"a\",[\"x\",\"y\",\"z\"]), (\"b\",[\"p\", \"r\"])])\n\u0026gt;\u0026gt;\u0026gt; rdd3 = spark.sparkContext.parallelize(range(100))","language":"bash"},"completion":"This code is written in Python and uses the PySpark library. \n\nThe first line creates an RDD (Resilient Distributed Dataset) called `rdd1` by parallelizing a list of tuples containing key-value pairs. The keys are strings 'a' and 'b', and the values are integers 7, 2, and 2. \n\nThe second line creates another RDD called `rdd2` by parallelizing a list of tuples containing key-value pairs. The keys are strings 'a' and 'b', and the values are lists of strings. \n\nThe third line creates an RDD called `rdd3` by parallelizing a range of integers from 0 to 99. \n\nOverall, this code demonstrates how to create RDDs in PySpark using the `parallelize` method."},{"id":1346882,"codeBlock":{"code":"\u0026gt;\u0026gt;\u0026gt; rdd1.reduce(lambda a,b: a+b)","language":"bash"},"completion":"This code is written in Python and it uses the `reduce()` function on an RDD (Resilient Distributed Dataset) object named `rdd1`. The `reduce()` function takes a function as an argument and applies it to the elements of the RDD in a cumulative way. In this case, the lambda function `lambda a,b: a+b` is used, which simply adds two elements together. The `reduce()` function applies this function to the first two elements of the RDD, then applies it to the result and the next element, and so on, until all elements have been processed and a single result is returned. Therefore, this code snippet sums all the elements in the RDD."},{"id":1346902,"codeBlock":{"code":"\u0026gt;\u0026gt;\u0026gt; rdd2.flatMapValues(lambda x: x).collect()\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]","language":"bash"},"completion":"This code is written in Python and uses PySpark RDDs (Resilient Distributed Datasets). \n\nThe `flatMapValues()` function is called on an RDD (`rdd2`) and takes a lambda function as an argument. The lambda function takes a single argument (`x`) and returns an iterable. \n\nIn this case, the lambda function simply returns `x`, which means that each value in the RDD will be returned as an iterable containing only that value. \n\nThe `flatMapValues()` function then flattens these iterables and returns a new RDD with the original keys and the flattened values. \n\nThe `collect()` function is then called on the new RDD to return a list of tuples containing the key-value pairs. \n\nSo, in this specific example, the output shows that the original RDD had two keys (`'a'` and `'b'`) and their corresponding values were flattened into individual elements. The resulting list contains tuples with each key-value pair."},{"id":1346861,"codeBlock":{"code":"# Import findspark \nimport findspark\n\n# Initialize and provide path\nfindspark.init(\"/usr/local/spark\")\n\n# Or use this alternative\n#findspark.init()","language":"python"},"completion":"This code imports the `findspark` module, which is used to locate the Spark installation on the local machine. The `findspark.init()` method is then called to initialize the Spark environment and provide the path to the Spark installation directory. In this case, the path is set to \"/usr/local/spark\". Alternatively, if no path is provided, `findspark.init()` will attempt to automatically locate the Spark installation on the machine."},{"id":1346894,"codeBlock":{"code":"# Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Build the SparkSession\nspark = SparkSession.builder \\\n   .master(\"local\") \\\n   .appName(\"Linear Regression Model\") \\\n   .config(\"spark.executor.memory\", \"1gb\") \\\n   .getOrCreate()\n   \nsc = spark.sparkContext","language":"python"},"completion":"This code is written in Python and it is used to create a SparkSession object. \n\nFirst, the code imports the SparkSession module from the pyspark.sql library. \n\nThen, the SparkSession object is built using the builder() method. The master() method is used to specify the URL of the cluster manager. In this case, it is set to \"local\" which means that the Spark application will run on a single machine. \n\nThe appName() method is used to set the name of the Spark application. In this case, it is set to \"Linear Regression Model\". \n\nThe config() method is used to set the configuration properties for the Spark application. In this case, it is setting the executor memory to 1gb. \n\nFinally, the getOrCreate() method is called to create a SparkSession object. The sc variable is then assigned the SparkContext object from the SparkSession object."},{"id":1346875,"codeBlock":{"code":"export SPARK_HOME=\"/usr/local/spark\"","language":"bash"},"completion":"This code sets the environment variable `SPARK_HOME` to the directory path `/usr/local/spark`. The `export` command is used in Unix-based systems to set environment variables that can be accessed by other processes or scripts. In this case, it is setting the `SPARK_HOME` variable to the location where Spark is installed on the system. This variable can then be used by other scripts or programs that need to reference the Spark installation directory."},{"id":1346862,"codeBlock":{"code":"# Set a fixed value for the hash seed secret\nexport PYTHONHASHSEED=0\n\n# Set an alternate Python executable\nexport PYSPARK_PYTHON=/usr/local/ipython/bin/ipython\n\n# Augment the default search path for shared libraries\nexport LD_LIBRARY_PATH=/usr/local/ipython/bin/ipython\n\n# Augment the default search path for private libraries \nexport PYTHONPATH=$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH:$SPARK_HOME/python/","language":"bash"},"completion":"This code snippet is a series of Bash commands that set environment variables for a Python and Spark installation. \n\nThe first command sets a fixed value for the hash seed secret, which is used to randomize hash functions. This can be useful for reproducibility in certain applications.\n\nThe second command sets an alternate Python executable to be used by PySpark. This can be useful if you have multiple Python installations on your system and want to specify which one to use.\n\nThe third command augments the default search path for shared libraries, which can be useful if you have custom libraries installed in non-standard locations.\n\nThe fourth command augments the default search path for private libraries, which can be useful if you have custom Python modules that you want to use with PySpark. \n\nOverall, these commands are used to customize the environment variables for a PySpark installation, which can be important for ensuring that your PySpark applications run correctly."},{"id":1346911,"codeBlock":{"code":"# Load in the data\nrdd = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.data')\n\n# Load in the header\nheader = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.domain')","language":"python"},"completion":"This code is written in Python and uses the PySpark library. \n\nThe first line loads in a dataset from a file located at the specified path. The `sc` variable refers to the SparkContext object, which is the entry point for Spark functionality. The `textFile()` method is used to read in the data as an RDD (Resilient Distributed Dataset), which is a fault-tolerant collection of elements that can be processed in parallel.\n\nThe second line loads in a header file located at the specified path. The header file contains information about the columns in the dataset. This file is also read in as an RDD using the `textFile()` method. \n\nOverall, these two lines of code are used to load in data and metadata for further processing and analysis using PySpark."},{"id":1346885,"codeBlock":{"code":"header.collect()","language":"python"},"completion":"This code is written in Python and it calls the `collect()` method on the `header` object. \n\nIn Spark, `header` is typically a `DataFrame` object that represents the first row of a dataset, which contains the column names. The `collect()` method is used to retrieve all the data from a distributed dataset and return it as a local array. \n\nTherefore, `header.collect()` retrieves the column names from the `header` DataFrame and returns them as a local array."},{"id":1346907,"codeBlock":{"code":"[u'longitude: continuous.', u'latitude: continuous.', u'housingMedianAge: continuous. ', u'totalRooms: continuous. ', u'totalBedrooms: continuous. ', u'population: continuous. ', u'households: continuous. ', u'medianIncome: continuous. ', u'medianHouseValue: continuous. ']","language":"python"},"completion":"This code snippet is a Python list containing strings that describe the features of a dataset. Each string represents a feature and its data type. The features include longitude, latitude, housing median age, total rooms, total bedrooms, population, households, median income, and median house value. The data type for each feature is continuous, which means that the values for each feature can take on any numerical value within a certain range. This list is commonly used in machine learning and data analysis to describe the features of a dataset."},{"id":1346900,"codeBlock":{"code":"rdd.take(2)","language":"python"},"completion":"This code is written in Python and it uses the `take()` method on an RDD (Resilient Distributed Dataset) object. The `take()` method is used to retrieve a specified number of elements from the RDD. In this case, the code is retrieving the first two elements of the RDD. The `take()` method returns a list of the specified number of elements."},{"id":1346865,"codeBlock":{"code":"[u'-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000', u'-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']","language":"python"},"completion":"This code snippet is a list of two strings, where each string represents a row of data. The data is in comma-separated format, where each value represents a feature of a housing dataset. The features are, in order: longitude, latitude, housing median age, total rooms, total bedrooms, population, households, median income, and median house value. \n\nThe 'u' before each string indicates that the string is a Unicode string."},{"id":1346867,"codeBlock":{"code":"# Split lines on commas\nrdd = rdd.map(lambda line: line.split(\",\"))\n\n# Inspect the first 2 lines \nrdd.take(2)","language":"python"},"completion":"This code is written in Python and it is using PySpark RDD (Resilient Distributed Datasets) to process data. \n\nThe first line of code is using the `map()` function to split each line in the RDD (which is assumed to be a CSV file) on commas. The `lambda` function is used to define the operation to be performed on each line. In this case, the `split()` method is called on each line, with the comma as the delimiter. The result is a new RDD with each line split into a list of values.\n\nThe second line of code is using the `take()` function to retrieve the first two lines of the RDD. This is a way to inspect the data and make sure that the splitting operation was successful. The result is a list of two lists, each containing the values from one line of the original CSV file."},{"id":1346906,"codeBlock":{"code":"[[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000'], [u'-122.220000', u'37.860000', u'21.000000', u'7099.000000', u'1106.000000', u'2401.000000', u'1138.000000', u'8.301400', u'358500.000000']]","language":"python"},"completion":"This code snippet is a list of lists in Python. Each inner list contains 9 elements, which are all strings. The first two elements in each inner list represent longitude and latitude coordinates, respectively. The third element represents the housing median age, the fourth element represents the total number of rooms, the fifth element represents the total number of bedrooms, the sixth element represents the population, the seventh element represents the number of households, the eighth element represents the median income, and the ninth element represents the median house value. \n\nThe 'u' before each string indicates that the string is a Unicode string."},{"id":1346873,"codeBlock":{"code":"# Inspect the first line \nrdd.first()\n\n# Take top elements\nrdd.top(2)","language":"python"},"completion":"This code is written in Python and is using an RDD (Resilient Distributed Dataset) object. \n\nThe first line, `rdd.first()`, retrieves the first element of the RDD. This is useful for quickly inspecting the data in the RDD without having to iterate through the entire dataset.\n\nThe second line, `rdd.top(2)`, retrieves the top 2 elements of the RDD based on their natural ordering. This is useful for quickly identifying the largest or smallest elements in the RDD. The `top()` method takes an argument that specifies the number of elements to retrieve."},{"id":1346871,"codeBlock":{"code":"[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000']","language":"python"},"completion":"This code snippet is a Python list containing several elements, each of which is a string. The strings represent different values, such as longitude, latitude, and housing prices. The 'u' before each string indicates that they are Unicode strings. The values are separated by commas and enclosed in square brackets, which is the syntax for creating a list in Python."},{"id":1346870,"codeBlock":{"code":"# Import the necessary modules \nfrom pyspark.sql import Row\n\n# Map the RDD to a DF\ndf = rdd.map(lambda line: Row(longitude=line[0], \n                              latitude=line[1], \n                              housingMedianAge=line[2],\n                              totalRooms=line[3],\n                              totalBedRooms=line[4],\n                              population=line[5], \n                              households=line[6],\n                              medianIncome=line[7],\n                              medianHouseValue=line[8])).toDF()","language":"python"},"completion":"This code is written in Python and uses the PySpark library. \n\nThe first line imports the Row module from the PySpark SQL library. \n\nThe second line maps an RDD (Resilient Distributed Dataset) to a DataFrame (DF). The RDD is assumed to contain data related to housing, with each line of the RDD representing a single record. \n\nThe lambda function in the map() method takes each line of the RDD and creates a Row object with named fields corresponding to the different attributes of the housing data. The Row object is then converted to a DataFrame using the toDF() method. \n\nThe resulting DataFrame, df, will have columns named longitude, latitude, housingMedianAge, totalRooms, totalBedRooms, population, households, medianIncome, and medianHouseValue, with each row representing a single record of housing data."},{"id":1346895,"codeBlock":{"code":"# Show the top 20 rows \ndf.show()","language":""},"completion":"This code is written in Python and uses the PySpark library. \n\nThe `df.show()` function is used to display the top 20 rows of a DataFrame `df`. This is a useful function for quickly inspecting the contents of a DataFrame and checking if the data has been loaded correctly. \n\nIf the DataFrame has more than 20 rows, only the first 20 rows will be displayed. The output will be in a tabular format with columns and rows."},{"id":1346896,"codeBlock":{"code":"# Print the data types of all `df` columns\n# df.dtypes\n\n# Print the schema of `df`\ndf.printSchema()","language":"python"},"completion":"This code is written in Python and is used to print the data types of all columns in a DataFrame `df` and the schema of `df`. \n\nThe first line of code is commented out and is just a reminder of how to print the data types of all columns in `df`. To use it, you would need to uncomment it by removing the `#` at the beginning of the line.\n\nThe second line of code prints the schema of `df`. The schema of a DataFrame describes the structure of the data, including the column names, data types, and any nested structures. This can be useful for understanding the data and for debugging any issues with the DataFrame."},{"id":1346903,"codeBlock":{"code":"root\n |-- households: string (nullable = true)\n |-- housingMedianAge: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- medianHouseValue: string (nullable = true)\n |-- medianIncome: string (nullable = true)\n |-- population: string (nullable = true)\n |-- totalBedRooms: string (nullable = true)\n |-- totalRooms: string (nullable = true)","language":"python"},"completion":"This code is written in PySpark, which is a Python library for distributed computing. \n\nThis code snippet defines the schema for a PySpark DataFrame. The DataFrame will have columns named \"households\", \"housingMedianAge\", \"latitude\", \"longitude\", \"medianHouseValue\", \"medianIncome\", \"population\", \"totalBedRooms\", and \"totalRooms\". \n\nEach column is of type \"string\" and is nullable, meaning that it can contain null values. \n\nThis schema will be used to read in data and create a PySpark DataFrame with the specified column names and data types."},{"id":1346905,"codeBlock":{"code":"from pyspark.sql.types import *\n\ndf = df.withColumn(\"longitude\", df[\"longitude\"].cast(FloatType())) \\\n   .withColumn(\"latitude\", df[\"latitude\"].cast(FloatType())) \\\n   .withColumn(\"housingMedianAge\",df[\"housingMedianAge\"].cast(FloatType())) \\\n   .withColumn(\"totalRooms\", df[\"totalRooms\"].cast(FloatType())) \\ \n   .withColumn(\"totalBedRooms\", df[\"totalBedRooms\"].cast(FloatType())) \\ \n   .withColumn(\"population\", df[\"population\"].cast(FloatType())) \\ \n   .withColumn(\"households\", df[\"households\"].cast(FloatType())) \\ \n   .withColumn(\"medianIncome\", df[\"medianIncome\"].cast(FloatType())) \\ \n   .withColumn(\"medianHouseValue\", df[\"medianHouseValue\"].cast(FloatType()))","language":"python"},"completion":"This code is written in Python and uses the PySpark library. \n\nThe code imports the `FloatType` class from the `pyspark.sql.types` module. \n\nThe `df` variable is a DataFrame object that contains data on housing prices. \n\nThe code then uses the `withColumn()` method to cast each column in the DataFrame to a `FloatType`. This is done by chaining multiple `withColumn()` calls together. \n\nEach `withColumn()` call takes two arguments: the name of the new column and the casted version of the original column. \n\nFor example, the first `withColumn()` call creates a new column called \"longitude\" and casts the original \"longitude\" column to a `FloatType`. \n\nThis process is repeated for each column in the DataFrame. \n\nThe end result is a new DataFrame with all columns casted to `FloatType`."},{"id":1346888,"codeBlock":{"code":"# Import all from `sql.types`\nfrom pyspark.sql.types import *\n\n# Write a custom function to convert the data type of DataFrame columns\ndef convertColumn(df, names, newType):\n  for name in names: \n     df = df.withColumn(name, df[name].cast(newType))\n  return df \n\n# Assign all column names to `columns`\ncolumns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n\n# Conver the `df` columns to `FloatType()`\ndf = convertColumn(df, columns, FloatType())","language":"python"},"completion":"This code is written in Python and uses the PySpark library for working with big data. \n\nThe first line imports all the classes and functions from the `sql.types` module of PySpark. \n\nThe `convertColumn` function takes three arguments: `df` (a DataFrame), `names` (a list of column names), and `newType` (the new data type to which the columns should be converted). \n\nThe function loops through each column name in the `names` list and uses the `withColumn` method of the DataFrame to cast the column to the new data type specified by `newType`. The `withColumn` method returns a new DataFrame with the updated column data types. \n\nFinally, the `columns` list is defined with the names of all the columns in the DataFrame that need to be converted. The `df` DataFrame is then updated by calling the `convertColumn` function with the `columns` list and the `FloatType()` data type. This converts all the specified columns to floating-point numbers. The updated DataFrame is then assigned back to the `df` variable."},{"id":1346891,"codeBlock":{"code":"df.select('population','totalBedRooms').show(10)","language":""},"completion":"This code uses the `select()` method on a DataFrame `df` to select and display the columns 'population' and 'totalBedRooms'. The `show()` method is then used to display the first 10 rows of the selected columns. \n\nIn other words, this code is displaying the population and total number of bedrooms for the first 10 rows of the DataFrame."},{"id":1346872,"codeBlock":{"code":"+----------+-------------+\n|population|totalBedRooms|\n+----------+-------------+\n|     322.0|        129.0|\n|    2401.0|       1106.0|\n|     496.0|        190.0|\n|     558.0|        235.0|\n|     565.0|        280.0|\n|     413.0|        213.0|\n|    1094.0|        489.0|\n|    1157.0|        687.0|\n|    1206.0|        665.0|\n|    1551.0|        707.0|\n+----------+-------------+\nonly showing top 10 rows","language":"python"},"completion":"This code snippet is not actually code, but rather a display of a table of data. The table has two columns: \"population\" and \"totalBedRooms\". The first row shows that the population is 322 and the total number of bedrooms is 129. The table appears to be showing data related to housing. The \"+----------+-------------+\" and \"only showing top 10 rows\" indicate that this table is being displayed using a tabular data display library, such as Pandas or Spark DataFrames."},{"id":1346881,"codeBlock":{"code":"df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()","language":"python"},"completion":"This code is written in Python and it performs the following operations:\n\n1. `df.groupBy(\"housingMedianAge\")`: This groups the DataFrame `df` by the column \"housingMedianAge\".\n2. `.count()`: This counts the number of occurrences of each unique value in the \"housingMedianAge\" column.\n3. `.sort(\"housingMedianAge\",ascending=False)`: This sorts the resulting DataFrame by the \"housingMedianAge\" column in descending order.\n4. `.show()`: This displays the resulting DataFrame.\n\nOverall, this code groups the DataFrame by the \"housingMedianAge\" column, counts the number of occurrences of each unique value, sorts the resulting DataFrame by the \"housingMedianAge\" column in descending order, and displays the resulting DataFrame."},{"id":1346874,"codeBlock":{"code":"+----------------+-----+                                                        \n|housingMedianAge|count|\n+----------------+-----+\n|            52.0| 1273|\n|            51.0|   48|\n|            50.0|  136|\n|            49.0|  134|\n|            48.0|  177|\n|            47.0|  198|\n|            46.0|  245|\n|            45.0|  294|\n|            44.0|  356|\n|            43.0|  353|\n|            42.0|  368|\n|            41.0|  296|\n|            40.0|  304|\n|            39.0|  369|\n|            38.0|  394|\n|            37.0|  537|\n|            36.0|  862|\n|            35.0|  824|\n|            34.0|  689|\n|            33.0|  615|\n+----------------+-----+\nonly showing top 20 rows","language":"python"},"completion":"This code is displaying a table of two columns: \"housingMedianAge\" and \"count\". The table shows the number of instances of each value of \"housingMedianAge\" in the dataset. The plus signs and hyphens are used to format the table for display in a console or notebook. This code is likely using the PySpark library to work with large datasets."},{"id":1346886,"codeBlock":{"code":"df.describe().show()","language":"python"},"completion":"This code is written in Python and it uses the `describe()` method to generate summary statistics of a DataFrame `df`. The `show()` method is then used to display the summary statistics in a tabular format. \n\nThe `describe()` method computes the count, mean, standard deviation, minimum, and maximum values for each column in the DataFrame. This is useful for quickly understanding the distribution of data in a DataFrame. \n\nOverall, this code snippet is used to display summary statistics of a DataFrame in a tabular format."},{"id":1346908,"codeBlock":{"code":"# Import all from `sql.functions` \nfrom pyspark.sql.functions import *\n\n# Adjust the values of `medianHouseValue`\ndf = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n\n# Show the first 2 lines of `df`\ndf.take(2)","language":"python"},"completion":"This code is written in Python and uses the PySpark library for working with big data. \n\nThe first line imports all functions from the `sql.functions` module of PySpark. \n\nThe second line adjusts the values of the `medianHouseValue` column in the PySpark DataFrame `df`. It divides each value in the `medianHouseValue` column by 100000 and replaces the original values with the new values. This is done using the `withColumn()` method and the `col()` function from the `sql.functions` module. \n\nThe third line shows the first two rows of the updated DataFrame `df` using the `take()` method."},{"id":1346909,"codeBlock":{"code":"[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0), Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]","language":"python"},"completion":"This code creates a list of two `Row` objects. Each `Row` object represents a set of data for a specific location. The data includes information such as the number of households, the median age of housing, the latitude and longitude coordinates, the median house value, the median income, the population, the total number of bedrooms, and the total number of rooms. \n\nThe `Row` objects are created using the `Row()` constructor function, which takes in keyword arguments for each of the data attributes. The values for these attributes are provided in the form of floating-point numbers. \n\nOverall, this code is likely part of a larger data analysis or machine learning project, where the `Row` objects will be used to train a model or perform some other type of analysis."},{"id":1346880,"codeBlock":{"code":"# Import all from `sql.functions` if you haven't yet\nfrom pyspark.sql.functions import *\n\n# Divide `totalRooms` by `households`\nroomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n\n# Divide `population` by `households`\npopulationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n\n# Divide `totalBedRooms` by `totalRooms`\nbedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n\n# Add the new columns to `df`\ndf = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n   \n# Inspect the result\ndf.first()","language":"python"},"completion":"This code is written in Python and uses the PySpark library for working with big data. \n\nFirst, the code imports all functions from the `sql.functions` module using the `*` wildcard. \n\nNext, the code creates three new columns by dividing certain columns in the DataFrame `df`. The first column is `roomsPerHousehold`, which is created by dividing the `totalRooms` column by the `households` column. The second column is `populationPerHousehold`, which is created by dividing the `population` column by the `households` column. The third column is `bedroomsPerRoom`, which is created by dividing the `totalBedRooms` column by the `totalRooms` column. \n\nFinally, the code adds these new columns to the `df` DataFrame using the `withColumn()` method. This method takes two arguments: the name of the new column and the expression used to create the new column. The `col()` function is used to reference columns in the DataFrame. \n\nThe `df.first()` method is used to inspect the first row of the DataFrame with the new columns added."},{"id":1346912,"codeBlock":{"code":"Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)","language":"python"},"completion":"This code creates a single row of data for a housing dataset. The row contains information about a specific house, including the number of households, the median age of the housing in the area, the latitude and longitude of the house, the median house value, median income, population, total number of bedrooms and rooms, and calculated values such as rooms per household, population per household, and bedrooms per room. \n\nThe code uses the `Row` function to create a new row object with the specified values for each column. This row object can then be added to a larger dataset or used for analysis."},{"id":1346892,"codeBlock":{"code":"# Re-order and select columns\ndf = df.select(\"medianHouseValue\", \n              \"totalBedRooms\", \n              \"population\", \n              \"households\", \n              \"medianIncome\", \n              \"roomsPerHousehold\", \n              \"populationPerHousehold\", \n              \"bedroomsPerRoom\")","language":"python"},"completion":"This code reorders and selects specific columns from a DataFrame called `df`. The `select()` method is used to specify the columns to be selected and their order. The columns selected are \"medianHouseValue\", \"totalBedRooms\", \"population\", \"households\", \"medianIncome\", \"roomsPerHousehold\", \"populationPerHousehold\", and \"bedroomsPerRoom\". \n\nBy reordering and selecting specific columns, this code can be used to prepare the data for further analysis or modeling."},{"id":1346889,"codeBlock":{"code":"# Import `DenseVector`\nfrom pyspark.ml.linalg import DenseVector\n\n# Define the `input_data` \ninput_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n\n# Replace `df` with the new DataFrame\ndf = spark.createDataFrame(input_data, [\"label\", \"features\"])","language":"python"},"completion":"This code is written in Python and uses the PySpark library. \n\nFirst, the `DenseVector` class is imported from the `pyspark.ml.linalg` module. \n\nNext, the `input_data` variable is defined by mapping over the RDD (Resilient Distributed Dataset) of the `df` DataFrame. The lambda function takes each row of the DataFrame (`x`) and creates a tuple with the first element (`x[0]`) as the label and the rest of the elements (`x[1:]`) as a `DenseVector`. The `DenseVector` is created using the `DenseVector()` constructor and passing in the rest of the elements as a list. \n\nFinally, a new DataFrame is created using the `createDataFrame()` method of the `spark` object. The `input_data` variable is passed in as the first argument, and a list of column names (`[\"label\", \"features\"]`) is passed in as the second argument. The resulting DataFrame has two columns: \"label\" and \"features\"."},{"id":1346897,"codeBlock":{"code":"# Import `StandardScaler` \nfrom pyspark.ml.feature import StandardScaler\n\n# Initialize the `standardScaler`\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n\n# Fit the DataFrame to the scaler\nscaler = standardScaler.fit(df)\n\n# Transform the data in `df` with the scaler\nscaled_df = scaler.transform(df)\n\n# Inspect the result\nscaled_df.take(2)","language":"python"},"completion":"This code uses the PySpark library to perform feature scaling on a DataFrame. \n\nFirst, the `StandardScaler` class is imported from the `pyspark.ml.feature` module. \n\nThen, an instance of the `StandardScaler` class is initialized with the input column set to \"features\" and the output column set to \"features_scaled\". \n\nNext, the `fit()` method is called on the `standardScaler` object with the DataFrame `df` as the argument. This fits the scaler to the data in `df`. \n\nAfter that, the `transform()` method is called on the `scaler` object with `df` as the argument. This applies the scaling transformation to the data in `df` and returns a new DataFrame called `scaled_df`. \n\nFinally, the `take()` method is called on `scaled_df` to inspect the first two rows of the transformed data."},{"id":1346910,"codeBlock":{"code":"[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])), Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]","language":"python"},"completion":"This code snippet is a list of two Spark SQL Row objects. Each Row object has three fields: label, features, and features_scaled. \n\nThe label field is a float value that represents the target variable for a machine learning model. \n\nThe features field is a DenseVector object that contains a set of features for the corresponding label. In this case, there are seven features represented as floats: [129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466] for the first Row object and [1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558] for the second Row object. \n\nThe features_scaled field is also a DenseVector object that contains the scaled version of the features field. The scaling is performed using the StandardScaler function in Spark MLlib. \n\nOverall, this code snippet represents a dataset with two labeled examples and their corresponding feature vectors."},{"id":1346887,"codeBlock":{"code":"# Split the data into train and test sets\ntrain_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)","language":"python"},"completion":"This code splits the data in `scaled_df` into two sets: `train_data` and `test_data`. The split is done randomly using the `randomSplit()` method, which takes a list of percentages as its argument. In this case, the list `[.8, .2]` specifies that 80% of the data should be used for training and 20% for testing. The `seed` parameter is used to ensure that the random split is reproducible. The resulting `train_data` and `test_data` sets can be used for training and evaluating a machine learning model, respectively."},{"id":1346879,"codeBlock":{"code":"# Import `LinearRegression`\nfrom pyspark.ml.regression import LinearRegression\n\n# Initialize `lr`\nlr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the data to the model\nlinearModel = lr.fit(train_data)","language":"python"},"completion":"This code is written in Python and uses the PySpark library for machine learning. \n\nFirst, the code imports the `LinearRegression` class from the `pyspark.ml.regression` module. \n\nNext, an instance of the `LinearRegression` class is created and assigned to the variable `lr`. The `labelCol` parameter is set to \"label\", which indicates the name of the label column in the dataset. The `maxIter` parameter is set to 10, which specifies the maximum number of iterations for the algorithm to converge. The `regParam` parameter is set to 0.3, which controls the amount of regularization applied to the model. The `elasticNetParam` parameter is set to 0.8, which determines the balance between L1 and L2 regularization. \n\nFinally, the `fit()` method is called on the `lr` object with the `train_data` dataset as the argument. This trains the linear regression model on the training data and returns a `LinearRegressionModel` object, which is assigned to the variable `linearModel`."},{"id":1346893,"codeBlock":{"code":"# Generate predictions\npredicted = linearModel.transform(test_data)\n\n# Extract the predictions and the \"known\" correct labels\npredictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\nlabels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n\n# Zip `predictions` and `labels` into a list\npredictionAndLabel = predictions.zip(labels).collect()\n\n# Print out first 5 instances of `predictionAndLabel` \npredictionAndLabel[:5]","language":"python"},"completion":"This code is used to generate predictions and compare them with the correct labels. \n\nFirst, the `linearModel` is used to transform the `test_data` and generate the `predicted` values. \n\nThen, the `predictions` and `labels` are extracted from the `predicted` data using the `select` method and `rdd` is used to convert the data into an RDD (Resilient Distributed Dataset). The `map` function is used to extract the first element of each row, which corresponds to the prediction or label value.\n\nNext, the `zip` function is used to combine the `predictions` and `labels` into a list of tuples called `predictionAndLabel`. \n\nFinally, the first 5 instances of `predictionAndLabel` are printed using slicing. \n\nOverall, this code is used to evaluate the performance of a linear regression model by comparing its predicted values with the actual labels."},{"id":1346884,"codeBlock":{"code":"[(1.4491508524918457, 0.14999), (1.5705029404692372, 0.14999), (2.148727956912464, 0.14999), (1.5831547768979277, 0.344), (1.5182107797955968, 0.398)]","language":"python"},"completion":"This code is a Python list containing tuples. Each tuple contains two floating-point numbers. The first number in each tuple represents a value of some measurement, and the second number represents the uncertainty or error associated with that measurement. The values and uncertainties in the list are arbitrary and could represent any type of measurement or calculation."},{"id":1346883,"codeBlock":{"code":"# Coefficients for the model\nlinearModel.coefficients\n\n# Intercept for the model\nlinearModel.intercept","language":"python"},"completion":"This code is written in Python and is used to access the coefficients and intercept of a linear regression model. \n\nThe first line `linearModel.coefficients` accesses the coefficients of the linear regression model stored in the variable `linearModel`. These coefficients represent the weights assigned to each feature in the model and are used to make predictions.\n\nThe second line `linearModel.intercept` accesses the intercept of the linear regression model stored in the variable `linearModel`. The intercept represents the value of the dependent variable when all independent variables are equal to zero. It is added to the weighted sum of the independent variables to produce the predicted value of the dependent variable."},{"id":1346901,"codeBlock":{"code":"# The coefficients\n[0.0,0.0,0.0,0.276239709215,0.0,0.0,0.0]\n\n# The intercept\n0.990399577462","language":"python"},"completion":"This code snippet does not contain any executable code. It simply displays the values of the coefficients and intercept of a model. Without additional context or information, it is difficult to determine what type of model these values belong to or how they were obtained."},{"id":1346899,"codeBlock":{"code":"# Get the RMSE\nlinearModel.summary.rootMeanSquaredError\n\n# Get the R2\nlinearModel.summary.r2","language":"python"},"completion":"This code is written in Python and is used to obtain the RMSE (Root Mean Squared Error) and R2 (R-squared) values from a linear regression model. \n\nThe first line of code retrieves the RMSE value from the summary of the linear regression model, which is stored in the `linearModel` variable. The `summary` attribute of the `linearModel` object contains various statistics and information about the model, including the RMSE.\n\nThe second line of code retrieves the R2 value from the same summary object. R2 is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in a regression model. \n\nOverall, these two lines of code allow the user to quickly obtain important metrics for evaluating the performance of a linear regression model."},{"id":1346898,"codeBlock":{"code":"# RMSE\n0.8692118678997669\n\n# R2\n0.4240895287218379\n","language":"python"},"completion":"This code snippet does not contain any actual code. It appears to be the output of two evaluation metrics, RMSE and R2, which have been printed to the console. RMSE stands for Root Mean Squared Error, which is a measure of the difference between predicted and actual values in a regression problem. R2, or R-squared, is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. The values printed represent the calculated RMSE and R2 scores for a particular model."},{"id":1346913,"codeBlock":{"code":"spark.stop()","language":""},"completion":"This code snippet is written in Apache Spark, which is a distributed computing framework used for big data processing. \n\nThe `spark.stop()` function is used to stop the SparkContext, which is the entry point to any Spark functionality. It shuts down all the Spark executors and releases the resources used by Spark. \n\nIn simpler terms, this code stops the Spark application and frees up the resources used by it."}],"title":"Apache Spark Tutorial: ML with PySpark","slug":"apache-spark-tutorial-machine-learning","publishedAt":"2022-08-23T12:00:28.316Z","publishDate":"2017-07-28T00:00:00.000Z","dateModified":null,"completionTime":"34","shortDescription":"Apache Spark tutorial introduces you to big data processing, analysis and ML with PySpark.","content":"","type":null,"breadcrumbs":null,"authors":[{"fullName":"Karlijn Willems","profileSlug":"karlijn","shortBio":"Former Data Journalist at DataCamp | Manager at NextWave Consulting","facebookProfileUrl":null,"linkedInProfileUrl":"https://www.linkedin.com/in/karlijnwillems/","twitterProfileUrl":null,"githubProfileUrl":null,"mediumProfileUrl":"https://medium.com/@kacawi","authorAvatar":{"url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1658157569/karlijn_5fd8178e25.png"}}],"category":{"tag":"Python","slug":"python","shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our blog is full of practical articles \u0026 use cases you can use to upskill.","badgeColor":{"color":null}},"contentImage":null,"subCategories":[{"tag":"Data Science","slug":"data-science","shortDescription":"Read advice, learn new skills, \u0026 advance your data career in our data science blog. We talk about programming languages, industry news, \u0026 much more.","badgeColor":{"color":null}},{"tag":"Python","slug":"python","shortDescription":"Keep up to date with the latest news, techniques, and resources for Python programming. Our blog is full of practical articles \u0026 use cases you can use to upskill.","badgeColor":{"color":null}},{"tag":"Machine Learning","slug":"machine-learning","shortDescription":"Get insights \u0026 best practices into AI \u0026 machine learning to drive data transformation, upskill, and build data cultures. Discover how you can use ML in your work.","badgeColor":{"color":null}}],"seo":[{"metaTitle":"Apache Spark Tutorial: Machine Learning","metaDescription":"Apache Spark tutorial introduces you to big data processing, analysis and Machine Learning (ML) with PySpark.","preventIndexing":null,"metaImage":null,"meta":[]}],"relatedPodcasts":[],"relatedBlogs":[],"relatedCheatsheets":[{"slug":"pyspark-cheat-sheet-spark-dataframes-in-python","seo":[{"metaImage":{"alternativeText":"","url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1649269389/Py_Spark_Cheat_Sheet_Spark_Data_Frames_in_Python_thv8v2_2e3293ea0c.webp"}}],"title":"PySpark Cheat Sheet: Spark DataFrames in Python","shortDescription":"This PySpark SQL cheat sheet is your handy companion to Apache Spark DataFrames in Python and includes code samples.","readTime":"5","authors":[{"fullName":"Karlijn Willems","profileSlug":"karlijn","authorAvatar":{"url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1658157569/karlijn_5fd8178e25.png"}}]},{"slug":"pyspark-cheat-sheet-spark-in-python","seo":[{"metaImage":{"alternativeText":"","url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1649266641/Py_Spark_Cheat_Sheet_Spark_in_Python_owornh_b06f2ca6ae.webp"}}],"title":"PySpark Cheat Sheet: Spark in Python ","shortDescription":"This PySpark cheat sheet with code samples covers the basics like initializing Spark in Python, loading data, sorting, and repartitioning.","readTime":"6","authors":[{"fullName":"Karlijn Willems","profileSlug":"karlijn","authorAvatar":{"url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1658157569/karlijn_5fd8178e25.png"}}]}],"relatedTutorials":[{"slug":"pyspark-tutorial-getting-started-with-pyspark","seo":[{"metaImage":null}],"title":"Pyspark Tutorial: Getting Started with Pyspark ","shortDescription":"Discover what Pyspark is and how it can be used while giving examples.","readTime":"10","authors":[{"fullName":"Natassha Selvaraj","profileSlug":"natassha6789","authorAvatar":{"url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1665761694/nattasha_167ec05e37.png"}}]},{"slug":"installation-of-pyspark","seo":[{"metaImage":null}],"title":"Installation of PySpark (All operating systems)","shortDescription":"This tutorial will demonstrate the installation of PySpark and hot to manage the environment variables in Windows, Linux, and Mac Operating System.","readTime":"8","authors":[{"fullName":"Olivia Smith","profileSlug":"oliviasmithdatsci","authorAvatar":null}]},{"slug":"snowpark-snoflake-introduction","seo":[{"metaImage":{"alternativeText":null,"url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1715358878/datarhys_an_absurdit_oil_painting_of_a_coder_using_their_laptop_7d368333_7b7a_48f5_a579_1531753d385c_afc6ec3be0.png"}}],"title":"Snowflake Snowpark: A Comprehensive Introduction","shortDescription":"Take the first steps to master in-database machine learning using Snowflake Snowpark.","readTime":"19","authors":[{"fullName":"Bex Tuychiev","profileSlug":"bexgboost","authorAvatar":{"url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1686304521/Bex_Tuychiev_1fa6cc0c26.jpg"}}]},{"slug":"machine-learning-python","seo":[{"metaImage":null}],"title":"Python Machine Learning: Scikit-Learn Tutorial","shortDescription":"An easy-to-follow scikit-learn tutorial that will help you get started with Python machine learning. ","readTime":"12","authors":[{"fullName":"Kurtis Pykes ","profileSlug":"kurtispykes","authorAvatar":{"url":"https://res.cloudinary.com/dyd911kmh/image/upload/v1658156357/Kurtis_e60df9583d.jpg"}}]}],"relatedCodeAlongs":[],"body":[{"__typename":"ComponentPageContentTableOfContents","headingsToAddDepth":"h3"},{"__typename":"ComponentPageContentRichText","text":"\u003ch2\u003eApache Spark and Python for Big Data and Machine Learning\u003c/h2\u003e\n\u003cp\u003eApache Spark is known as a fast, easy-to-use and general engine for big data processing that has built-in modules for streaming, SQL, Machine Learning (ML) and graph processing. This technology is an in-demand skill for data engineers, but also data scientists can benefit from learning Spark when doing Exploratory Data Analysis (EDA), feature extraction and, of course, ML.\u003c/p\u003e\n\u003cp\u003eIn this tutorial, you\u0026rsquo;ll interface Spark with Python through PySpark, the Spark Python API that exposes the Spark programming model to Python. More concretely, you\u0026rsquo;ll focus on:\u003c/p\u003e\n\u003cnav\u003e\n\u003cul\u003e\n\u003cli\u003eInstalling PySpark locally on your personal computer and setting it up so that you can work with the interactive Spark shell to do some quick, interactive analyses on your data. You\u0026rsquo;ll see how to do this with \u003ccode\u003epip\u003c/code\u003e, Homebrew and via the Spark download page.\u003c/li\u003e\n\u003cli\u003eLearning how to work with the basics of Spark: you\u0026rsquo;ll see how you can create RDDs and perform basic operations on them.\u003c/li\u003e\n\u003cli\u003eGetting started with PySpark in Jupyter Notebook and loading in a real-life data set.\u003c/li\u003e\n\u003cli\u003eExploring and preprocessing the data that you loaded in at the first step the help of DataFrames, which demands that you make use of Spark SQL, which allows you to query structured data inside Spark programs.\u003c/li\u003e\n\u003cli\u003eCreating a Linear Regression model with Spark ML to feed the data to it, after which you\u0026rsquo;ll be able to make predictions. And, lastly,\u003c/li\u003e\n\u003cli\u003eEvaluating the machine learning model that you made.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/nav\u003e\n\u003cdiv id=\"scoped-content\"\u003e\n\u003cp\u003e\u003cimg style=\"width: 400px; height: 261px;\" src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Spark+ML/content_macro-1452986_960_720.jpg\" alt=\"Apache Spark Tutorial\" /\u003e\u003c/p\u003e\n\u003cp\u003eIf you're rather interested in using Spark with R, you should check out DataCamp\u0026rsquo;s free \u003ca href=\"https://www.datacamp.com/courses/introduction-to-spark-in-r-using-sparklyr\"\u003eIntroduction to Spark in R with sparklyr\u003c/a\u003e or download the \u003ca href=\"https://www.datacamp.com/cheat-sheet/pyspark-cheat-sheet-spark-dataframes-in-python\"\u003ePySpark SQL\u003c/a\u003e cheat sheet.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"install\"\u003eInstalling Apache Spark\u003c/h2\u003e\n\u003cp\u003eInstalling Spark and getting it to work can be a challenge. In this section, you\u0026rsquo;ll cover some steps that will show you how to get it installed on your pc.\u003c/p\u003e\n\u003cp\u003eFirst thing that you want to do is checking whether you meet the prerequisites. Spark is written in Scala Programming Language and runs on Java Virtual Machine (JVM) environment. That\u0026rsquo;s why you need to check if you have a \u003ca href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\"\u003eJava Development Kit (JDK)\u003c/a\u003e installed. You do this because the JDK will provide you with one or more implementations of the JVM. Preferably, you want to pick the latest one, which, at the time of writing is the JDK8.\u003c/p\u003e\n\u003cp\u003eNext, you\u0026rsquo;re reading to download Spark!\u003c/p\u003e\n\u003cdiv id=\"downloading-pyspark-with-pip\" class=\"section level3\"\u003e\n\u003ch3\u003eDownloading \u003ccode\u003epyspark\u003c/code\u003e with \u003ccode\u003epip\u003c/code\u003e\u003c/h3\u003e\n\u003cp\u003eThen, you can download and install PySpark it with the help of \u003ccode\u003epip\u003c/code\u003e. This is fairly easy and much like installing any other package. You just run the usual command and the heavy lifting gets done for you:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ pip install pyspark\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively, you can also go to the \u003ca href=\"https://spark.apache.org/downloads.html\"\u003eSpark download page\u003c/a\u003e. Keep the default options in the first three steps and you\u0026rsquo;ll find a downloadable link in step 4. Click on that link to download it. For this tutorial, you\u0026rsquo;ll download the 2.2.0 Spark Release and the \u0026ldquo;Pre-built for Apache Hadoop 2.7 and later\u0026rdquo; package type.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e that the download can take some time to finish!\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv id=\"downloading-spark-with-homebrew\" class=\"section level3\"\u003e\n\u003ch3\u003eDownloading Spark with Homebrew\u003c/h3\u003e\n\u003cp\u003eYou can also install Spark with the \u003ca href=\"https://brew.sh\"\u003eHomebrew\u003c/a\u003e, a free and open-source package manager. This is especially handy if you\u0026rsquo;re working with macOS.\u003c/p\u003e\n\u003cp\u003eSimply run the following commands to search for Spark, to get more information and to finally install it on your personal computer:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e# Search for spark\n$ brew search spark\n\n# Get more information on apache-spark\n$ brew info apache-spark\n\n# Install apache-spark\n$ brew install apache-spark\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e\n\u003cdiv id=\"download-and-set-up-spark\" class=\"section level3\"\u003e\n\u003ch3\u003eDownload and Set Up Spark\u003c/h3\u003e\n\u003cp\u003eNext, make sure that you untar the directory that appears in your \u003ccode\u003eDownloads\u003c/code\u003e folder. This can happen automatically for you, by double clicking the \u003ccode\u003espark-2.2.0-bin-hadoop2.7.tgz\u003c/code\u003e archive or by opening up your Terminal and running the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e$ tar xvf spark-2.2.0-bin-hadoop2.7.tgz\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, move the untarred folder to \u003ccode\u003e/usr/local/spark\u003c/code\u003e by running the following line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e$ mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e that if you get an error that says that the permission is denied to move this folder to the new location, you should add \u003ccode\u003esudo\u003c/code\u003e in front of this command. The line above will then become \u003ccode\u003e$ sudo mv spark-2.1.0-bin-hadoop2.7 /usr/local/spark\u003c/code\u003e. You\u0026rsquo;ll be prompted to give your password, which is usually the one that you also use to unlock your pc when you start it up :)\u003c/p\u003e\n\u003cp\u003eNow that you\u0026rsquo;re all set to go, open the README file in the file path \u003ccode\u003e/usr/local/spark\u003c/code\u003e. You can do this by executing\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e$ cd /usr/local/spark\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will brings you to the folder that you need to be. Then, you can start inspecting the folder and reading the README file that is incuded in it.\u003c/p\u003e\n\u003cp\u003eFirst, use \u003ccode\u003e$ ls\u003c/code\u003e to get a list of the files and folders that are in this \u003ccode\u003espark\u003c/code\u003e folder. You\u0026rsquo;ll see that there\u0026rsquo;s a README.md file in there. You can open it by executing one of the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e# Open and edit the file\n$ nano README.md\n\n# Just read the file \n$ cat README.md\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e use the tab button on your keyboard to autocomplete as you\u0026rsquo;re typing the file name :) This will save you some time.\u003c/p\u003e\n\u003cp\u003eYou\u0026rsquo;ll see that this README provides you with some general information about Spark, online documentation, building Spark, the Interactive Scala and Python shells, example programs and much more.\u003c/p\u003e\n\u003cp\u003eThe thing that could interest you most here is the section on how to build Spark but note that this will only be particularly relevant if you haven\u0026rsquo;t downloaded a pre-built version. For this tutorial, however, you downloaded a pre-built version. You can press CTRL + X to exit the README, which brings you back to the spark folder.\u003c/p\u003e\n\u003cp\u003eIn case you selected a version that hasn\u0026rsquo;t been built yet, make sure you run the command that is listed in the README file. At the time of writing, this is the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e$ build/mvn -DskipTests clean package run\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that this command can take a while to run.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"basics\"\u003ePySpark Basics: RDDs\u003c/h2\u003e\n\u003cp\u003eNow that you\u0026rsquo;ve successfully installed Spark and PySpark, let\u0026rsquo;s first start off by exploring the interactive Spark Shell and by nailing down some of the basics that you will need when you want to get started. In the rest of this tutorial, however, you\u0026rsquo;ll work with PySpark in a Jupyter notebook.\u003c/p\u003e\n\u003cdiv id=\"spark-applications-versus-spark-shell\" class=\"section level3\"\u003e\n\u003ch3\u003eSpark Applications Versus Spark Shell\u003c/h3\u003e\n\u003cp\u003eThe interactive shell is an example of a Read-Eval(uate)-Print-Loop (REPL) environment; That means that whatever you type in is read, evaluated and printed out to you so that you can continue your analysis. This might remind you of IPython, which is a powerful interactive Python shell that you might know from working with Jupyter. If you want to know more, consider reading DataCamp\u0026rsquo;s \u003ca href=\"https://www.datacamp.com/blog/ipython-or-jupyter\"\u003eIPython or Jupyter\u003c/a\u003e blog post.\u003c/p\u003e\n\u003cp\u003eThis means that you can use the shell, which is available for Python as well as Scala, for all interactive work that you need to do.\u003c/p\u003e\n\u003cp\u003eBesides this shell, you can also write and deploy Spark applications. In contrast to writing Spark applications, the SparkSession has already been created for you so that you can just start working and not waste valuable time on creating one.\u003c/p\u003e\n\u003cp\u003eNow you might wonder: what is the SparkSession?\u003c/p\u003e\n\u003cp\u003eWell, it\u0026rsquo;s the main entry point for Spark functionality: it represents the connection to a Spark cluster and you can use it to create RDDs and to broadcast variables on that cluster. When you\u0026rsquo;re working with Spark, everything starts and ends with this SparkSession. \u003cstrong\u003eNote\u003c/strong\u003e that before Spark 2.0.0, the three main connection objects were SparkContext, SqlContext and HiveContext.\u003c/p\u003e\n\u003cp\u003eYou\u0026rsquo;ll see more on this later on. For now, let\u0026rsquo;s just focus on the shell.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv id=\"the-python-spark-shell\" class=\"section level3\"\u003e\n\u003ch3\u003eThe Python Spark Shell\u003c/h3\u003e\n\u003cp\u003eFrom within the \u003ccode\u003espark\u003c/code\u003e folder located at \u003ccode\u003e/usr/local/spark\u003c/code\u003e, you can run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ./bin/pyspark\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt first, you\u0026rsquo;ll see some text appearing. And then, you\u0026rsquo;ll see \u0026ldquo;Spark\u0026rdquo; appearing, just like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003ePython 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016, 12:39:47) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n17/07/26 11:41:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/07/26 11:41:47 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.0\n      /_/\n\nUsing Python version 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016 12:39:47)\nSparkSession available as 'spark'.\n\u0026gt;\u0026gt;\u0026gt;\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen you see this, you know that you\u0026rsquo;re ready to start experimenting within the interactive shell!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: if you prefer using the IPython shell instead of the Spark shell, you can do this by setting the following environment variable:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYSPARK_DRIVER_PYTHON=\"/usr/local/ipython/bin/ipython\"\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv id=\"creating-rdds\" class=\"section level3\"\u003e\n\u003ch3\u003eCreating RDDs\u003c/h3\u003e\n\u003cp\u003eNow, let\u0026rsquo;s start small and make an RDD, which is the most basic building block of Spark. An RDD simply represents data but it\u0026rsquo;s not one object, a collection of records, a result set or a data set. That is because it\u0026rsquo;s intended for data that resides on multiple computers: a single RDD could be spread over thousands of Java Virtual Machines (JVMs), because Spark automatically partitions the data under the hood to get this parallelism. Of course, you can adjust the parallelism to get more partitions. That\u0026rsquo;s why an RDD is actually a collection of partitions.\u003c/p\u003e\n\u003cp\u003eYou can easily create a simple RDD by using the \u003ccode\u003eparallelize()\u003c/code\u003e function and by simply passing some data (an iterable, like a list, or a collection) to it:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e\u0026gt;\u0026gt;\u0026gt; rdd1 = spark.sparkContext.parallelize([('a',7),('a',2),('b',2)])\n\u0026gt;\u0026gt;\u0026gt; rdd2 = spark.sparkContext.parallelize([(\"a\",[\"x\",\"y\",\"z\"]), (\"b\",[\"p\", \"r\"])])\n\u0026gt;\u0026gt;\u0026gt; rdd3 = spark.sparkContext.parallelize(range(100))\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e that the SparkSession object has the SparkContext object, which you can access with \u003ccode\u003espark.sparkContext\u003c/code\u003e. For backwards compatibility reasons, it\u0026rsquo;s also still possible to call the SparkContext with \u003ccode\u003esc\u003c/code\u003e, as in \u003ccode\u003erdd1 = sc.parallelize(['a',7),('a',2),('b',2)])\u003c/code\u003e.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv id=\"rdd-operations\" class=\"section level3\"\u003e\n\u003ch3\u003eRDD Operations\u003c/h3\u003e\n\u003cp\u003eNow that you have created the RDDs, you can use the distributed data in \u003ccode\u003erdd1\u003c/code\u003e and \u003ccode\u003erdd2\u003c/code\u003e to operate on in parallel. You have two types of operations: transformations and actions.\u003c/p\u003e\n\u003cp\u003eNow, to intuitively get the difference between these two, consider some of the most common transformations are \u003ccode\u003emap()\u003c/code\u003e, \u003ccode\u003efilter()\u003c/code\u003e, \u003ccode\u003eflatMap()\u003c/code\u003e, \u003ccode\u003esample()\u003c/code\u003e, \u003ccode\u003erandomSplit()\u003c/code\u003e, \u003ccode\u003ecoalesce()\u003c/code\u003e and \u003ccode\u003erepartition()\u003c/code\u003e and some of the most common actions are \u003ccode\u003ereduce()\u003c/code\u003e, \u003ccode\u003ecollect()\u003c/code\u003e, \u003ccode\u003efirst()\u003c/code\u003e, \u003ccode\u003etake()\u003c/code\u003e, \u003ccode\u003ecount()\u003c/code\u003e, \u003ccode\u003esaveAsHadoopFile()\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTransformations are lazy operations on a RDD that create one or many new RDDs, while actions produce non-RDD values: they return a result set, a number, a file, \u0026hellip;\u003c/p\u003e\n\u003cp\u003eYou can, for example, aggregate all the elements of \u003ccode\u003erdd1\u003c/code\u003e using the following, simple lambda function and return the results to the driver program:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e\u0026gt;\u0026gt;\u0026gt; rdd1.reduce(lambda a,b: a+b)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExecuting this line of code will give you the following result: \u003ccode\u003e('a', 7, 'a', 2, 'b', 2)\u003c/code\u003e. Another example of a transformation is \u003ccode\u003eflatMapValues()\u003c/code\u003e, which you run on key-value pair RDDs, such as \u003ccode\u003erdd2\u003c/code\u003e. In this case, you pass each value in the key-value pair RDD \u003ccode\u003erdd2\u003c/code\u003e through a flatMap function without changing the keys, which is the lambda function defined below and you perform an action after that by collecting hte results with \u003ccode\u003ecollect()\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e\u0026gt;\u0026gt;\u0026gt; rdd2.flatMapValues(lambda x: x).collect()\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2 id=\"load\"\u003eThe Data\u003c/h2\u003e\n\u003cp\u003eNow that you have covered some basics with the interactive shell, it\u0026rsquo;s time to get started with some real data. For this tutorial, you\u0026rsquo;ll make use of the \u003ca href=\"http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\"\u003eCalifornia Housing\u003c/a\u003e data set. Note, of course, that this is actually \u0026lsquo;small\u0026rsquo; data and that using Spark in this context might be overkill; This tutorial is for educational purposes only and is meant to give you an idea of how you can use PySpark to build a machine learning model.\u003c/p\u003e\n\u003ch2\u003eLoading and Exploring your Data\u003c/h2\u003e\n\u003cp\u003eEven though you know a bit more about your data, you should take the time to go ahead and explore it more thoroughly; Before you do this, however, you will set up your Jupyter Notebook with Spark and you\u0026rsquo;ll take some first steps to defining the SparkContext.\u003c/p\u003e\n\u003cdiv id=\"pyspark-in-jupyter-notebook\" class=\"section level3\"\u003e\n\u003ch3\u003ePySpark in Jupyter Notebook\u003c/h3\u003e\n\u003cp\u003eFor this part of the tutorial, you won\u0026rsquo;t use the ishell but you\u0026rsquo;ll build your own application. You\u0026rsquo;ll do this in a Jupyter Notebook. You already have all the things that you need installed, so you don\u0026rsquo;t need to do much to get PySpark to work in Jupyter.\u003c/p\u003e\n\u003cp\u003eYou can just launch the notebook application the same way like you always do, by running \u003ccode\u003e$ jupyter notebook\u003c/code\u003e. Then, you make a new notebook and you simply import the \u003ccode\u003efindspark\u003c/code\u003e library and use the \u003ccode\u003einit()\u003c/code\u003e function. In this case, you\u0026rsquo;re going to supply the path \u003ccode\u003e/usr/local/spark\u003c/code\u003e to \u003ccode\u003einit()\u003c/code\u003e because you\u0026rsquo;re certain that this is the path where you installed Spark.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import findspark \nimport findspark\n\n# Initialize and provide path\nfindspark.init(\"/usr/local/spark\")\n\n# Or use this alternative\n#findspark.init()\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: if you have no idea whether your path is set correctly or where you have installed Spark on your pc, you can always use \u003ccode\u003efindspark.find()\u003c/code\u003e to automatically detect the location of where Spark is installed.\u003c/p\u003e\n\u003cp\u003eIf you\u0026rsquo;re looking for alternative ways to work with Spark in Jupyter, consult our \u003ca href=\"https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark\"\u003eApache Spark in Python: Beginner\u0026rsquo;s Guide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNow that you have got all of that settled, you can finally start by creating your first Spark program!\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv id=\"creating-your-first-spark-program\" class=\"section level3\"\u003e\n\u003ch3\u003eCreating your First Spark Program\u003c/h3\u003e\n\u003cp\u003eWhat you first want to be doing is importing the \u003ccode\u003eSparkContext\u003c/code\u003e from the \u003ccode\u003epyspark\u003c/code\u003e package and initializing it. \u003cstrong\u003eRemember\u003c/strong\u003e that you didn\u0026rsquo;t have to do this before because the interactive Spark shell automatically created and initialized it for you! Here, you\u0026rsquo;ll need to do a little bit more work yourself :)\u003c/p\u003e\n\u003cp\u003eImport the \u003ccode\u003eSparkSession\u003c/code\u003e module from \u003ccode\u003epyspark.sql\u003c/code\u003e and build a SparkSession with the \u003ccode\u003ebuilder()\u003c/code\u003e method. Afterwards, you can set the master URL to connect to, the application name, add some additional configuration like the executor memory and then lastly, use \u003ccode\u003egetOrCreate()\u003c/code\u003e to either get the current Spark session or to create one if there is none running.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Build the SparkSession\nspark = SparkSession.builder \\\n   .master(\"local\") \\\n   .appName(\"Linear Regression Model\") \\\n   .config(\"spark.executor.memory\", \"1gb\") \\\n   .getOrCreate()\n   \nsc = spark.sparkContext\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e that if you get an error where there\u0026rsquo;s a FileNotFoundError similar to this one: \u0026ldquo;No such file or directory: \u0026lsquo;/User/YourName/Downloads/spark-2.1.0-bin-hadoop2.7/./bin/spark-submit\u0026rsquo;\u0026rdquo;, you know that you have to (re)set your Spark PATH. Go to your home directory by executing \u003ccode\u003e$ cd\u003c/code\u003e and then edit the \u003ccode\u003e.bash_profile\u003c/code\u003e file by running \u003ccode\u003e$ nano .bash_profile\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAdd something like the following to the bottom of the file\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003eexport SPARK_HOME=\"/usr/local/spark\"\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUse CTRL + X to exit the file but make sure to save your adjustments by also entering \u003cem\u003eY\u003c/em\u003e to confirm the changes. Next, don\u0026rsquo;t forget to set the changes in motion by running \u003ccode\u003esource .bash_profile\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: you can also set additional environment variables if you want; You probably don\u0026rsquo;t need them, but it\u0026rsquo;s definitely good to know that you can set them if desired. Consider the following examples:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-bash\"\u003e# Set a fixed value for the hash seed secret\nexport PYTHONHASHSEED=0\n\n# Set an alternate Python executable\nexport PYSPARK_PYTHON=/usr/local/ipython/bin/ipython\n\n# Augment the default search path for shared libraries\nexport LD_LIBRARY_PATH=/usr/local/ipython/bin/ipython\n\n# Augment the default search path for private libraries \nexport PYTHONPATH=$SPARK_HOME/python/lib/py4j-*-src.zip:$PYTHONPATH:$SPARK_HOME/python/\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e also that now you have initialized a default SparkSession. However, in most cases, you\u0026rsquo;ll want to configure this further. You\u0026rsquo;ll see that this will be really needed when you start working with big data. If you want to know more about it, check \u003ca href=\"https://spark.apache.org/docs/latest/configuration.html\"\u003ethis page\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv id=\"loading-in-your-data\" class=\"section level3\"\u003e\n\u003ch3\u003eLoading in your Data\u003c/h3\u003e\n\u003cp\u003eThis tutorial makes use of the California Housing data set. It appeared in a 1997 paper titled \u003cem\u003eSparse Spatial Autoregressions\u003c/em\u003e, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\u003c/p\u003e\n\u003cp\u003eThe data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. You\u0026rsquo;ll gather this information from \u003ca href=\"http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\"\u003ethis web page\u003c/a\u003e or by reading the paper which was mentioned above and which you can find \u003ca href=\"http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/html/ms_sp_lets1.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThese spatial data contain 20,640 observations on housing prices with 9 economic variables:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eLongitude\u003c/em\u003e refers to the angular distance of a geographic place north or south of the earth\u0026rsquo;s equator for each block group;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLatitude\u003c/em\u003e refers to the angular distance of a geographic place east or west of the earth\u0026rsquo;s equator for each block group;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eHousing median age\u003c/em\u003e is the median age of the people that belong to a block group. \u003cstrong\u003eNote\u003c/strong\u003e that the median is the value that lies at the midpoint of a frequency distribution of observed values;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eTotal rooms\u003c/em\u003e is the total number of rooms in the houses per block group;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eTotal bedrooms\u003c/em\u003e is the total number of bedrooms in the houses per block group;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003ePopulation\u003c/em\u003e is the number of inhabitants of a block group;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eHouseholds\u003c/em\u003e refers to units of houses and their occupants per block group;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eMedian income\u003c/em\u003e is used to register the median income of people that belong to a block group; And,\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eMedian house value\u003c/em\u003e is the dependent variable and refers to the median house value per block group.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhat\u0026rsquo;s more, you also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003eMedian house value\u003c/em\u003e is the dependent variable and will be assigned the role of the target variable in your ML model.\u003c/p\u003e\n\u003cp\u003eYou can download the data \u003ca href=\"http://lib.stat.cmu.edu/datasets\"\u003ehere\u003c/a\u003e. Look for the \u003cem\u003ehouses.zip\u003c/em\u003e folder, download and untar it so that you can access the data folders.\u003c/p\u003e\n\u003cp\u003eNext, you\u0026rsquo;ll use the \u003ccode\u003etextFile()\u003c/code\u003e method to read in the data from the folder that you downloaded it to RDDs. This method takes an URI for the file, which is in this case the local path of your machine, and reads it as a collection of lines. For all convenience, you\u0026rsquo;ll not only read in the .data file, but also the .domain file that contains the header. This will allow you to double check the order of the variables.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Load in the data\nrdd = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.data')\n\n# Load in the header\nheader = sc.textFile('/Users/yourName/Downloads/CaliforniaHousing/cal_housing.domain')\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"explore\"\u003eData Exploration\u003c/h2\u003e\n\u003cp\u003eYou already gathered a lot of information by just looking at the web page where you found the data set, but it\u0026rsquo;s always better to get hands-on and inspect your data with the help of Spark with Python, in this case.\u003c/p\u003e\n\u003cp\u003eImportant to understand here is that, because Spark\u0026rsquo;s execution is \u0026ldquo;lazy\u0026rdquo; execution, nothing has been executed yet. Your data hasn\u0026rsquo;t been actually read in. The \u003ccode\u003erdd\u003c/code\u003e and \u003ccode\u003eheader\u003c/code\u003e variables are actually just concepts in your mind. You have to push Spark to work for you, so let\u0026rsquo;s use the \u003ccode\u003ecollect()\u003c/code\u003e method to look at the \u003ccode\u003eheader\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eheader.collect()\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003ecollect()\u003c/code\u003e method brings the entire RDD to a single machine, and you\u0026rsquo;ll get to see the following result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e[u'longitude: continuous.', u'latitude: continuous.', u'housingMedianAge: continuous. ', u'totalRooms: continuous. ', u'totalBedrooms: continuous. ', u'population: continuous. ', u'households: continuous. ', u'medianIncome: continuous. ', u'medianHouseValue: continuous. ']\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: be careful when using \u003ccode\u003ecollect()\u003c/code\u003e! Running this line of code can possibly cause the driver to run out of memory. That\u0026rsquo;s why the following approach with the \u003ccode\u003etake()\u003c/code\u003e method is a safer approach if you want to just print a few elements of the RDD. In general, it\u0026rsquo;s a good principle to limit your result set whenever possible, \u003ca href=\"https://www.datacamp.com/tutorial/sql-tutorial-query\"\u003ejust like when you\u0026rsquo;re using SQL\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou learn that the order of the variables is the same as the one that you saw above in the presentation of the data set, and you also learn that all columns should have continuous values. Let\u0026rsquo;s force Spark to do some more work and take a look at the California housing data to confirm this.\u003c/p\u003e\n\u003cp\u003eCall the \u003ccode\u003etake()\u003c/code\u003e method on your RDD:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003erdd.take(2)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy executing the previous line of code, you take the first 2 elements of the RDD. The result is as you expected: because you read in the files with the \u003ccode\u003etextFile()\u003c/code\u003e function, the lines are just all read in together. The entries are separated by a single comma and the rows themselves are also separated by a comma:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e[u'-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000', u'-122.220000,37.860000,21.000000,7099.000000,1106.000000,2401.000000,1138.000000,8.301400,358500.000000']\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou definitely need to solve this. Now, you don\u0026rsquo;t need to split the entries, but you definitely need to make sure that the rows of your data are separate elements. To solve this, you\u0026rsquo;ll use the \u003ccode\u003emap()\u003c/code\u003e function to which you pass a lambda function to split the line at the comma. Then, check your result by running the same line with the \u003ccode\u003etake()\u003c/code\u003e method, just like you did before:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRemember\u003c/strong\u003e that lambda functions are anonymous functions which are created at runtime.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Split lines on commas\nrdd = rdd.map(lambda line: line.split(\",\"))\n\n# Inspect the first 2 lines \nrdd.take(2)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou\u0026rsquo;ll get the following result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e[[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000'], [u'-122.220000', u'37.860000', u'21.000000', u'7099.000000', u'1106.000000', u'2401.000000', u'1138.000000', u'8.301400', u'358500.000000']]\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively, you can also use the following functions to inspect your data:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Inspect the first line \nrdd.first()\n\n# Take top elements\nrdd.top(2)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you\u0026rsquo;re used to working with Pandas or data frames in R, you\u0026rsquo;ll have probably also expected to see a header, but there is none. To make your life easier, you will move on from the RDD and convert it to a DataFrame. Dataframes are preferred over RDDs whenever you can use them. Especially when you\u0026rsquo;re working with Python, the performance of DataFrames is better than RDDs.\u003c/p\u003e\n\u003cp\u003eBut what is the difference between the two?\u003c/p\u003e\n\u003cp\u003eYou can use RDDs when you want to perform low-level transformations and actions on your unstructured data. This means that you don\u0026rsquo;t care about imposing a schema while processing or accessing the attributes by name or column. Tying in to what was said before about performance, by using RDDs, you don\u0026rsquo;t necessarily want the performance benefits that DataFrames can offer for (semi-) structured data. Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.\u003c/p\u003e\n\u003cp\u003eTo recapitulate, you\u0026rsquo;ll switch to DataFrames now to use high-level expressions, to perform SQL queries to explore your data further and to gain columnar access.\u003c/p\u003e\n\u003cp\u003eSo let\u0026rsquo;s do this.\u003c/p\u003e\n\u003cp\u003eThe first step is to make a SchemaRDD or an RDD of Row objects with a schema. This is normal, because just like a DataFrame, you eventually want to come to a situation where you have rows and columns. Each entry is linked to a row and a certain column and columns have data types.\u003c/p\u003e\n\u003cp\u003eYou\u0026rsquo;ll use the \u003ccode\u003emap()\u003c/code\u003e function again and another lambda function in which you\u0026rsquo;ll map each entry to a field in a Row. To make this more visual, consider this first line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e[u'-122.230000', u'37.880000', u'41.000000', u'880.000000', u'129.000000', u'322.000000', u'126.000000', u'8.325200', u'452600.000000']\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe lambda function says that you\u0026rsquo;re going to construct a row in a SchemaRDD and that the element at index 0 will have the name \u0026ldquo;longitude\u0026rdquo;, and so on.\u003c/p\u003e\n\u003cp\u003eWith this SchemaRDD in place, you can easily convert the RDD to a DataFrame with the \u003ccode\u003etoDF()\u003c/code\u003e method.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import the necessary modules \nfrom pyspark.sql import Row\n\n# Map the RDD to a DF\ndf = rdd.map(lambda line: Row(longitude=line[0], \n                              latitude=line[1], \n                              housingMedianAge=line[2],\n                              totalRooms=line[3],\n                              totalBedRooms=line[4],\n                              population=line[5], \n                              households=line[6],\n                              medianIncome=line[7],\n                              medianHouseValue=line[8])).toDF()\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow that you have your DataFrame \u003ccode\u003edf\u003c/code\u003e, you can inspect it with the methods that you have also used before, namely \u003ccode\u003efirst()\u003c/code\u003e and \u003ccode\u003etake()\u003c/code\u003e, but also with \u003ccode\u003ehead()\u003c/code\u003e and \u003ccode\u003eshow()\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Show the top 20 rows \ndf.show()\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou\u0026rsquo;ll immediately see that this looks much different from the RDD that you were working with before:\u003c/p\u003e\n\u003cdiv class=\"figure\"\u003e\u003cimg src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Spark+ML/Spark+DataFrame.png\" alt=\"pyspark tutorial\" /\u003e\u003c/div\u003e\n\u003cbr /\u003e\n\u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: use \u003ccode\u003edf.columns\u003c/code\u003e to return the columns of your DataFrame.\u003c/p\u003e\n\u003cp\u003eThe data seems all nicely ordered into columns, but what about the data types? By reading in your data, Spark will try to infer a schema, but has this been successful here? Use either \u003ccode\u003edf.dtypes\u003c/code\u003e or \u003ccode\u003edf.printSchema()\u003c/code\u003e to get to know more about the data types that are contained within your DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Print the data types of all `df` columns\n# df.dtypes\n\n# Print the schema of `df`\ndf.printSchema()\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBecause you don\u0026rsquo;t execute the first line of code, you will only get back the following result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eroot\n |-- households: string (nullable = true)\n |-- housingMedianAge: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- medianHouseValue: string (nullable = true)\n |-- medianIncome: string (nullable = true)\n |-- population: string (nullable = true)\n |-- totalBedRooms: string (nullable = true)\n |-- totalRooms: string (nullable = true)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll columns are still of data type string\u0026hellip; That\u0026rsquo;s disappointing!\u003c/p\u003e\n\u003cp\u003eIf you want to continue with this DataFrame, you\u0026rsquo;ll need to rectify this situation and assign \u0026ldquo;better\u0026rdquo; or more accurate data types to all columns. Your performance will also benefit from this. Intuitively, you could go for a solution like the following, where you declare that each column of the DataFrame \u003ccode\u003edf\u003c/code\u003e should be cast to a \u003ccode\u003eFloatType()\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003efrom pyspark.sql.types import *\n\ndf = df.withColumn(\"longitude\", df[\"longitude\"].cast(FloatType())) \\\n   .withColumn(\"latitude\", df[\"latitude\"].cast(FloatType())) \\\n   .withColumn(\"housingMedianAge\",df[\"housingMedianAge\"].cast(FloatType())) \\\n   .withColumn(\"totalRooms\", df[\"totalRooms\"].cast(FloatType())) \\ \n   .withColumn(\"totalBedRooms\", df[\"totalBedRooms\"].cast(FloatType())) \\ \n   .withColumn(\"population\", df[\"population\"].cast(FloatType())) \\ \n   .withColumn(\"households\", df[\"households\"].cast(FloatType())) \\ \n   .withColumn(\"medianIncome\", df[\"medianIncome\"].cast(FloatType())) \\ \n   .withColumn(\"medianHouseValue\", df[\"medianHouseValue\"].cast(FloatType()))\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBut these repeated calls are quite obscure, error-proof and don\u0026rsquo;t really look nice. Why don\u0026rsquo;t you write a function that can do all of this for you in a more clean way?\u003c/p\u003e\n\u003cp\u003eThe following User-Defined Function (UDF) takes a DataFrame, column names, and the new data type that you want the have the columns to have. You say that for every column name, you take the column and you cast it to a new data type. Then, you return the DataFrame:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import all from `sql.types`\nfrom pyspark.sql.types import *\n\n# Write a custom function to convert the data type of DataFrame columns\ndef convertColumn(df, names, newType):\n  for name in names: \n     df = df.withColumn(name, df[name].cast(newType))\n  return df \n\n# Assign all column names to `columns`\ncolumns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n\n# Conver the `df` columns to `FloatType()`\ndf = convertColumn(df, columns, FloatType())\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat already looks much better! You can quickly inspect the data types of \u003ccode\u003edf\u003c/code\u003e with the \u003ccode\u003eprintSchema()\u003c/code\u003e method, just like you have done before.\u003c/p\u003e\n\u003cp\u003eNow that you\u0026rsquo;ve got that all sorted out, it\u0026rsquo;s time to really get started on the data exploration. You have seen that columnar access and SQL queries were two advantages of using DataFrames. Well, now it\u0026rsquo;s time to dig a little bit further into that. Let\u0026rsquo;s start small and just select two columns from \u003ccode\u003edf\u003c/code\u003e of which you only want to see 10 rows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edf.select('population','totalBedRooms').show(10)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis query gives you the following result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e+----------+-------------+\n|population|totalBedRooms|\n+----------+-------------+\n|     322.0|        129.0|\n|    2401.0|       1106.0|\n|     496.0|        190.0|\n|     558.0|        235.0|\n|     565.0|        280.0|\n|     413.0|        213.0|\n|    1094.0|        489.0|\n|    1157.0|        687.0|\n|    1206.0|        665.0|\n|    1551.0|        707.0|\n+----------+-------------+\nonly showing top 10 rows\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can also make your queries more complex, as you see in the following example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edf.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhich gives you the following result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e+----------------+-----+                                                        \n|housingMedianAge|count|\n+----------------+-----+\n|            52.0| 1273|\n|            51.0|   48|\n|            50.0|  136|\n|            49.0|  134|\n|            48.0|  177|\n|            47.0|  198|\n|            46.0|  245|\n|            45.0|  294|\n|            44.0|  356|\n|            43.0|  353|\n|            42.0|  368|\n|            41.0|  296|\n|            40.0|  304|\n|            39.0|  369|\n|            38.0|  394|\n|            37.0|  537|\n|            36.0|  862|\n|            35.0|  824|\n|            34.0|  689|\n|            33.0|  615|\n+----------------+-----+\nonly showing top 20 rows\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBesides querying, you can also choose to describe your data and get some summary statistics. This will most definitely help you after!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edf.describe().show()\u003c/code\u003e\u003c/pre\u003e\n\u003cbr /\u003e\n\u003cdiv class=\"figure\"\u003e\u003cimg src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Spark+ML/SummaryStatistics-PySpark.png\" alt=\"PySpark Machine Learning\" /\u003e\u003c/div\u003e\n\u003cbr /\u003e\n\u003cp\u003eLook at the minimum and maximum values of all the (numerical) attributes. You see that multiple attributes have a wide range of values: you will need to normalize your dataset.\u003c/p\u003e\n\u003ch2 id=\"preprocess\"\u003eData Preprocessing\u003c/h2\u003e\n\u003cp\u003eWith all this information that you gathered from your small exploratory data analysis, you know enough to preprocess your data to feed it to the model.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYou shouldn\u0026rsquo;t care about missing values; all zero values have been excluded from the data set.\u003c/li\u003e\n\u003cli\u003eYou should probably standardize your data, as you have seen that the range of minimum and maximum values is quite big.\u003c/li\u003e\n\u003cli\u003eThere are possibbly some additional attributes that you could add, such as a feature that registers the number of bedrooms per room or the rooms per household.\u003c/li\u003e\n\u003cli\u003eYour dependent variable is also quite big; To make your life easier, you\u0026rsquo;ll have to adjust the values slightly.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv id=\"preprocessing-the-target-values\" class=\"section level3\"\u003e\n\u003ch3\u003ePreprocessing the Target Values\u003c/h3\u003e\n\u003cp\u003eFirst, let\u0026rsquo;s start with the \u003ccode\u003emedianHouseValue\u003c/code\u003e, your dependent variable. To facilitate your working with the target values, you will express the house values in units of 100,000. That means that a target such as \u003ccode\u003e452600.000000\u003c/code\u003e should become \u003ccode\u003e4.526\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import all from `sql.functions` \nfrom pyspark.sql.functions import *\n\n# Adjust the values of `medianHouseValue`\ndf = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n\n# Show the first 2 lines of `df`\ndf.take(2)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can clearly see that the values have been adjusted correctly when you look at the result of the \u003ccode\u003etake()\u003c/code\u003e method:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0), Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e\n\u003cdiv id=\"feature-engineering\" class=\"section level3\"\u003e\n\u003ch3\u003eFeature Engineering\u003c/h3\u003e\n\u003cp\u003eNow that you have adjusted the values in \u003ccode\u003emedianHouseValue\u003c/code\u003e, you can also add the additional variables that you read about above. You\u0026rsquo;re going to add the following columns to the data set:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eRooms per household\u003c/em\u003e which refers to the number of rooms in households per block group;\u003c/li\u003e\n\u003cli\u003e\u003cem\u003ePopulation per household\u003c/em\u003e, which basically gives you an indication of how many people live in households per block group; And\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eBedrooms per room\u003c/em\u003e which will give you an idea about how many rooms are bedrooms per block group;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr /\u003e\n\u003cp\u003eAs you\u0026rsquo;re working with DataFrames, you can best use the \u003ccode\u003eselect()\u003c/code\u003e method to select the columns that you\u0026rsquo;re going to be working with, namely \u003ccode\u003etotalRooms\u003c/code\u003e, \u003ccode\u003ehouseholds\u003c/code\u003e, and \u003ccode\u003epopulation\u003c/code\u003e. Additionally, you have to indicate that you\u0026rsquo;re working with columns by adding the \u003ccode\u003ecol()\u003c/code\u003e function to your code. Otherwise, you won\u0026rsquo;t be able to do element-wise operations like the division that you have in mind for these three variables:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import all from `sql.functions` if you haven't yet\nfrom pyspark.sql.functions import *\n\n# Divide `totalRooms` by `households`\nroomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n\n# Divide `population` by `households`\npopulationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n\n# Divide `totalBedRooms` by `totalRooms`\nbedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n\n# Add the new columns to `df`\ndf = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n   \n# Inspect the result\ndf.first()\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou see that, for the first row, there are about 6.98 rooms per household, the households in the block group consist of about 2.5 people and the amount of bedrooms is quite low with 0.14:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eRow(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, -and this is already forseeing an issue that you might have when you\u0026rsquo;ll standardize the values in your data set- you\u0026rsquo;ll also re-order the values. Since you don\u0026rsquo;t want to necessarily standardize your target values, you\u0026rsquo;ll want to make sure to isolate those in your data set.\u003c/p\u003e\n\u003cp\u003eIn this case, you\u0026rsquo;ll need to do this by using the \u003ccode\u003eselect()\u003c/code\u003e method and passing the column names in the order that is more appropriate. In this case, the target variable \u003ccode\u003emedianHouseValue\u003c/code\u003e is put first, so that it won\u0026rsquo;t be affected by the standardization.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e also that this is the time to leave out variables that you might not want to consider in your analysis. In this case, let\u0026rsquo;s leave out variables such as \u003ccode\u003elongitude\u003c/code\u003e, \u003ccode\u003elatitude\u003c/code\u003e, \u003ccode\u003ehousingMedianAge\u003c/code\u003e and \u003ccode\u003etotalRooms\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Re-order and select columns\ndf = df.select(\"medianHouseValue\", \n              \"totalBedRooms\", \n              \"population\", \n              \"households\", \n              \"medianIncome\", \n              \"roomsPerHousehold\", \n              \"populationPerHousehold\", \n              \"bedroomsPerRoom\")\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e\n\u003cdiv id=\"standardization\" class=\"section level3\"\u003e\n\u003ch3\u003eStandardization\u003c/h3\u003e\n\u003cp\u003eNow that you have re-ordered the data, you\u0026rsquo;re ready to normalize the data. Or almost, at least. There is just one more step that you need to go through: separating the features from the target variable. In essence, this boils down to isolating the first column in your DataFrame from the rest of the columns.\u003c/p\u003e\n\u003cp\u003eIn this case, you\u0026rsquo;ll use the \u003ccode\u003emap()\u003c/code\u003e function that you use with RDDs to perform this action. You also see that you make use of the \u003ccode\u003eDenseVector()\u003c/code\u003e function. A dense vector is a local vector that is backed by a double array that represents its entry values. In other words, it's used to store arrays of values for use in PySpark.\u003c/p\u003e\n\u003cp\u003eNext, you go back to making a DataFrame out of the \u003ccode\u003einput_data\u003c/code\u003e and you re-label the columns by passing a list as a second argument. This list consists of the column names \u003ccode\u003e\"label\"\u003c/code\u003e and \u003ccode\u003e\"features\"\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import `DenseVector`\nfrom pyspark.ml.linalg import DenseVector\n\n# Define the `input_data` \ninput_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n\n# Replace `df` with the new DataFrame\ndf = spark.createDataFrame(input_data, [\"label\", \"features\"])\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, you can finally scale the data. You can use Spark ML to do this: this library will make machine learning on big data scalable and easy. You\u0026rsquo;ll find tools such as ML algorithms and everything you need to build practical ML pipelines. In this case, you don\u0026rsquo;t need to do that much preprocessing so a pipeline would maybe be overkill, but if you want to look into it, definitely consider visiting the \u003ca href=\"https://spark.apache.org/docs/latest/ml-pipeline.html\"\u003ethis page\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe input columns are the features, and the output column with the rescaled that will be included in the \u003ccode\u003escaled_df\u003c/code\u003e will be named \u003ccode\u003e\"features_scaled\"\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import `StandardScaler` \nfrom pyspark.ml.feature import StandardScaler\n\n# Initialize the `standardScaler`\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n\n# Fit the DataFrame to the scaler\nscaler = standardScaler.fit(df)\n\n# Transform the data in `df` with the scaler\nscaled_df = scaler.transform(df)\n\n# Inspect the result\nscaled_df.take(2)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet\u0026rsquo;s take a look at your DataFrame and the result. You see that, indeed, a third column \u003ccode\u003efeatures_scaled\u003c/code\u003e was added to your DataFrame, which you can use to compare with \u003ccode\u003efeatures\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])), Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e that these lines of code are very similar to what you would be doing in Scikit-Learn.\u003c/p\u003e\n\u003ch2 id=\"model\"\u003eBuilding a Machine Learning Model with Spark ML\u003c/h2\u003e\n\u003cp\u003eWith all the preprocessing done, it\u0026rsquo;s finally time to start building your Linear Regression model! Just like always, you first need to split the data into training and test sets. Luckily, this is no issue with the\u0026nbsp;\u003ccode\u003erandomSplit()\u003c/code\u003e method:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Split the data into train and test sets\ntrain_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou pass in a list with two numbers that represent the size that you want your training and test sets to have and a seed, which is needed for reproducibility reasons. If you want to know more about this, consider DataCamp\u0026rsquo;s \u003ca href=\"https://www.datacamp.com/tutorial/machine-learning-python\"\u003ePython Machine Learning Tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThen, without further ado, you can make your model!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e that the argument \u003ccode\u003eelasticNetParam\u003c/code\u003e corresponds to \u0026alpha; or the vertical intercept and that the \u003ccode\u003eregParam\u003c/code\u003e or the regularization paramater corresponds to \u0026lambda;. Go \u003ca href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression\"\u003ehere\u003c/a\u003e for more information.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Import `LinearRegression`\nfrom pyspark.ml.regression import LinearRegression\n\n# Initialize `lr`\nlr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the data to the model\nlinearModel = lr.fit(train_data)\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWith your model in place, you can generate predictions for your test data: use the \u003ccode\u003etransform()\u003c/code\u003e method to predict the labels for your \u003ccode\u003etest_data\u003c/code\u003e. Then, you can use RDD operations to extract the predictions as well as the true labels from the DataFrame and zip these two values together in a list called \u003ccode\u003epredictionAndLabel\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eLastly, you can then inspect the predicted and real values by simply accessing the list with square brackets \u003ccode\u003e[]\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Generate predictions\npredicted = linearModel.transform(test_data)\n\n# Extract the predictions and the \"known\" correct labels\npredictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\nlabels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n\n# Zip `predictions` and `labels` into a list\npredictionAndLabel = predictions.zip(labels).collect()\n\n# Print out first 5 instances of `predictionAndLabel` \npredictionAndLabel[:5]\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou\u0026rsquo;ll see the following real and predicted values (in that order):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e[(1.4491508524918457, 0.14999), (1.5705029404692372, 0.14999), (2.148727956912464, 0.14999), (1.5831547768979277, 0.344), (1.5182107797955968, 0.398)]\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"evaluate\"\u003eEvaluating the Model\u003c/h2\u003e\n\u003cp\u003eLooking at predicted values is one thing, but another and better thing is looking at some metrics to get a better idea of how good your model actually is. You can first start by printing out the coefficients and the intercept of the model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Coefficients for the model\nlinearModel.coefficients\n\n# Intercept for the model\nlinearModel.intercept\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhich gives you the following result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# The coefficients\n[0.0,0.0,0.0,0.276239709215,0.0,0.0,0.0]\n\n# The intercept\n0.990399577462\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, you can also use the \u003ccode\u003esummary\u003c/code\u003e attribute to pull up the \u003ccode\u003erootMeanSquaredError\u003c/code\u003e and the \u003ccode\u003er2\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Get the RMSE\nlinearModel.summary.rootMeanSquaredError\n\n# Get the R2\nlinearModel.summary.r2\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe RMSE measures how much error there is between two datasets comparing a predicted value and an observed or known value. The smaller an RMSE value, the closer predicted and observed values are.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe R2 (\u0026ldquo;R squared\u0026rdquo;) or the coefficient of determination is a measure that shows how close the data are to the fitted regression line. This score will always be between 0 and a 100% (or 0 to 1 in this case), where 0% indicates that the model explains none of the variability of the response data around its mean, and 100% indicates the opposite: it explains all the variability. That means that, in general, the higher the R-squared, the better the model fits your data.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou'll get back the following result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# RMSE\n0.8692118678997669\n\n# R2\n0.4240895287218379\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e\n\u003cdiv id=\"before-you-go\" class=\"section level2\"\u003e\n\u003cp\u003eThere's definitely some improvements needed to your model! If you want to continue with this model, you can play around with the parameters that you passed to your model, the variables that you included in your original DataFrame, .... But this is where the tutorial ends for now!\u0026nbsp;\u003c/p\u003e\n\u003ch2\u003eBefore you Go\u0026hellip;\u003c/h2\u003e\n\u003cp\u003eBefore you go, make sure to stop the SparkSession with the following line of code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003espark.stop()\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e\n\u003cdiv id=\"taking-big-data-further\" class=\"section level2\"\u003e\n\u003ch2\u003eTaking Big Data Further\u003c/h2\u003e\n\u003cp\u003eCongrats! You have made it to the end of this tutorial, where you learned how to make a linear regression model with the help of Spark ML.\u003c/p\u003e\n\u003cp\u003eIf you are interested in learning more about PySpark, consider taking DataCamp\u0026rsquo;s \u003ca href=\"https://www.datacamp.com/courses/introduction-to-pyspark\"\u003eIntroduction to PySpark\u003c/a\u003e course and take a look at the \u003ca href=\"https://www.datacamp.com/tutorial/apache-spark-tutorial-machine-learning\"\u003eApache Spark Tutorial: ML with PySpark\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e"},{"__typename":"ComponentDataCampProductsCoursesOrTracksCarousel","id":"229","title":"\u003cp\u003eLearn more about Python and PySpark\u003c/p\u003e","description":"\u003cp\u003eLearn more about Python and PySpark\u003c/p\u003e","seeMoreUrl":null,"carousel":[{"algoliaItem":"4452__Introduction to PySpark__introduction-to-pyspark","description":"\u003cp\u003eIn this course, you'll learn how to use Spark from Python! Spark is a tool for doing parallel computation with large datasets and it integrates well with Python. PySpark is the Python package that makes the magic happen. You'll use this package to work with data about flights from Portland and Seattle. You'll learn to wrangle this data and build a whole machine learning pipeline to predict whether or not flights will be delayed. Get ready to put some Spark in your Python code and dive into the world of high-performance machine learning!\u003c/p\u003e"},{"algoliaItem":"14989__Machine Learning with PySpark__machine-learning-with-pyspark","description":"\u003cp\u003eSpark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you'll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you'll analyse a large dataset of flight delays and spam text messages. With this background you'll be ready to harness the power of Spark and apply it on your own Machine Learning projects!\u003c/p\u003e"},{"algoliaItem":"16473__Cleaning Data with PySpark__cleaning-data-with-pyspark","description":"\u003cp\u003eWorking with data is tricky - working with millions or even billions of rows is worse. Did you receive some data processing code written on a laptop with fairly pristine data? Chances are you\u0026rsquo;ve probably been put in charge of moving a basic data process from prototype to production. You may have worked with real world datasets, with missing fields, bizarre formatting, and orders of magnitude more data. Even if this is all new to you, this course helps you learn what\u0026rsquo;s needed to prepare data processes using Python with Apache Spark. You\u0026rsquo;ll learn terminology, methods, and some best practices to create a performant, maintainable, and understandable data processing platform.\u003c/p\u003e"},{"algoliaItem":"6249__Feature Engineering with PySpark__feature-engineering-with-pyspark","description":"\u003cp\u003eThe real world is messy and your job is to make sense of it. Toy datasets like MTCars and Iris are the result of careful curation and cleaning, even so the data needs to be transformed for it to be useful for powerful machine learning algorithms to extract meaning, forecast, classify or cluster. This course will cover the gritty details that data scientists are spending 70-80% of their time on; data wrangling and feature engineering. With size of datasets now becoming ever larger, let's use PySpark to cut this Big Data problem down to size!\u003c/p\u003e"},{"algoliaItem":"13023__Big Data Fundamentals with PySpark__big-data-fundamentals-with-pyspark","description":"\u003cp\u003eThere's been a lot of buzz about Big Data over the past few years, and it's finally become mainstream for many companies. But what is this Big Data? This course covers the fundamentals of Big Data via PySpark. Spark is a \"lightning fast cluster computing\" framework for Big Data. It provides a general data processing platform engine and lets you run programs up to 100x faster in memory, or 10x faster on disk, than Hadoop. You\u0026rsquo;ll use PySpark, a Python package for Spark programming and its powerful, higher-level libraries such as SparkSQL, MLlib (for machine learning), etc. You will explore the works of William Shakespeare, analyze Fifa 2018 data and perform clustering on genomic datasets. At the end of this course, you will have gained an in-depth understanding of PySpark and its application to general Big Data analysis.\u003c/p\u003e"},{"algoliaItem":"7049__Building Recommendation Engines with PySpark__recommendation-engines-in-pyspark","description":"\u003cp\u003eThis course will show you how to build recommendation engines using Alternating Least Squares in PySpark. Using the popular MovieLens dataset and the Million Songs dataset, this course will take you step by step through the intuition of the Alternating Least Squares algorithm as well as the code to train, test and implement ALS models on various types of customer data.\u003c/p\u003e"}],"cardResponses":[{"slug":"introduction-to-pyspark","title":"Introduction to PySpark","contentType":"course","numberOfUsers":134846,"shortDescription":"Learn to implement distributed data management and machine learning in Spark using the PySpark package.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"machine-learning-with-pyspark","title":"Machine Learning with PySpark","contentType":"course","numberOfUsers":22006,"shortDescription":"Learn how to make predictions from data with Apache Spark, using decision trees, logistic regression, linear regression, ensembles, and pipelines.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"cleaning-data-with-pyspark","title":"Cleaning Data with PySpark","contentType":"course","numberOfUsers":24770,"shortDescription":"Learn how to clean data with Apache Spark in Python.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"feature-engineering-with-pyspark","title":"Feature Engineering with PySpark","contentType":"course","numberOfUsers":13781,"shortDescription":"Learn the gritty details that data scientists are spending 70-80% of their time on; data wrangling and feature engineering.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"big-data-fundamentals-with-pyspark","title":"Big Data Fundamentals with PySpark","contentType":"course","numberOfUsers":46985,"shortDescription":"Learn the fundamentals of working with big data with PySpark.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false},{"slug":"recommendation-engines-in-pyspark","title":"Building Recommendation Engines with PySpark","contentType":"course","numberOfUsers":11686,"shortDescription":"Learn tools and techniques to leverage your own big data to facilitate positive experiences for your users.","timeNeededInHours":4,"certificationAvailable":false,"skillVerificationAvailable":false}]}]},"preview":false,"slug":"apache-spark-tutorial-machine-learning","unoptimisedInlineCarouselComponents":[],"unoptimizedCarouselCardResponses":[],"workspaceCtaComponent":null,"_nextI18Next":{"initialI18nStore":{"en":{"global":{"Alpa":{"calloutButton":"Start Learning For Free","calloutText":"Learn the data and AI skills you need online at your own pace—from non-coding essentials to data science, AI, and machine learning.","calloutTitle":"What is DataCamp?","mainText":"DataCamp's proven learning methodology.","mainTitle":"We learn best by doing"},"App":{"metaDescription":"Learn Data Science \u0026 AI from the comfort of your browser, at your own pace with DataCamp's video tutorials \u0026 coding challenges on R, Python, Statistics \u0026 more.","ogImage":"https://www.datacamp.com/datacamp-socials-share.png","ogTitle":"Learn R, Python \u0026 Data Science Online","title":"Learn Data Science and AI Online | DataCamp"},"CampusDragAndDropFigure":{"caption":"\u003chighlight\u003ePractice\u003c/highlight\u003e and \u003chighlight\u003eapply\u003c/highlight\u003e your skills","imageAlt":"Screenshot of campus exercise and dragging answer","imageSrc":"/Marketing/Illustrations/campus-drag-and-drop.png"},"CampusExerciseFigure":{"captionOne":"Interactive exercises, short videos","captionThree":"— run code from your browser","captionTwo":"No installation required","cta":"Start Learning for Free","imageAlt":"Screenshot of Campus exercise","imageSrc":"/Marketing/Screenshots/screenshot-campus-encoding-time.png"},"CertificationAvailable":{"aiFundamentals":"AI Fundamentals","dataLiteracy":"Data Literacy","sqlAssociate":"SQL Associate"},"CertificationDetails":{"Labels":{"language":"Language","levels":"Levels","price":"Price","programmingLanguages":"Programming Languages"},"Values":{"associate":"Associate","english":"English","price":"$25 per month (included in our {link})"}},"CertificationSection":{"certificationAvailable":"Certification available","imageAlt":"Screenshot of data scientist and hiring manager","imageSrc":"/Marketing/Certification/certification_light.png","primaryButton":"Learn More","skillVerificationAvailable":"Skill verification available","statementOfAccomplishment":"Statement of Accomplishment","text":"From a certification in data science to personalized resume reviews and interview prep—we've got you covered.","title":"Land your dream job in data science","trackPromotion":"Get certified to gain industry recognition and showcase your newly acquired skills."},"Companies":{"heading":"DataCamp is trusted by top companies globally","text":"Loved by learners at thousands of companies"},"CoreValues":{"actionParagraph":"We are a fast-moving company that understands the importance of taking action.","actionTitle":"Action","actLikeOwnerParagraph":"We take initiative and embrace accountability.","actLikeOwnerTitle":"Act like an owner","customerFocusedParagraph":"We prioritize meeting learners and members needs above everything else.","customerFocusedTitle":"Customer focused","dataDrivenParagraph":"Data skills are our business. We believe in making decisions based on data and facts.","dataDrivenTitle":"Data-driven decision-making","heading":"Our \u003chighlight\u003ecore values\u003c/highlight\u003e","subheading":"We live by a set of core values designed to set us on the best path to achieve our mission of democratizing data skills for everyone.","transparencyParagraph":"We believe that being transparent leads to improvement and success.","transparencyTitle":"Transparency"},"CourseChapters":{"hideDetails":"Hide chapter details","primaryButton":"Play Chapter Now","viewDetails":"View chapter details"},"CourseDetails":{"audioRecordedBy":"audio recorded by","heading":"Course Description","tracksHeading":"In other tracks"},"Different":{"button":"Learn More","heading":"What makes DataCamp \u003chighlight\u003edifferent\u003c/highlight\u003e?","text":"We believe in learning by doing. With DataCamp, you complete real-world coding exercises directly in your browser – no downloads or complex installations. Roll up your sleeves and start learning today."},"Error":{"404":"404 | Not Found","500":"500 | Internal Server Error","heading":"Oops, something went wrong"},"FlashSaleTwoFigure":{"imageAlt":"Screenshot of campus exercise with photo of Richie Cotton"},"FlashSaleTwoFigures":{"captionOne":"Interactive exercises","captionTwo":"short videos"},"FormSignup":{"title":"Create Your Free Account"},"ForOrganizationLnDBanner":{"heading":"For Organization Learning \u0026 Development","paragraph":"Maximize organizational learning with a fully-configured and safe sandbox environment for data experimentation.","primaryButton":"Request a Demo"},"FreeBusinessButtons":{"primaryButton":"Start Learning For Free","secondaryButton":"DataCamp For Business"},"ImproveHiring":{"heading":"Streamline your hiring for data professionals and get ahead of the competition","SectionOne":{"heading":"Access job-ready data professionals","text":"1,000+ qualified data professionals join us each week"},"SectionTwo":{"heading":"Stop wasting your time","text":"Match with and interview candidates that meet your hiring needs"}},"InstructorCard":{"bioLink":"See full bio","courses":"Instructor of \u003ccourseLink\u003e{title}\u003c/courseLink\u003e {numOtherCourses, plural, =0 {} =1 {and \u003cportfolioLink\u003eone other course\u003c/portfolioLink\u003e} other {and \u003cportfolioLink\u003e# other courses\u003c/portfolioLink\u003e}}","heading":"Learn with"},"InstructorsFigure":{"captionOne":"Learn from the","captionTwo":"best instructors","cta":"Become an instructor","imageAlt":"Screenshot of Campus exercise"},"IntroSignup":{"primaryButton":"Start Learning For Free","secondaryButton":"DataCamp For Business","subtitle":"Unlock the power of data and AI by learning Python, ChatGPT, SQL, Power BI, and earn industry-leading Certifications.","title":"Learn data and AI skills"},"LearningExperienceFigure":{"captionOne":"Interactive exercises","captionTwo":"short videos","imageAlt":"Screenshot of campus exercise with photo of Richie Cotton","imageSrc":"/Marketing/Illustrations/hands-on-learning-experience.png"},"ReviewBreakdown":{"fromReviews":"from {count, plural, one {# review} other {# reviews}}"},"Reviews":{"heading":"Don’t just take our word for it"},"ReviewsList":{"hideAll":"Hide all reviews","seeAll":"See all reviews","SortOptions":{"highestToLowest":"Highest to Lowest","lowestToHighest":"Lowest to Highest","mostRecent":"Most recent","topReviews":"Top reviews"}},"ScrollSectionsPrimary":{"title":"\u003chighlight\u003eHands-on\u003c/highlight\u003e interactive learning experience"},"SectionFreeBusiness":{"heading":"Ready to learn?","text":"Join over {users} million learners and go further, faster, with DataCamp."},"Test":{"heading":"Test"},"Testimonials":{"devonJosephText":"I've used other sites—Coursera, Udacity, things like that—but DataCamp's been the one that I've stuck with.","gabrielLagesSource":"Gabriel Lages, Business Intelligence and Analytics Manager, Hotmart","gabrielLagesText":"As a manager, DataCamp helps me show my team new packages and new ways to solve problems.","globalHospitalityCompanySource":"Head of Training and Talent at an International Government Agency","globalHospitalityCompanyText":"We were building a learning cohort curriculum, and our Customer Success Manager at DataCamp helped map out courses that aligned to our curriculum, which was critical in building the learning cohort.","intlGovernmentAgencySource":"Director of Learning and Development for a Global Hospitality Company","intlGovernmentAgencyText":"I wish I had Advanced Enterprise Reporting in every LMS. I could enter certain internal marketing campaigns, and assess adoption improvement using the Adoption Funnel.","jamenLongSource":"Jamen Long, Data Scientist, Westfield Labs","jamenLongText":"DataCamp was essential for getting my job, which was my break into data science.","martijnTheuwissenSource":"Martijn Theuwissen, DataCamp COO","martijnTheuwissenText":"Data is the core of a business today. Yet most companies only analyze a fraction of their data, and do so inefficiently. Many relegate data science knowledge to a small group within the company. Consequently, they face an enormous skill gap that they can’t hire their way out of. This runs counter to the data transformation initiatives that most companies are going through today. Democratizing data skills and making entire organizations data fluent is where we come in.","michaelAmesSource":"Michael Ames, Wake Forest University, USA","michaelAmesText":"DataCamp is a great resource for students learning to code. The interactive learning environment, quality, and variety of content can't be beat. Coding can be intimidating to students and the DataCamp platform allows me to lower the barriers for students as they learn to code. \u003cbr /\u003e\u003cbr /\u003eThis has allowed my classes to have a higher quality engagement with students because we can spend our time together diving deep on a variety of analyses rather than spending our time on syntax issues.","rachelAltSimmonsText":"We think of it as everyone's responsibility to be more data-driven. After all, every single one of us is probably touching data in some way.","sarahSchlobohmSource":"Sarah Schlobohm Senior Analytics Manager, Global Risk Analytics, HSBC","sarahSchlobohmText":"We've trialed a number of other online learning solutions, but only DataCamp provides the interactive experience that reinforces learning. There’s a great depth of content on the site. It’s great for absolute beginners, but there is very advanced content for users with more experience."},"TextEnterprise":{"text":"Contact sales for pricing"},"TextPremium":{"billedAnually":"billed annually","firstMonth":"first month","perMonth":"/month","perMonthBilledAnnually":"/Month, Billed Annually","perYear":"/Year"},"TextPro":{"text":"per month or {yearlyPricePerYear} per year"},"TextStarter":{"text":"Free"},"TextTeams":{"billedAnually":"billed annually","firstMonth":"per user for first month","firstYear":"Per User / 1st Year","perMonth":"per user /month","perMonthFirstYear":"Per Month, Per User / 1st Year"},"TracksFundamentalsTableauPage":{"noInstallationRequired":"No software installation required","technologiesButton":"See all {count} courses","technologiesHeading":"Advance your career with DataCamp","technologiesText":"Develop concrete, in-demand data analytics skills and be ready to apply your knowledge in the professional world."},"TracksGetStarted":{"text":"No coding required—learn everything you’ve ever been afraid to ask about data science."},"TracksPlansSection":{"careerTracksTipText":"Career tracks guide you through a series of curated courses to build your career in data science and analytics.","coursesTipText":"Acquire new skills by taking courses that combine short expert videos and hands-on coding exercises.","firstChapter":"First chapter of all courses","freeDescription":"Give DataCamp a try","groups":"Groups","heading":"Get the data skills you need to \u003chighlight\u003efuture proof\u003c/highlight\u003e your career with our hands-on training.","perMonth":"/month","practiceChallenge":"{count, plural, one {practice challenge (# set)} other {practice challenges (# sets)}}","practiceChallengesTipText":"Practice your newly acquired skills with fun, bite-sized coding challenges.","practiceChallengesUnlimited":"Practice challenges (Unlimited)","premiumDescription":"Advanced content for ambitious learners","premiumDisclaimer":"Billed annually","projectsTipText":"Apply your skills to real-world situations and build your project portfolio.","skillAssessments":"Skill assessments (Unlimited)","skillTracksTipText":"Skill tracks guide you through a series of curated courses to acquire a certain skill.","technologiesContent":"{technologies} content","text":"Find your plan to start learning today.","workspaceTipText":"An in-browser tool to write, run and share your data analysis."},"TracksSlugPage":{"signUpTitle":"Join over \u003clearners\u003e{count} learners\u003c/learners\u003e and start {trackTitle} {trackSubtitle} today!"},"TracksTrackList":{"filter":"Filter by Technology","heading":"Start learning \u003chighlight\u003efor free\u003c/highlight\u003e","help":"Help with choosing a technology","text":"Grow your data skills by following a guided series of videos and interactive exercises."},"WorkspaceSection":{"imageAlt":"Workspaces in a user's portfolio","imageSrc":"Marketing/Workspace/workspaces-in-portfolio.png","primaryButton":"Learn More","text":"Start from 60+ datasets and templates to create insightful analyses with DataCamp’s AI-enabled data notebook.","title":"Build your data portfolio"}},"glossary":{"alpa":{"apply":{"description":"Solve real-world problems","name":"Apply"},"assess":{"description":"Test your skills and track progress","name":"Assess"},"learn":{"description":"Complete interactive courses","name":"Learn"},"practice":{"description":"Practice with quick daily challenges","name":"Practice"},"review":{"description":"Reinforce what you're learning","name":"Review"}},"article":"{count, plural, one {article} other {articles}}","assessment":"{count, plural, one {assessment} other {assessments}}","assignment":"{count, plural, one {assignment} other {assignments}}","blog":"{count, plural, one {blog} other {blogs}}","blogPost":"{count, plural, one {blog post} other {blog posts}}","bookmark":"{count, plural, one {bookmark} other {bookmarks}}","career":"{count, plural, one {career} other {careers}}","careers":{"associateDataScientist":"Associate Data Scientist","dataAnalyst":"Data Analyst","dataBiAnalyst":"Data/BI Analyst","dataEngineer":"Data Engineer","dataScientist":"Data Scientist","machineLearningScientist":"Machine Learning Scientist","programmer":"Developer","statistician":"Statistician"},"careerTrack":"{count, plural, one {career track} other {career tracks}}","caseStudy":"{count, plural, one {case study} other {case studies}}","category":"{count, plural, one {category} other {categories}}","certification":"{count, plural, one {certification} other {certifications}}","chapter":"{count, plural, one {chapter} other {chapters}}","cheatSheet":"{count, plural, one {cheat sheet} other {cheat sheets}}","codeAlong":"{count, plural, one {code-along} other {code-alongs}}","collaborator":"{count, plural, one {collaborator} other {collaborators}}","company":"{count, plural, one {company} other {companies}}","competition":"{count, plural, one {competition} other {competitions}}","contentType":"{count, plural, one {Content type} other {Content types}}","course":"{count, plural, one {course} other {courses}}","customTrack":"{count, plural, one {custom track} other {custom tracks}}","dataCampLearn":"DataCamp Learn","dataCampSignal":"DataCamp Signal","dataset":"{count, plural, one {dataset} other {datasets}}","dateLaunched":"Date launched","difficulties":{"advanced":"Advanced","beginner":"Beginner","intermediate":"Intermediate"},"doc":"{count, plural, one {doc} other {docs}}","duration":"{count, plural, one {duration} other {durations}}","exercise":"{count, plural, one {exercise} other {exercises}}","flashcard":"{count, plural, one {flashcard} other {flashcards}}","free":"Free","industry":"{count, plural, one {industry} other {industries}}","instruction":"{count, plural, one {instruction} other {instructions}}","instructor":"{count, plural, one {instructor} other {instructors}}","integration":"{count, plural, one {integration} other {integrations}}","leaderboard":"Leaderboard","learner":"{count, plural, one {learner} other {learners}}","myLibrary":"my library","organizationTrack":"{count, plural, one {organization track} other {organization tracks}}","participant":"{count, plural, one {participant} other {participants}}","plan":"{count, plural, one {plan} other {plans}}","plans":{"basic":"Basic","enterprise":"Enterprise","premium":"Premium","starter":"Starter","teams":"Teams"},"podcast":"{count, plural, one {podcast} other {podcasts}}","practice":"{count, plural, one {practice} other {practices}}","prerequisite":"{count, plural, one {prerequisite} other {prerequisites}}","products":{"certification":"Certification","datalab":"DataLab","learn":"Learn","recruit":"Recruit","workspace":"DataLab"},"project":"{count, plural, one {project} other {projects}}","rating":"{count, plural, one {rating} other {ratings}}","resource":"{count, plural, one {resource} other {resources}}","skillAssessment":"{count, plural, one {skill assessment} other {skill assessments}}","skillTrack":"{count, plural, one {skill track} other {skill tracks}}","statementOfAccomplishment":"{count, plural, one {statement of accomplishment} other {statements of accomplishment}}","technologies":{"ai":"AI","airflow":"Airflow","alteryx":"Alteryx","aws":"AWS","azure":"Azure","bigquery":"BigQuery","chatGpt":"ChatGPT","databricks":"Databricks","dataEngineering":"Data Engineering","dbt":"dbt","docker":"Docker","dvc":"DVC","excel":"Excel","gcp":"GCP","git":"Git","googleSheets":"Google Sheets","java":"Java","julia":"Julia","kafka":"Kafka","kubernetes":"Kubernetes","mlflow":"MLflow","openAi":"OpenAI","oracle":"Oracle","powerBi":"Power BI","python":"Python","pytorch":"PyTorch","r":"R","redshift":"Redshift","rust":"Rust","scala":"Scala","shell":"Shell","snowflake":"Snowflake","spark":"Spark","spreadsheets":"Spreadsheets","sql":"SQL","tableau":"Tableau","theory":"Theory"},"technology":"{count, plural, one {technology} other {technologies}}","template":"{count, plural, one {template} other {templates}}","topic":"{count, plural, one {topic} other {topics}}","topics":{"other":"Other","appliedFinance":"Applied Finance","artificialIntelligence":"Artificial Intelligence","career":"Career","caseStudies":"Case Studies","cloud":"Cloud","dataAnalysis":"Data Analysis","dataEngineering":"Data Engineering","dataFundamentals":"Data Fundamentals","dataLiteracy":"Data Literacy","dataManagement":"Data Management","dataManipulation":"Data Manipulation","dataPreparation":"Data Preparation","dataVisualization":"Data Visualization","dataWarehouse":"Data Warehouse","exploratoryDataAnalysis":"Exploratory Data Analysis","importingAndCleaningData":"Importing \u0026 Cleaning Data","leadership":"Leadership","machineLearning":"Machine Learning","probabilityAndStatistics":"Probability \u0026 Statistics","programming":"Software Development","pythonProgramming":"Python Programming","reporting":"Reporting","rProgramming":"R Programming","skills":"Skills"},"track":"{count, plural, one {track} other {tracks}}","tutorial":"{count, plural, one {tutorial} other {tutorials}}","video":"{count, plural, one {video} other {videos}}","webinar":"{count, plural, one {webinar} other {webinars}}","whitePaper":"{count, plural, one {white paper} other {white papers}}","workbook":"{count, plural, one {workbook} other {workbooks}}","workspace":"{count, plural, one {workspace} other {workspaces}}","xp":"XP"},"common":{"about":"About","all":"All","back":"Back","backToTop":"Back to top","business":"Business","close":"Close","comingSoon":"Coming soon","continue":"Continue","country":"{count, plural, one {country} other {countries}}","day":"{count, plural, one {day} other {days}}","documentation":"Documentation","duration":"duration","event":"{count, plural, one {event} other {events}}","filter":"filter","getStarted":"Get started","goToCourse":"Go to Course","home":"Home","hour":"{count, plural, one {hour} other {hours}}","included":"Included","learnMore":"Learn more","lessThan":"Less than {time} {unit}","limited":"Limited","logIn":"Log in","minute":"{count, plural, one {minute} other {minutes}}","mostRelevant":"Most relevant","new":"New","newest":"Newest","next":"Next","no":"no","notIncluded":"Not included","or":"or","other":"other","over":"Over {time} {unit}","previous":"Previous","pricing":"Pricing","readLess":"Read Less","readMore":"Read More","result":"{count, plural, one {result} other {results}}","reviews":"{count, plural, one {# review} other {# reviews}}","role":"{count, plural, one {role} other {roles}}","search":"Search","second":"{count, plural, one {second} other {seconds}}","seeAll":"See All","seeMore":"See More","selectACountry":"Select a Country","showLess":"Show Less","showMore":"Show More","skillLevel":"Skill Level","soon":"Soon","sortBy":"Sort by","support":"Support","tm":"TM","tool":"{count, plural, one {tool} other {tools}}","tryAgain":"Try Again","type":"type","university":"{count, plural, one {university} other {universities}}","unlimited":"Unlimited","upgrade":"Upgrade","yes":"yes"},"media-centre":{"allPosts":"All posts","Breadcrumbs":{"home":"Home"},"CodeBlock":{"codeCopied":"Copied!","experimentWithCode":"Experiment with this code in","explainCode":"Explain code","feedbackConfirmed":"Thanks! We will use your feedback to improve our experience.","hideCodeExplanation":"Hide code explanation","runCode":"Run code","wasThisHelpful":"Was this helpful?"},"contents":"Contents","copySuccess":"The doc link was copied to clipboard","CtaWorkspaceButton":{"cheatSheet":"Run and edit the code from this cheat sheet online","tutorial":"Run and edit the code from this tutorial online"},"featured":"Featured","HeaderSpotlight":{"label":"In the spotlight"},"ListingHeader":{"badge":"learning data science","categoriesLabel":"Discover categories","codeAlongBadge":"applying data science"},"MediaHeaderContent":{"updated":"Updated"},"mostRecent":"Most recent","Pagination":{"next":"Next","previous":"Previous"},"read":"read","related":"Related","share":"Share","Subnavigation":{"category":"Category","cta":"Write for us","ctaDemoRequest":"Request a Demo"},"TableOfContents":{"ariaLabel":"Table of contents","closeAriaLabel":"Close menu","docsTitle":"Documents","title":"Contents"},"topics":"Topics","trending":"Trending","viewMore":"View more","WorkspaceCtaComponent":{"cheatSheet":"Run and edit the code from this cheatSheet online","runAndEditTheCode":"Run and edit the code from this tutorial online","tutorial":"Run and edit the code from this tutorial online"}}}},"initialLocale":"en","ns":["global","glossary","common","media-centre"],"userConfig":null},"_sentryTraceData":"b57eb7920b984ea19a78848dd87c62d4-b2b55bb85b2dae4f-0"},"__N_SSP":true},"page":"/tutorial/[slug]","query":{"utm_source":"google","utm_medium":"paid_search","utm_campaignid":"19589720824","utm_adgroupid":"157156376111","utm_device":"m","utm_keyword":"","utm_matchtype":"","utm_network":"g","utm_adpostion":"","utm_creative":"716160943435","utm_targetid":"aud-517318241987:dsa-2218886984300","utm_loc_interest_ms":"","utm_loc_physical_ms":"1009899","utm_content":"","utm_campaign":"230119_1-sea~dsa~tofu_2-b2c_3-row-p2_4-prc_5-na_6-na_7-le_8-pdsh-go_9-nb-e_10-na_11-na-oct24","gad_source":"1","gclid":"Cj0KCQjw4Oe4BhCcARIsADQ0cskwCCJkoGWILJvUh5NZgj_VFuSHme5vGerxTGQSBnbR6VxCz0RZ1DwaApxFEALw_wcB","slug":"apache-spark-tutorial-machine-learning"},"buildId":"K4oD9ew1IaXuKIxkgk4N0","assetPrefix":"/media-app","isFallback":false,"isExperimentalCompile":false,"dynamicIds":[56321,3233,12055,23396],"gssp":true,"locale":"en","locales":["en","es","pt","de","fr"],"defaultLocale":"en","scriptLoader":[]}</script></body></html>