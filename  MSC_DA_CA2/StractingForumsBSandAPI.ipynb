{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import praw\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"REDDIT_USER_AGENT\")\n",
    ")\n",
    "\n",
    "#url = 'https://www.reddit.com/r/ireland/comments/w5lhur/we_need_to_get_out_of_animal_farming_all_together/'\n",
    "url = 'https://www.reddit.com/r/ireland/comments/1aghrfn/irish_farmers_protest_in_solidarity_with_eu/'\n",
    "submission = reddit.submission(url=url)\n",
    "\n",
    "# Collecting author and comment\n",
    "comments = []\n",
    "submission.comments.replace_more(limit=0)\n",
    "for comment in submission.comments.list():\n",
    "    author = comment.author.name if comment.author else \"Deleted\"  # Handling deleted users\n",
    "    comments.append({'author': author, 'statement': comment.body})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the comments to a JSON file\n",
    "with open('extracted_redditfarming.json', 'w') as f:\n",
    "    json.dump(comments, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              author                                          statement\n",
      "0       Franz_Werfel  the famers protests on the continent were abou...\n",
      "1         lamahorses      Over 30% of the European budget is on CAP etc\n",
      "2  ConnolysMoustache  What are they even protesting?\\n\\nThe IFA repr...\n",
      "3            bintags                     These people are brainwashed.Â \n",
      "4  gofuckyoureself21  Stop bitching at each other and get out and su...\n"
     ]
    }
   ],
   "source": [
    "# Load data from JSON file into a DataFrame\n",
    "df_comments = pd.read_json('extracted_redditfarming.json')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to extracted_forum4farming.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Base URL of the forum thread\n",
    "base_url = \"https://www.forum4farming.com/forum/index.php?threads/cap-2023-2027.20587\"\n",
    "\n",
    "# Regular expression to match \"X said:\" and capture X and the statement\n",
    "pattern = r\"(\\w+) said:\\s*(.*)\"\n",
    "\n",
    "# List to store the filtered data\n",
    "extracted_data = []\n",
    "\n",
    "# Loop through the pages of the thread\n",
    "for page_num in range(1, 7):  # going over 6 pages\n",
    "    # Construct the URL for each page\n",
    "    url = f\"{base_url}/page-{page_num}\" if page_num > 1 else base_url + \"/\"\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract all text from the forum\n",
    "        posts = soup.find_all('article', class_='message--post')\n",
    "        for post in posts:\n",
    "            text = post.find('div', class_='bbWrapper').text\n",
    "            # Find all matches of the pattern\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "            for match in matches:\n",
    "                name, statement = match\n",
    "                # Clean the statement of newlines and extra spaces\n",
    "                statement = re.sub(r'\\s+', ' ', statement.strip())\n",
    "                extracted_data.append({\"author\": name, \"statement\": statement})\n",
    "\n",
    "# Save the extracted data to a JSON file\n",
    "output_file_path = 'extracted_forum4farming.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(extracted_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "#print(f'Data has been written to {output_file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
